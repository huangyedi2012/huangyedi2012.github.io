<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>最大公约数（辗转相除法）</title>
    <url>/2017/04/09/2017-04-09-%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0%EF%BC%88%E8%BE%97%E8%BD%AC%E7%9B%B8%E9%99%A4%E6%B3%95%EF%BC%89/</url>
    <content><![CDATA[<p>辗转相除法，又称欧几里得算法，是求最大公约数(Greatest Common Divisor)的算法。</p>
<span id="more"></span>
<h1>算法描述</h1>
<p>设两数为$a$、$b$ $(a&gt;b)$，求$a$和$b$最大公约数$gcd(a，b)$的步骤如下：</p>
<ol>
<li>用$b$除$a$，得$a÷b=q…r(0\leq r)$。</li>
<li>若$r=0$，则$gcd(a,b)=b$；结束。</li>
<li>若$r≠0$，取$a=b,b=r$，执行第1步。</li>
</ol>
<h1>原理证明</h1>
<p>设两数为$a$、$b$ $(b\leq a)$，用$gcd(a,b)$表示$a$，$b$的最大公约数，$r=a\ mod\ b $为$a$除以$b$以后的余数，$k$为$a$除以$b$的商，即$a÷b=k…r$。</p>
<p>辗转相除法即是要证明$gcd(a,b)=gcd(b,r)$。</p>
<ol>
<li>令$c=gcd(a,b)$，则设$a=mc$，$b=nc$</li>
<li>则$r =a-kb=mc-knc=(m-kn)c$</li>
<li>即$c$也是$r$的因数</li>
<li>可以断定$m-kn$与$n$互素【否则，可设$m-kn=xd$，$n=yd$，$(d&gt;1)$，则$m=kn+xd=kyd+xd=(ky+x)d$，则$a=mc=(ky+x)dc$，$b=nc=ycd$，故$a$与$b$最大公约数成为$cd$，而非$c$，与前面结论矛盾】</li>
</ol>
<p>从而可知$gcd(b,r)=c$，继而$gcd(a,b)=gcd(b,r)$。</p>
<h1>算法实现(c++)</h1>
<p>递归方式：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">gcd</span><span class="params">(<span class="type">int</span> a,<span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(b == <span class="number">0</span>)</span><br><span class="line">    	<span class="keyword">return</span> b;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    	<span class="keyword">return</span> <span class="built_in">gcd</span>(b, a % b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>迭代方式：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">gcd</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(b != <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> r = a % b;</span><br><span class="line">        a = b;</span><br><span class="line">        b = r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>gcd</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 2.7.3 安装</title>
    <url>/2017/04/14/2017-04-14-Hadoop-2-7-3-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>在此记录hadoop 2.7.3版本的安装过程以及基本配置过程。</p>
<span id="more"></span>
<h1>安装环境</h1>
<ol>
<li>jdk1.8</li>
<li>hadoop 2.7.3</li>
<li>CentOS release 6.7 (Final) * 3</li>
</ol>
<table>
<thead>
<tr>
<th>hostname</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>172.168.170.84</td>
</tr>
<tr>
<td>slave1</td>
<td>172.168.170.88</td>
</tr>
<tr>
<td>slave2</td>
<td>172.168.170.89</td>
</tr>
</tbody>
</table>
<h1>必需软件</h1>
<ol>
<li>JDK安装(<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">下载地址</a>)</li>
<li>ssh安装<br>
hadoop中使用ssh来实现cluster中各个node的登陆认证，同时需要进行ssh免密登陆。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install ssh</span><br></pre></td></tr></table></figure>
</li>
<li>rsync安装<br>
Ubuntu 12.10已经自带rsync。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install rsync</span><br></pre></td></tr></table></figure>
</li>
<li>hadoop下载<br>
从官方<a href="http://www.apache.org/mirrors/#cn">mirrors</a>下载对应版本的hadoop。</li>
</ol>
<h1>安装Hadoop</h1>
<ol>
<li>创建hadoop用户组以及用户 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> addgroup hadoop</span><br><span class="line"><span class="built_in">sudo</span> adduser --ingroup hadoop hadoop</span><br></pre></td></tr></table></figure>
重新用hadoop用户登陆到Linux中。</li>
<li>将hadoop解压到目录<code>/home/hadoop/local/opt</code>中</li>
<li>配置hadoop环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HOME</span>/local/opt/hadoop-2.7.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>
</li>
<li>进入<code>hadoop-2.7.3/etc/hadoop</code>文件夹修改<code>core-site.xml</code>文件 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/local/var/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li>修改<code>hdfs-site.xml</code>文件 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span># 通过web界面来查看HDFS状态 <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/local/var/hadoop/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hadoop/local/var/hadoop/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span># 每个Block有1个备份<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li>修改<code>mapred-site.xml</code><br>
这个是mapreduce任务的配置，由于hadoop2.x使用了yarn框架，所以要实现分布式部署，必须在<code>mapreduce.framework.name</code>属性下配置为yarn。<code>mapred.map.tasks</code>和<code>mapred.reduce.tasks</code>分别为map和reduce的任务数。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li>修改<code>yarn-site.xml</code> <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li>修改<code>slaves</code>文件 <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
</li>
<li>修改<code>hosts</code>文件，命名各个节点的名称。 <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">172.168.170.84 master</span><br><span class="line">172.168.170.88 slave1</span><br><span class="line">172.168.170.89 slave2</span><br></pre></td></tr></table></figure>
</li>
<li>节点之间ssh免密登陆<br>
在<code>master</code>节点中生成密钥，并添加到<code>.ssh/authorized_keys</code>文件中。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="built_in">cat</span> id_rsa.pub&gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure>
将<code>master</code>中的<code>/etc/hosts</code>文件和<code>.ssh/authorized_keys</code>文件发送到slave1和slave2文件中。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp /etc/hosts hadoop@slave1:/etc/hosts</span><br><span class="line">scp /home/hadoop/.ssh/authorized_keys hadoop@slave1:/home/hadoop/.ssh/authorized_keys</span><br><span class="line">scp /home/hadoop/.ssh/authorized_keys hadoop@slave2:/home/hadoop/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
完成之后可以利用以下语句测试免密登陆。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh slave1</span><br><span class="line">ssh slave2</span><br></pre></td></tr></table></figure>
</li>
<li>将<code>hadoop-2.7.3</code>文件拷贝至slave1和slave2<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -r /home/hadoop/local/opt/hadoop-2.7.3 hadoop@slave1:/home/hadoop/local/opt/</span><br><span class="line">scp -r /home/hadoop/local/opt/hadoop-2.7.3 hadoop@slave2:/home/hadoop/local/opt/</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1>启动Hadoop</h1>
<ol>
<li>在master节点使用hadoop用户初始化NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode –format</span><br><span class="line"><span class="comment">#执行后控制台输出，看到 Exiting with status 0 表示格式化成功。</span></span><br><span class="line"><span class="comment">#如有错误，先删除var目录下的临时文件，然后重新运行该命令</span></span><br></pre></td></tr></table></figure>
</li>
<li>启动hadoop <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#启动hdfs</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="comment">#启动yarn分布式计算框架</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li>用jps命令查看hadoop集群运行情况<br>
master节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Jps</span><br><span class="line">NameNode</span><br><span class="line">ResourceManager</span><br><span class="line">SecondaryNameNode</span><br><span class="line">JobHistoryServer</span><br></pre></td></tr></table></figure>
slave节点 <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Jps</span><br><span class="line">DataNode</span><br><span class="line">NodeManager</span><br></pre></td></tr></table></figure>
</li>
<li>通过以下网址查看集群状态 <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">http://172.168.170.84:50070</span><br><span class="line">http://172.168.170.84:8088</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之HDFS架构</title>
    <url>/2017/04/14/2017-04-14-Hadoop%E4%B9%8BHDFS%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<p>HDFS即Hadoop Distributed File System分布式文件系统，它的设计目标是把超大数据集存储到分布在网络中的多台普通商用计算机上，并且能够提供高可靠性和高吞吐量的服务。</p>
<span id="more"></span>
<h1>HDFS架构原理</h1>
<p>HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。</p>
<h2 id="数据块">数据块</h2>
<p>磁盘数据块是磁盘读写的基本单位，与普通文件系统类似，hdfs也会把文件分块来存储。hdfs默认数据块大小为64MB，磁盘块一般为512B，HDFS块为何如此之大呢？块增大可以减少寻址时间与文件传输时间的比例，若寻址时间为10ms，磁盘传输速率为100MB/s，那么寻址与传输比仅为1%。当然，磁盘块太大也不好，因为一个MapReduce通常以一个块作为输入，块过大会导致整体任务数量过小，降低作业处理速度。</p>
<p>hdfs按块存储还有如下好处：</p>
<ol>
<li>文件可以任意大，也不用担心单个结点磁盘容量小于文件的情况</li>
<li>简化了文件子系统的设计，子系统只存储文件块数据，而文件元数据则交由其它系统（NameNode）管理</li>
<li>有利于备份和提高系统可用性，因为可以以块为单位进行备份，hdfs默认备份数量为3。</li>
<li>有利于负载均衡</li>
</ol>
<h2 id="NameNode">NameNode</h2>
<h3 id="NameNode中的元信息">NameNode中的元信息</h3>
<p>当一个客户端请求一个文件或者存储一个文件时，它需要先知道具体到哪个DataNode上存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。</p>
<p>NameNode管理着文件系统命名空间，它维护这文件系统树及树中的所有文件和目录。NameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件数据的保存与操作，都是由DataNode负责。当一个客户端请求数据时，它仅仅是从NameNode中获取文件的元信息，而具体的数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互。</p>
<p>NameNode保存元信息的种类有：</p>
<ul>
<li>文件名目录名及它们之间的层级关系</li>
<li>文件目录的所有者及其权限</li>
<li>每个文件块的名及文件有哪些块组成</li>
</ul>
<p>需要注意的是，<strong>NameNode元信息并不包含每个块的位置信息</strong>，这些信息会在NameNode启动时从各个DataNode获取并保存在内存中，因为这些信息会在系统启动时由数据节点重建。把块位置信息放在内存中，在读取数据时会减少查询时间，增加读取效率。</p>
<h3 id="元信息的持久化">元信息的持久化</h3>
<p>在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个文件edits中。并且edits文件和fsimage文件会被SecondaryNameNode周期性的合并（合并过程会在SecondaryNameNode中详细介绍）。</p>
<h3 id="其它问题">其它问题</h3>
<p>运行NameNode会占用大量内存和I/O资源，一般NameNode不会存储用户数据或执行MapReduce任务。</p>
<p>为了简化系统的设计，Hadoop只有一个NameNode，这也就导致了hadoop集群的单点故障问题。因此，对NameNode节点的容错尤其重要，hadoop提供了如下两种机制来解决：</p>
<ul>
<li>将hadoop元数据写入到本地文件系统的同时再实时同步到一个远程挂载的网络文件系统（NFS）。</li>
<li>运行一个secondaryNameNode，它的作用是与NameNode进行交互，定期通过编辑日志文件合并命名空间镜像，当NameNode发生故障时它会通过自己合并的命名空间镜像副本来恢复。需要注意的是secondaryNameNode保存的状态总是滞后于NameNode，所以这种方式难免会导致丢失部分数据（后面会详细介绍）。</li>
</ul>
<h2 id="SecondaryNameNode">SecondaryNameNode</h2>
<p>需要注意，SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS文件的元信息都保存在NameNode的内存中。在NameNode启动时，它首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在了内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。</p>
<p>Edits文件存在的目的是为了提高系统的操作效率，NameNode在更新内存中的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题而诞生的。</p>
<p>SecondaryNameNode的角色就是定期的合并edits和fsimage文件，我们来看一下合并的步骤：</p>
<p><img src="/imgs/Hadoop/hdfs/copyfsimage.jpg" alt="  Secondary NameNode处理流程 "></p>
<ol>
<li>合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。</li>
<li>SecondaryNameNode从NameNode请求fsimage和edits文件</li>
<li>SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件</li>
<li>NameNode从SecondaryNameNode获取合并好的新的fsimage并将旧的替换掉，并把edits用第一步创建的edits.new文件替换掉</li>
<li>更新fstime文件中的检查点</li>
</ol>
<p>最后再总结一下整个过程中涉及到NameNode中的相关文件</p>
<ul>
<li>fsimage ：保存的是上个检查点的HDFS的元信息</li>
<li>edits ：保存的是从上个检查点开始发生的HDFS元信息状态改变信息</li>
<li>fstime：保存了最后一个检查点的时间戳</li>
</ul>
<h2 id="DataNode">DataNode</h2>
<p>DataNode是HDFS中的worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除、和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。当对hdfs文件系统进行读写时，NameNode告知客户端每个数据驻留在哪个DataNode，客户端直接与DataNode进行通信，DataNode还会与其它DataNode通信，复制这些块以实现冗余。</p>
<h2 id="数据备份">数据备份</h2>
<p>HDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其它所有数据块大小都是一样的。NameNode负责各个数据块的备份，DataNode会通过心跳的方式定期的向NameNode发送自己节点上的Block报告，这个报告中包含了DataNode节点上的所有数据块的列表。</p>
<p>一个大型的HDFS文件系统一般都是需要跨很多机架的，不同机架之间的数据传输需要经过网关，并且，同一个机架中机器之间的带宽要大于不同机架机器之间的带宽。如果把所有的副本都放在不同的机架中，这样既可以防止机架失败导致数据块不可用，又可以在读数据时利用到多个机架的带宽，并且也可以很容易的实现负载均衡。但是，如果是写数据，各个数据块需要同步到不同的机架，会影响到写数据的效率。而在Hadoop中，如果副本数量是3的情况下，Hadoop默认是这么存放的，把第一个副本放到机架的一个节点上，另一个副本放到同一个机架的另一个节点上，把最后一个节点放到不同的机架上。这种策略减少了跨机架副本的个数提高了写的性能，也能够允许一个机架失败的情况，算是一个很好的权衡。</p>
<p><img src="/imgs/Hadoop/hdfs/copylocation.jpg" alt="副本摆放策略 "></p>
<h2 id="安全模式">安全模式</h2>
<p>关于安全模式，当 Hadoop的NameNode节点启动时，会进入安全模式阶段。在此阶段，DataNode会向NameNode上传它们数据块的列表，让 NameNode得到块的位置信息，并对每个文件对应的数据块副本进行统计。当最小副本条件满足时，即一定比例的数据块都达到最小副本数，系统就会退出安全模式，而这需要一定的延迟时间。当最小副本条件未达到要求时，就会对副本数不足的数据块安排DataNode进行复制，直至达到最小副本数。而在安全模式下，系统会处于只读状态，NameNode不会处理任何块的复制和删除命令。</p>
<h2 id="HDFS负载均衡">HDFS负载均衡</h2>
<p>HDFS的数据也许并不是非常均匀的分布在各个DataNode中。一个常见的原因是在现有的集群上经常会增添新的DataNode节点。当新增一个数据块（一个文件的数据被保存在一系列的块中）时，NameNode在选择DataNode接收这个数据块之前，会考虑到很多因素。其中的一些考虑的是：</p>
<ul>
<li>将数据块的一个副本放在正在写这个数据块的节点上。</li>
<li>尽量将数据块的不同副本分布在不同的机架上，这样集群可在完全失去某一机架的情况下还能存活。</li>
<li>一个副本通常被放置在和写文件的节点同一机架的某个节点上，这样可以减少跨越机架的网络I/O。</li>
<li>尽量均匀地将HDFS数据分布在集群的DataNode中。</li>
</ul>
<h1>HDFS健壮性</h1>
<p>HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是： Namenode出错 , Datanode出错和网络割裂(network partitions)。</p>
<h2 id="磁盘数据错误，心跳检测和重新复制">磁盘数据错误，心跳检测和重新复制</h2>
<p>每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟 Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号 Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。</p>
<h2 id="数据完整性">数据完整性</h2>
<p>从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode 获取该数据块的副本。</p>
<h2 id="元数据磁盘错误">元数据磁盘错误</h2>
<p>FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS 实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事 务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当 Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。</p>
<p>Namenode是HDFS集群中的单点故障(single point of failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。</p>
<h1>HDFS网络</h1>
<h2 id="HDFS中的沟通协议">HDFS中的沟通协议</h2>
<p>所有的HDFS中的沟通协议都是基于tcp/ip协议，一个客户端通过指定的tcp端口与NameNode机器建立连接，并通过ClientProtocol协议与NameNode交互。而DataNode则通过DataNode Protocol协议与NameNode进行沟通。HDFS的RCP(远程过程调用)对ClientProtocol和DataNode Protocol做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。</p>
<h2 id="HDFS机架感知与网络拓扑">HDFS机架感知与网络拓扑</h2>
<p>通常，大型Hadoop集群是以机架的形式来组织的，而HDFS不能够自动判断集群中各个datanode的网络拓扑情况，因此Hadoop允许集群的管理员通过配置<code>dfs.network.script</code>参数来确定节点所处的机架。 文件提供了IP-&gt;rackid的翻译。NameNode通过这个得到集群中各个datanode机器的rackid。如果<code>topology.script.file.name</code>没有设定，则每个IP都会翻译 成<code>/default-rack</code>。</p>
<p>Hadoop把网络拓扑看成是一棵树，两个节点的距离=它们到最近共同祖先距离的总和，而树的层次可以这么划分：</p>
<ul>
<li>同一节点中的进程</li>
<li>同一机架上的不同节点</li>
<li>同一数据中心不同机架</li>
<li>不同数据中心的节点</li>
</ul>
<p><img src="/imgs/Hadoop/hdfs/datanodetop.jpg" alt="datanode 网络拓扑图"></p>
<p>若数据中心$d_1$中一个机架$r_1$中一个节点$n_1$表示为$d_1/r_1/n_1$,则：</p>
<p>$$<br>
distance(d_1/r_1/n_1,d_1/r_1/n_1)=0; 相同的datanode\\<br>
distance(d_1/r_1/n_1,d_1/r_1/n_2)=2; 同一rack下的不同datanode\\<br>
distance(d_1/r_1/n_1,d_1/r_2/n_3)=4; 同一IDC下的不同datanode\\<br>
distance(d_1/r_1/n_1,d_2/r_3/n_4)=6; 不同IDC下的datanode<br>
$$</p>
<h1>hdfs文件读写过程剖析</h1>
<p><img src="/imgs/Hadoop/hdfs/hdfs-architecture.gif" alt="NameNode和DataNode架构图"></p>
<h2 id="hdfs文件读取过程">hdfs文件读取过程</h2>
<p>HDFS有一个FileSystem实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。HDFS通过rpc调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回含有该块副本的DataNode的节点地址，另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。</p>
<p>HDFS会返回一个FSDataInputStream对象，FSDataInputStream类转而封装成DFSDataInputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：</p>
<ol>
<li>客户端发起读请求</li>
<li>客户端与NameNode得到文件的块及位置信息列表</li>
<li>客户端直接和DataNode交互读取数据</li>
<li>读取完成关闭连接</li>
</ol>
<p>当FSDataInputStream与DataNode通信时遇到错误，它会选取另一个较近的DataNode，并为出故障的DataNode做标记以免重复向其读取数据。FSDataInputStream还会对读取的数据块进行校验和确认，发现块损坏时也会重新读取并通知NameNode。</p>
<p>这样设计的巧妙之处：</p>
<ol>
<li>让客户端直接联系DataNode检索数据，可以使HDFS扩展到大量的并发客户端，因为数据流就是分散在集群的每个节点上的，在运行MapReduce任务时，每个客户端就是一个DataNode节点。</li>
<li>NameNode仅需相应块的位置信息请求（位置信息在内存中，速度极快），否则随着客户端的增加，NameNode会很快成为瓶颈。</li>
</ol>
<h2 id="HDFS文件写入过程">HDFS文件写入过程</h2>
<p>HDFS有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件。DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。</p>
<p>同读文件过程一样，FSDataOutputStream类转而封装成DFSDataOutputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是：</p>
<ol>
<li>客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件</li>
<li>待临时文件达到块大小时开始向NameNode请求DataNode信息</li>
<li>NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）</li>
<li>客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode</li>
<li>当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见</li>
</ol>
<p>上面第四步描述的flush过程实际处理过程比较负杂，现在单独描述一下：</p>
<ol>
<li>首先，第一个DataNode是以数据包(数据包一般4KB)的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。</li>
<li>在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包</li>
<li>第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点</li>
<li>传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK,最终，第一个DataNode会向客户端发回一个ACK</li>
<li>当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点。然后，客户端会向NameNode发送一个确认</li>
<li>如果管道中的任何一个DataNode失败，管道会被关闭。数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点</li>
<li>数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在hdfs中，供读取时进行完整性校验</li>
</ol>
<h2 id="hdfs文件删除过程">hdfs文件删除过程</h2>
<p>hdfs文件删除过程一般需要如下几步：</p>
<ol>
<li>一开始删除文件，NameNode只是重命名被删除的文件到/trash目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在/trash中文件会被保留一定间隔的时间（可配置，默认是6小时），在这期间，文件可以很容易的恢复，恢复只需要将文件从/trash移出即可。</li>
<li>当指定的时间到达，NameNode将会把文件从命名空间中删除</li>
<li>标记删除的文件块释放空间，HDFS文件系统显示空间增加</li>
</ol>
<h1>HDFS缺点</h1>
<p>一般来说，一条元信息记录会占用200byte内存空间。假设块大小为64MB，备份数量是3 ，那么一个1GB大小的文件将占用1GB/64MB*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。我们可以发现，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。</p>
<p>还有一个问题就是，因为 Map task 的数量是由 splits 来决定的，所以 用 MR 处理大量的小文件时，就会产生过多的 Maptask ，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M ，那就会有10000个Maptasks，会有很大的线程开销；若每个split为 100M，则只有100个Maptasks，每个Maptask 将会有更多的事情做，而线程的管理开销也将减小很多。</p>
<h1>参考文献</h1>
<blockquote>
<p><a href="https://my.oschina.net/leejun2005/blog/151872">HDFS 原理、架构与特性介绍</a><br>
<a href="http://blog.csdn.net/suifeng3051/article/details/48548341">Hadoop核心之HDFS 架构设计</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之Yarn原理</title>
    <url>/2017/04/17/2017-04-17-Hadoop%E4%B9%8BYarn%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。</p>
<span id="more"></span>
<h1>Hadoop MapReduce 框架的问题</h1>
<p>随着分布式系统集群的规模和其工作负荷的增长，MapReduce 框架的问题逐渐浮出水面，主要的问题集中如下：</p>
<ol>
<li>JobTracker 是 Map-reduce 的集中处理点，存在单点故障。</li>
<li>JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM(Out-Of-Memory)。</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
<li>源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。</li>
<li>从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。</li>
</ol>
<h1>Yarn介绍</h1>
<p>Yarn的核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。</p>
<p>YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等) 部署到一个公共集群中，共享集群的资源。并提供如下功能：</p>
<ol>
<li>资源的统一管理和调度：<br>
集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。</li>
<li>资源隔离：<br>
YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</li>
</ol>
<p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。</p>
<p>YARN可以看成一个云操作系统，由一个ResourceManager和多个NodeManager组成， 它负责管理所有NodeManger上多维度资源， 并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑) 或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。</p>
<h1>YARN体系架构</h1>
<p>YARN架构如下图所示：</p>
<p><img src="/imgs/Hadoop/yarn/architecture.png" alt="YARN架构图"></p>
<p>YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。</p>
<ul>
<li>
<p><strong>ResourceManager(RM)</strong><br>
负责对各NodeManager上的资源进行统一管理和调度。将ApplicationMaster分配空闲的Container运行并监控其运行状态。对ApplicationMaster申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器：</p>
<ol>
<li><strong>调度器(Scheduler)</strong>：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Scheduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。</li>
<li><strong>应用程序管理器(Applications Manager)</strong>：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li>
</ol>
</li>
<li>
<p><strong>NodeManager (NM)</strong><br>
NodeManager是每个节点上的资源和任务管理器。它会定时地向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自ApplicationMaster的Container 启动/停止等请求。</p>
</li>
<li>
<p><strong>ApplicationMaster (AM)</strong><br>
用户提交的应用程序均包含一个ApplicationMaster，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。</p>
</li>
<li>
<p><strong>Container</strong><br>
Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</p>
</li>
</ul>
<h1>YARN应用工作流程</h1>
<p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p>
<ol>
<li>启动ApplicationMaster，如下步骤1~3；</li>
<li>由ApplicationMaster创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li>
</ol>
<p><img src="/imgs/Hadoop/yarn/yarn.png" alt="YARN应用工作流程图"></p>
<ol>
<li>用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</li>
<li>ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动ApplicationMaster；</li>
<li>ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager査看应用程序的运行状态，运行状态通过 <code>AMRMClientAsync.CallbackHandler</code>的<code>getProgress()</code> 方法来传递给ResourceManager。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</li>
<li>ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源；资源的协调通过<code>AMRMClientAsync</code>异步完成,相应的处理方法封装在<code>AMRMClientAsync.CallbackHandler</code>中。</li>
<li>一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务；通常需要指定一个<code>ContainerLaunchContext</code>，提供Container启动时需要的信息。</li>
<li>NodeManager为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NodeManager的通信通过<code>NMClientAsync object</code>来完成，容器的所有事件通过<code>NMClientAsync.CallbackHandler</code>来处理。例如启动、状态更新、停止等。</li>
<li>应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</li>
</ol>
<h1>YARN资源调度模型</h1>
<p>YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。</p>
<p>YARN采用了双层资源调度模型：</p>
<ul>
<li>ResourceManager中的资源调度器将资源分配给各个ApplicationMaster：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的ApplicationMaster，而是暂时放到一个缓冲区中，等待ApplicationMaster通过周期性的心跳主动来取；</li>
<li>ApplicationMaster领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。</li>
</ul>
<p>也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。</p>
<p>YARN目前采用的资源分配算法有三种。但真实的调度器实现中还对算法做了一定程度的优化。</p>
<ol>
<li>Capacity Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。<br>
该调度器保证共享集群的各个组织能够得到容量的保证，同时可以超额使用集群中暂时没有人使用的资源。Capacity Scheduler为了实现这些目标，抽象了queue的概念，queue通常由管理员配置。为了进一步细分容量的使用，调度器支持层级化的queue（hierarchical queues），使得在特定组织内部，可以进一步有效利用集群资源。<br>
Capacity调度器支持的一些特性如下：
<ul>
<li>层级队列（Hierarchical Queues）</li>
<li>容量保证</li>
<li>安全性：每个队列都有队列的访问权限控制（ACL）</li>
<li>弹性： 空闲资源可以额外分配给任何需要的队列</li>
<li>多租户</li>
<li>基于资源的调度（resouce-based scheduling): 对资源敏感的应用程序，可以有效地控制资源情况</li>
<li>支持用户（组）到queue的映射：基于用户组提交作业到对应queue。</li>
<li>运营支持：支持运行时配置队列的容量，ACL等。也可以在运行时停止queue阻止进一步往queue提交作业。</li>
</ul>
</li>
<li>Fair Scheduler：公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。<br>
这种策略使得运行时间短的应用能够尽快结束，而不至于在等待资源时被饿死。另外，也可以为应用程序配置优先级，优先级用于决定资源使用量的占比。</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<p><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/">Hadoop 新MapReduce 框架Yarn 详解</a><br>
<a href="http://blog.csdn.net/bingduanlbd/article/details/51880019">理解Hadoop YARN架构</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>纵表、横表互转</title>
    <url>/2017/12/03/2017-12-03-%E7%BA%B5%E8%A1%A8%E3%80%81%E6%A8%AA%E8%A1%A8%E4%BA%92%E8%BD%AC/</url>
    <content><![CDATA[<p>mysql中横表和纵表的互换。</p>
<span id="more"></span>
<p>将纵表中的多行记录转化成横表中的一条记录。</p>
<h2 id="1、-建表">1、 建表</h2>
<p>纵表结构：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> Table_A</span><br><span class="line">(</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    course <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    score <span class="type">int</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;chinese&#x27;</span>,<span class="number">60</span>);</span><br><span class="line"><span class="keyword">insert into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;math&#x27;</span>,<span class="number">70</span>);</span><br><span class="line"><span class="keyword">insert into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="string">&#x27;english&#x27;</span>,<span class="number">80</span>);</span><br><span class="line"><span class="keyword">insert into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;chinese&#x27;</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert into</span> Table_A(name,course,score) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="string">&#x27;math&#x27;</span>,<span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<p>横表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> Table_B</span><br><span class="line">(</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    chinese <span class="type">int</span>,</span><br><span class="line">    math <span class="type">int</span>,</span><br><span class="line">    english <span class="type">int</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert into</span> Table_B(name,chinese,math,english) <span class="keyword">values</span>(<span class="string">&#x27;zhangsan&#x27;</span>,<span class="number">60</span>,<span class="number">70</span>,<span class="number">80</span>);</span><br><span class="line"><span class="keyword">insert into</span> Table_B(name,chinese,math,english) <span class="keyword">values</span>(<span class="string">&#x27;lisi&#x27;</span>,<span class="number">90</span>,<span class="number">100</span>,<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h2 id="2、纵表变横表">2、纵表变横表</h2>
<p>方法一：聚合函数[max或sum]配合case语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;chinese&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> chinese,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;math&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> math,</span><br><span class="line"><span class="built_in">sum</span> (<span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">&#x27;english&#x27;</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> english</span><br><span class="line"><span class="keyword">from</span> Table_A</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure>
<p>方法二：使用pivot</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Table_A pivot (<span class="built_in">max</span>(score)<span class="keyword">for</span> course <span class="keyword">in</span>(chinese,math,english)) tmp_table;</span><br></pre></td></tr></table></figure>
<h2 id="3、横表变纵表">3、横表变纵表</h2>
<p>方法一：union all</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;chinese&#x27;</span> <span class="keyword">as</span> course,chinese <span class="keyword">as</span> score <span class="keyword">from</span> Table_B <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;math&#x27;</span> <span class="keyword">as</span> course,math <span class="keyword">as</span> score <span class="keyword">from</span> Table_B <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> name,<span class="string">&#x27;english&#x27;</span> <span class="keyword">as</span> course,english <span class="keyword">as</span> score <span class="keyword">from</span> Table_B</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name,course <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<p>方法二：使用unpivot</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,course,score <span class="keyword">from</span> Table_B</span><br><span class="line">unpivot</span><br><span class="line">(score <span class="keyword">for</span> course <span class="keyword">in</span> ([chinese],[math],english)) tmp_table;</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<blockquote>
<p><a href="http://www.cnblogs.com/liushen/p/3333936.html">纵表、横表互转的SQL</a></p>
</blockquote>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之MapReduce原理</title>
    <url>/2017/04/17/2017-04-17-Hadoop%E4%B9%8BMapReduce%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>
<span id="more"></span>
<h1>概述</h1>
<p>一个Map/Reduce作业（job）通常会把输入的数据集切分为若干独立的数据块，由map任务（task）以完全并行的方式处理它们。框架会对map的输出先进行排序，然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>
<p>通常，Map/Reduce框架和分布式文件系统是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。</p>
<p>Map/Reduce框架由一个单独的master JobTracker和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p>
<h1>MapReduce的架构</h1>
<p>和HDFS一样，MapReduce也是采用Master/Slave的架构，其架构图如下所示。</p>
<p><img src="/imgs/Hadoop/mapreduce/architecture.jpg" alt="MapReduce架构图"></p>
<p>MapReduce包含四个组成部分，分别为Client、JobTracker、TaskTracker和Task，下面我们详细介绍这四个组成部分。</p>
<h2 id="Client-客户端">Client 客户端</h2>
<p>每一个Job 都会在用户端通过Client类将应用程序以及配置参数Configuration打包成JAR文件存储在HDFS，并把路径提交到JobTracker的master服务，然后由master创建每一个Task（即MapTask 和ReduceTask）将它们分发到各个TaskTracker服务中去执行。</p>
<h2 id="JobTracker">JobTracker</h2>
<p>JobTracke负责资源监控和作业调度。JobTracker监控所有TaskTracker与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。</p>
<h2 id="TaskTracker">TaskTracker</h2>
<p>TaskTracker会周期性地通过Heartbeat将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用&quot;slot&quot;等量划分本节点上的资源量。&quot;slot&quot;代表计算资源（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别供Map Task和Reduce Task使用。TaskTracker通过slot数目（可配置参数）限定Task的并发度。</p>
<h2 id="Task">Task</h2>
<p>Task 分为Map Task和Reduce Task两种，均由TaskTracker启动。HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单位是split。</p>
<p>Map Task执行过程如下图所示：由该图可知，Map Task先将对应的split迭代解析成一个个key/value 对，依次调用用户自定义的map()函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition将被一个Reduce Task处理。</p>
<p><img src="/imgs/Hadoop/mapreduce/maptask.jpg" alt="Map Task执行过程"></p>
<p>Reduce Task执行过程下图所示。该过程分为三个阶段：</p>
<p><img src="/imgs/Hadoop/mapreduce/reducetask.jpg" alt="Reduce Task执行过程"></p>
<ol>
<li>从远程节点上读取Map Task 中间结果（称为“Shuffle 阶段”）；</li>
<li>按照key 对key/value 对进行排序（称为“Sort 阶段”）；</li>
<li>依次读取&lt; key, value list&gt;，调用用户自定义的reduce() 函数处理，并将最终结果存到HDFS 上（称为“Reduce 阶段”）。</li>
</ol>
<h1>MapReduce运行机制</h1>
<p>下面从逻辑实体的角度介绍mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。</p>
<p><img src="/imgs/Hadoop/mapreduce/mrjob.png" alt="MapReduce作业运行流程"></p>
<p>在MapReduce运行过程中，最重要的是Map，Shuffle，Reduce三个阶段，各个阶段的作用简述如下：</p>
<ul>
<li>Map:数据输入,做初步的处理,输出形式的中间结果；</li>
<li>Shuffle:按照partition、key对中间结果进行排序合并,输出给reduce线程；</li>
<li>Reduce:对相同key的输入进行最终的处理,并将结果写入到文件中。</li>
</ul>
<p><img src="/imgs/Hadoop/mapreduce/shuffle.png" alt="MapReduce Shuffle过程"></p>
<p>上图是把MapReduce过程分为两个部分，而实际上从两边的Map和Reduce到中间的那一大块都属于Shuffle过程，也就是说，Shuffle过程有一部分是在Map端，有一部分是在Reduce端。</p>
<h2 id="输入分片（input-split）">输入分片（input split）</h2>
<h3 id="InputFormat">InputFormat</h3>
<p>InputFormat为Map/Reduce作业描述输入的细节规范。Map/Reduce框架根据作业的InputFormat来进行以下操作：</p>
<ul>
<li>检查作业输入的有效性。</li>
<li>把输入文件切分成多个逻辑InputSplit实例，并把每一实例分别分发给一个Mapper。</li>
<li>提供RecordReader的实现，这个RecordReader从逻辑InputSplit中获得输入记录，这些记录将由Mapper处理。</li>
</ul>
<p>InputFormat只包含了两个接口函数:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">InputSplit[] getSplits(JobConf job, <span class="type">int</span> numSplits) <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">RecordReader &lt; K, V&gt; getRecordReader(InputSplit split, JobConf job, Reporter reporter) <span class="keyword">throws</span> IOException;</span><br></pre></td></tr></table></figure>
<p>getSplits就是现在要使用的划分函数。job参数是任务的配置集合，从中可以取到用户在启动MapReduce时指定的输入文件路径。而numSplits参数是一个Split数目的建议值，是否考虑这个值，由具体的InputFormat实现来决定。返回的是InputSplit数组，它描述了所有的Split信息，一个InputSplit描述一个Split。</p>
<p>getRecordReader方法返回一个RecordReader对象，该对象将输入的InputSplit解析成若干个key/value对，MapReduce框架在Map Task执行过程中，会不断的调用RecordReader对象中的方法，迭代获取key/value对并交给map函数处理。</p>
<h3 id="InputSplit">InputSplit</h3>
<p>InputSplit是一个单独的Mapper要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。InputSplit也只有两个接口函数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">long</span> <span class="title function_">getLength</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line"></span><br><span class="line">String[] getLocations() <span class="keyword">throws</span> IOException;</span><br></pre></td></tr></table></figure>
<p>这个interface仅仅描述了Split有多长，以及存放这个Split的Location信息（也就是这个Split在HDFS上存放的机器。它可能有多个replication，存在于多台机器上）。除此之外，就再没有任何直接描述Split的信息了。</p>
<p>而Split中真正重要的描述信息还是只有InputFormat会关心。在需要读取一个Split的时候，其对应的InputSplit会被传递到InputFormat的第二个接口函数getRecordReader，然后被用于初始化一个RecordReader，以解析输入数据。</p>
<p>在分配Map任务时，Split的Location信息就要发挥作用了。JobTracker会根据TaskTracker的地址来选择一个Location与之最接近的Split所对应的Map任务（注意一个Split可以有多个Location）。这样一来，输入文件中Block的Location信息经过一系列的整合（by InputFormat）和传递，最终就影响到了Map任务的分配。其结果是Map任务倾向于处理存放在本地的数据，以保证效率。</p>
<h3 id="RecordReader">RecordReader</h3>
<p>RecordReader从InputSlit读入&lt;key, value&gt;对。</p>
<p>一般的，RecordReader 把由InputSplit 提供的字节样式的输入文件，转化成由Mapper处理的记录样式的文件。因此RecordReader负责处理记录的边界情况和把数据表示成&lt;keys, values&gt;对形式。</p>
<h2 id="Map阶段">Map阶段</h2>
<p>在进行海量数据处理时，外存文件数据I/O访问会成为一个制约系统性能的瓶颈，因此，Hadoop的Map过程实现的一个重要原则就是：计算靠近数据，这里主要指两个方面：</p>
<ol>
<li>代码靠近数据：
<ul>
<li>原则：本地化数据处理（locality），即一个计算节点尽可能处理本地磁盘上所存储的数据；</li>
<li>尽量选择数据所在DataNode启动Map任务；</li>
<li>这样可以减少数据通信，提高计算效率；</li>
</ul>
</li>
<li>数据靠近代码：
<ul>
<li>当本地没有数据处理时，尽可能从同一机架或最近其他节点传输数据进行处理（host选择算法）。</li>
</ul>
</li>
</ol>
<p>map的经典流程图如下：</p>
<p><img src="/imgs/Hadoop/mapreduce/map-shuffle.png" alt="Map Shuffle过程"></p>
<h3 id="输入">输入</h3>
<ol>
<li>map task只读取split分片，split与block（HDFS的最小存储单位，默认为64MB）可能是一对一也能是一对多，但是对于一个split只会对应一个文件的一个block或多个block，不允许一个split对应多个文件的多个block；</li>
<li>这里切分和输入数据的时会涉及到InputFormat的文件切分算法和host选择算法。</li>
</ol>
<p>文件切分算法，主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。FileInputFormat以文件为单位切分生成InputSplit，对于每个文件，由以下三个属性值决定其对应的InputSplit的个数：</p>
<ul>
<li>goalSize： 它是根据用户期望的InputSplit数目计算出来的，即totalSize/numSplits。其中，totalSize为文件的总大小；numSplits为用户设定的Map Task个数，默认情况下是1；</li>
<li>minSize：InputSplit的最小值，由配置参数mapred.min.split.size确定，默认是1；</li>
<li>blockSize：文件在hdfs中存储的block大小，不同文件可能不同，默认是64MB。</li>
</ul>
<p>这三个参数共同决定InputSplit的最终大小，计算方法如下：<br>
$$<br>
splitSize=\max(minSize, \min(gogalSize,blockSize))<br>
$$</p>
<h3 id="Partitioner">Partitioner</h3>
<p>作用：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。</p>
<p>实现功能：</p>
<ol>
<li>map输出的是&lt;key,value&gt;对，决定于当前的mapper的partition交给哪个reduce的方法是：mapreduce提供的Partitioner接口，对key进行hash后，再以reducetask数量取模，然后到指定的job上（HashPartitioner，可以通过<code>job.setPartitionerClass(MyPartition.class)</code>自定义）。</li>
<li>然后将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。&lt;key,value&gt;对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组。</li>
</ol>
<p>要求：负载均衡，效率；</p>
<h3 id="spill（溢写）">spill（溢写）</h3>
<p>作用：把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）；</p>
<p>注意：</p>
<ol>
<li>这个spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区（两个指针的方向不会变，下面会详述）；</li>
<li>在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，这个就是sort操作，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
</ol>
<h3 id="Combiner">Combiner</h3>
<p>combine：执行combine操作要求开发者必须在程序中设置了combine（程序中通过<code>job.setCombinerClass(myCombine.class)</code>自定义combine操作）。</p>
<p>程序中有两个阶段可能会执行combine操作：</p>
<ol>
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性<code>min.num.spills.for.combine</code>配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
<p>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</p>
<p>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：</p>
<ol>
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ol>
<h3 id="merge">merge</h3>
<p>当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。</p>
<p>注意：</p>
<ol>
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性<code>min.num.spills.for.combine</code>配置；</li>
<li>多个溢出文件合并时，会进行一次排序，排序算法是<strong>多路归并排序</strong>；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做<code>file.out.index</code>。</li>
</ol>
<h3 id="内存缓冲区">内存缓冲区</h3>
<p>在Map Task任务的业务处理方法map()中，最后一步通过<code>OutputCollector.collect(key,value)</code>或<code>context.write(key,value)</code>输出Map Task的中间处理结果，在相关的<code>collect(key,value)</code>方法中，会调用<code>Partitioner.getPartition(K2 key, V2 value, int numPartitions)</code>方法获得输出的&lt;key,value&gt;对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数<code>io.sort.mb</code>来调整其大小。</p>
<p>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的Linux本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件。</p>
<p>缓存有一个阀值比例配置，当达到整个缓存的这个比例时，会触发spill操作；触发时，map输出还会接着往剩下的空间写入，但是写满的空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区；</p>
<h2 id="Reduce阶段">Reduce阶段</h2>
<p>Reduce过程的经典流程图如下：</p>
<p><img src="/imgs/Hadoop/mapreduce/reduce-shuffle.png" alt="Redece过程流程图"></p>
<h3 id="copy">copy</h3>
<p>作用：拉取数据；</p>
<p>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为这时map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</p>
<p>默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的5%后，JobTracker便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动<code>mapred.reduce.parallel.copies</code>(默认为5）个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p>
<h3 id="merge-2">merge</h3>
<p>Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。</p>
<p>这里需要强调的是，merge有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</p>
<p>在远程copy数据的同时，Reduce Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</p>
<h3 id="Reduce">Reduce</h3>
<p>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</p>
<p>Reduce的数目建议是0.95或1.75乘以 (<code>&lt;no. of nodes&gt; * mapred.tasktracker.reduce.tasks.maximum</code>)。</p>
<p>用0.95，所有reduce可以在maps一完成时就立刻启动，开始传输map的输出结果。用1.75，速度快的节点可以在完成第一轮reduce任务后，可以开始第二轮，这样可以得到比较好的负载均衡的效果。</p>
<p>如果没有归约要进行，那么设置reduce任务的数目为零是合法的。这种情况下，map任务的输出会直接被写入由 setOutputPath(Path)指定的输出路径。框架在把它们写入FileSystem之前没有对它们进行排序。</p>
<h1>参考文献</h1>
<blockquote>
<p><a href="http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html">Hadoop Map/Reduce教程</a><br>
<a href="http://blog.csdn.net/u010330043/article/details/51200712">深入理解MapReduce的架构及原理</a><br>
<a href="http://blog.csdn.net/hsuxu/article/details/7673171/">Hadoop InputFormat浅析–hadoop如何分配输入</a><br>
<a href="http://wangzzu.github.io/2016/03/02/hadoop-shuffle/">MapReduce之Shuffle过程详述</a></p>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>Map Reduce</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql入门</title>
    <url>/2017/12/04/2017-12-04-mysql%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>本文记录mysql的一些常用命令。</p>
<span id="more"></span>
<ol>
<li>
<p>启动与停止mysql</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">net <span class="keyword">start</span> mysql</span><br><span class="line">net stop mysql</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建一个名称为mydb1的数据库</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb1;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>显示所有数据库</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建一个使用utf-8字符集的mydb2数据库。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb2 <span class="keyword">character set</span> utf8;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建一个使用utf-8字符集，并带校对规则的mydb3数据库</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb3 <span class="keyword">character set</span> utf8 <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查看前面创建的mydb2数据库的定义信息</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> database mydb2;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>删除前面创建的mydb1数据库</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> database mydb1;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查看服务器中的数据库，并把mydb2库的字符集修改为gb2312;</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> database mydb2 <span class="keyword">character set</span> gb2312;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>备份mydb3库中的数据，并恢复</p>
<p>备份（退到window命令行窗口）：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysqldump <span class="operator">-</span>u root <span class="operator">-</span>p mydb3<span class="operator">&gt;</span>c:\test.sql</span><br></pre></td></tr></table></figure>
<p>恢复：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database mydb3;</span><br><span class="line">use mydb3;</span><br><span class="line">source c:\test.sql</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建一个员工表</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> employee</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    sex <span class="type">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    birthday <span class="type">date</span>,</span><br><span class="line">    entry_date <span class="type">date</span>,</span><br><span class="line">    job <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span>,</span><br><span class="line">    resume text</span><br><span class="line">)<span class="keyword">character set</span> utf8 <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在上面员工表的基本上增加一个image列。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> employee <span class="keyword">add</span> image <span class="type">blob</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查看表</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">desc</span> employee;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改job列，使其长度为60。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> employee modify job <span class="type">varchar</span>(<span class="number">60</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>删除sex列</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> employee <span class="keyword">drop</span> sex;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>表名改为user</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rename <span class="keyword">table</span> employee <span class="keyword">to</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改表的字符集为utf-8</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> <span class="keyword">user</span> <span class="keyword">character set</span> utf8;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查看表的字符集</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create table</span> <span class="keyword">user</span>;(表的创建语句)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>列名name修改为username</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> <span class="keyword">user</span> change <span class="keyword">column</span> name username <span class="type">varchar</span>(<span class="number">20</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用insert语句向表中插入三个员工的信息。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rename <span class="keyword">table</span> <span class="keyword">user</span> <span class="keyword">to</span> employee;</span><br><span class="line"><span class="keyword">insert into</span> employee(id,username,birthday,entry_date,job,salary,resume) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;aaa&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="number">90</span>,<span class="string">&#x27;aaaa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert into</span> employee(id,username,birthday,entry_date,job,salary,resume) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;1980-09-09&#x27;</span>,<span class="string">&#x27;bbb&#x27;</span>,<span class="number">90</span>,<span class="string">&#x27;aaaa&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>插入中文数据</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert into</span> employee(id,username) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;小李子&#x27;</span>);</span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;chara%&#x27;</span>;</span><br><span class="line"><span class="keyword">set</span> character_set_cilent<span class="operator">=</span>gb2312;</span><br><span class="line"><span class="keyword">insert into</span> employee(id,username) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;小李子&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询时如果发生乱码：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> character_set_results<span class="operator">=</span>gb2312;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employee;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>将所有员工薪水修改为5000元。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">5000</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>将姓名为’aaa’的员工薪水修改为3000元。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">3000</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>将姓名为’aaa’的员工薪水修改为4000元,job改为ccc。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> employee <span class="keyword">set</span> salary<span class="operator">=</span><span class="number">4000</span>,job<span class="operator">=</span><span class="string">&#x27;ccc&#x27;</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>将aaa的薪水在原有基础上增加1000元。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> employee <span class="keyword">set</span> salary<span class="operator">=</span>salary<span class="operator">+</span><span class="number">1000</span> <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>删除表中名称为’zs’的记录</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> employee <span class="keyword">where</span> username<span class="operator">=</span><span class="string">&#x27;aaa&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>删除表中所有记录。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> employee;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用truncate删除表中记录</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> employee</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>执行sql脚本</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">source c:\student.sql</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询表中所有学生的信息。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询表中所有学生的姓名和对应的英语成绩。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,english <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>过滤表中重复数据。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> english <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在所有学生总分上加10分特长分。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用别名表示学生分数。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">as</span> 姓名,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> <span class="keyword">as</span> 总分 <span class="keyword">from</span> student;</span><br><span class="line"><span class="keyword">select</span> name 姓名,(english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">+</span><span class="number">10</span> 总分 <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询姓名为wu的学生成绩</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;王五&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询英语成绩大于90分的同学</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english<span class="operator">&gt;</span><span class="number">90</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询总分大于200分的所有同学</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> (english<span class="operator">+</span>chinese<span class="operator">+</span>math)<span class="operator">&gt;</span><span class="number">200</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询英语分数在 80－90之间的同学</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english<span class="operator">&gt;</span><span class="number">80</span> <span class="keyword">and</span> english<span class="operator">&lt;</span><span class="number">90</span>;</span><br><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> english <span class="keyword">between</span> <span class="number">80</span> <span class="keyword">and</span> <span class="number">90</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询数学分数为89,90,91的同学</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">=</span><span class="number">80</span> <span class="keyword">or</span> math<span class="operator">=</span><span class="number">90</span> <span class="keyword">or</span> math<span class="operator">=</span><span class="number">91</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> math <span class="keyword">in</span>(<span class="number">80</span>,<span class="number">90</span>,<span class="number">91</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询所有姓李的学生成绩。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="keyword">like</span> <span class="string">&#x27;李%&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询数学分&gt;80，语文分&gt;80的同学。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">&gt;</span><span class="number">80</span> <span class="keyword">and</span> chinese<span class="operator">&gt;</span><span class="number">80</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>对数学成绩排序后输出</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">order</span> <span class="keyword">by</span> math;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>对总分排序后输出，然后再按从高到低的顺序输出</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">order</span> <span class="keyword">by</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>对姓李的学生成绩排序输出</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> name <span class="keyword">like</span> <span class="string">&#x27;李%&#x27;</span> <span class="keyword">order</span> <span class="keyword">by</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计一个班级共有多少学生？</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计数学成绩大于90的学生有多少个？</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student <span class="keyword">where</span> math<span class="operator">&gt;</span><span class="number">90</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计总分大于250的人数有多少？</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> student <span class="keyword">where</span> (math<span class="operator">+</span>english<span class="operator">+</span>chinese)<span class="operator">&gt;</span><span class="number">250</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计一个班级数学总成绩？</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计一个班级语文、英语、数学各科的总成绩</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese),<span class="built_in">sum</span>(english),<span class="built_in">sum</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计一个班级语文、英语、数学的成绩总和</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese<span class="operator">+</span>math<span class="operator">+</span>english) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>统计一个班级语文成绩平均分</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(chinese)<span class="operator">/</span><span class="built_in">count</span>(chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>求一个班级数学平均分？</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(math) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>求一个班级总分平均分</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">avg</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>求班级最高分和最低分</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">max</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese),<span class="built_in">min</span>(math<span class="operator">+</span>english<span class="operator">+</span>chinese) <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>第6种形式的select:</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> orders(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    product <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">    price <span class="type">float</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">insert into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;电视&#x27;</span>,<span class="number">900</span>);</span><br><span class="line"><span class="keyword">insert into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;洗衣机&#x27;</span>,<span class="number">100</span>);</span><br><span class="line"><span class="keyword">insert into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">&#x27;洗衣粉&#x27;</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">&#x27;桔子&#x27;</span>,<span class="number">9</span>);</span><br><span class="line"><span class="keyword">insert into</span> orders(id,product,price) <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">&#x27;洗衣粉&#x27;</span>,<span class="number">90</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>对订单表中商品归类后，显示每一类商品的总价</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product,<span class="built_in">sum</span>(price) <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>查询购买了几类商品，并且每类总价大于100的商品</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product <span class="keyword">where</span> <span class="built_in">sum</span>(price)<span class="operator">&gt;</span><span class="number">100</span>;×</span><br><span class="line"><span class="keyword">select</span> product <span class="keyword">from</span> orders <span class="keyword">group</span> <span class="keyword">by</span> product <span class="keyword">having</span> <span class="built_in">sum</span>(price)<span class="operator">&gt;</span><span class="number">100</span>;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>主键约束</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">	<span class="keyword">create table</span> test1</span><br><span class="line">	(</span><br><span class="line">	    id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">	    name <span class="type">varchar</span>(<span class="number">20</span>)</span><br><span class="line">	);</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 主键自动增长：</span><br><span class="line"></span><br><span class="line">	```<span class="keyword">sql</span></span><br><span class="line">	<span class="keyword">create table</span> test2</span><br><span class="line">	(</span><br><span class="line">	    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">	    name <span class="type">varchar</span>(<span class="number">20</span>)</span><br><span class="line">	 );</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>唯一约束和非空约束</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> <span class="keyword">user</span></span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    username <span class="type">varchar</span>(<span class="number">40</span>) <span class="keyword">not null</span> <span class="keyword">unique</span>,</span><br><span class="line">    password <span class="type">varchar</span>(<span class="number">40</span>) <span class="keyword">not null</span>,</span><br><span class="line">    email <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not null</span> <span class="keyword">unique</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>外键约束</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> male</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">40</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> female</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">40</span>),</span><br><span class="line">    male_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">constraint</span> male_id_FK <span class="keyword">foreign key</span>(male_id) <span class="keyword">references</span> male(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建部门表和员工表(一对多或多对一)</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> department</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> employee</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span>,</span><br><span class="line">    department_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">constraint</span> department_id_FK <span class="keyword">foreign key</span>(department_id)<span class="keyword">references</span> department(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建学生、老师表（多对多）</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> teacher</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    salary <span class="keyword">double</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> student</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> teacher_student</span><br><span class="line">(</span><br><span class="line">    teacher_id <span class="type">int</span>,</span><br><span class="line">    student_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">primary key</span>(teacher_id,student_id),</span><br><span class="line">    <span class="keyword">constraint</span> teacher_id_FK <span class="keyword">foreign key</span>(teacher_id) <span class="keyword">references</span> teacher(id),</span><br><span class="line">    <span class="keyword">constraint</span> student_id_FK <span class="keyword">foreign key</span>(student_id) <span class="keyword">references</span> student(id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert into</span> teacher(name,salary) <span class="keyword">values</span>(<span class="string">&#x27;老王&#x27;</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert into</span> teacher(name,salary) <span class="keyword">values</span>(<span class="string">&#x27;老李&#x27;</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;aaa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;bbb&#x27;</span>);</span><br><span class="line"><span class="keyword">insert into</span> student(name) <span class="keyword">values</span>(<span class="string">&#x27;ccc&#x27;</span>);</span><br><span class="line"><span class="keyword">insert into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line"><span class="keyword">insert into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">insert into</span> teacher_student(teacher_id,student_id) <span class="keyword">values</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>已知老师的id为1，查询出1号老师所有的学生</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> s.<span class="operator">*</span> <span class="keyword">from</span> teacher_student t_s,student s <span class="keyword">where</span> teacher_id<span class="operator">=</span><span class="number">1</span> <span class="keyword">and</span> t_s.student_id<span class="operator">=</span>s.id;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>1号学生有几个老师</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> teacher.<span class="operator">*</span> <span class="keyword">from</span> teacher_student,teacher <span class="keyword">where</span> student_id<span class="operator">=</span><span class="number">1</span> <span class="keyword">and</span> teacher_student.teacher_id<span class="operator">=</span>teacher.id;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建人、身份证表（一对一）</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> person</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span> auto_increment,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">30</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> idcard</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    num <span class="type">varchar</span>(<span class="number">30</span>) <span class="keyword">not null</span> <span class="keyword">unique</span>,</span><br><span class="line">    <span class="keyword">constraint</span> person_id_FK <span class="keyword">foreign key</span>(id) <span class="keyword">references</span> person(id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>数据分页</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> news limits <span class="number">0</span>,<span class="number">20</span>;第<span class="number">0</span>个位置取<span class="number">20</span>条</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>当数据库是自动生成主键时，可以使用如下的语句：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">st <span class="operator">=</span> con.prepareStatement(<span class="keyword">sql</span>,Statement.RETURN_GENERATED_KEYS);</span><br><span class="line">st.executeUpdate();</span><br><span class="line">rs <span class="operator">=</span> st.getGeneratedKeys();</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>事物：逻辑上的一组操作，要么全部成功，要么全部都不成功。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">	<span class="keyword">start</span> transaction 开启事物</span><br><span class="line">	<span class="keyword">Rollback</span> 回滚事物</span><br><span class="line">	<span class="keyword">Commit</span>提交事物</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> JDBC控制事物语句</span><br><span class="line"></span><br><span class="line">	```<span class="keyword">sql</span></span><br><span class="line">	conn.setAutoCommit(<span class="literal">false</span>);</span><br><span class="line">	conn.commit();</span><br><span class="line">	conn.rollback();</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>事物设置回滚点</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sp <span class="operator">=</span> conn.setSavepoint();</span><br><span class="line">conn.rollback(sp);</span><br><span class="line">conn.commit(); 回滚了也要提交事物</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>上海天壤智能科技有限公司</title>
    <url>/2020/07/19/%E4%B8%8A%E6%B5%B7%E5%A4%A9%E5%A3%A4%E6%99%BA%E8%83%BD%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="a637393c06c0affca6a9628298a4f7090da978b82f422d649b06f8d2feb6fd91">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>Map Reduce</tag>
        <tag>数据倾斜</tag>
        <tag>XGBoost</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库索引</title>
    <url>/2017/12/05/2017-12-05-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<p>本文介绍数据库索隐的基本结构、聚集索引与非聚集索引等问题。</p>
<span id="more"></span>
<h1>B-Tree</h1>
<p>我们常见的数据库系统，其索引使用的数据结构多是B-Tree或者B+Tree。例如，MsSql使用的是B+Tree，Oracle及Sysbase使用的是B-Tree。所以在最开始，简单地介绍一下B-Tree。</p>
<p>B-Tree不同于Binary Tree（二叉树，最多有两个子树），一棵M阶的B-Tree满足以下条件：</p>
<pre><code>1）每个结点至多有M个孩子；
2）除根结点和叶结点外，其它每个结点至少有M/2个孩子；
3）根结点至少有两个孩子（除非该树仅包含一个结点）；
4）所有叶结点在同一层，叶结点不包含任何关键字信息；
5）有K个关键字的非叶结点恰好包含K+1个孩子；
</code></pre>
<p>另外，对于一个结点，其内部的关键字是从小到大排序的。以下是B-Tree（M=4）的样例：</p>
<img title="B-Tree样例" src="/imgs/sql/index/b-tree.png" style="display:block;margin:auto" />
<p>对于每个结点，主要包含一个关键字数组Key[]，一个指针数组（指向儿子）Son[]。在B-Tree内，查找的流程是：使用顺序查找（数组长度较短时）或折半查找方法查找Key[]数组，若找到关键字K，则返回该结点的地址及K在Key[]中的位置；否则，可确定K在某个Key[i]和Key[i+1]之间，则从Son[i]所指的子结点继续查找，直到在某结点中查找成功；或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。</p>
<p>接着，我们使用以下图片演示如何生成B-Tree（M=4，依次插入1~6）：</p>
<p>从图可见，当我们插入关键字4时，由于原结点已经满了，故进行分裂，基本按一半的原则进行分裂，然后取出中间的关键字2，升级（这里是成为根结点）。其它的依类推，就是这样一个大概的过程。</p>
<img title="B-Tree样例" src="/imgs/sql/index/b-tree2.png" style="display:block;margin:auto" />
<h1>数据库索引</h1>
<h2 id="什么是索引">什么是索引</h2>
<p>在数据库中，索引的含义与日常意义上的“索引”一词并无多大区别（想想小时候查字典），它是用于提高数据库表数据访问速度的数据库对象。</p>
<pre><code>A）索引可以避免全表扫描。多数查询可以仅扫描少量索引页及数据页，而不是遍历所有数据页。
B）对于非聚集索引，有些查询甚至可以不访问数据页。
C）聚集索引可以避免数据插入操作集中于表的最后一个数据页。
D）一些情况下，索引还可用于避免排序操作。
</code></pre>
<p>当然，众所周知，虽然索引可以提高查询速度，但是它们也会导致数据库系统更新数据的性能下降，因为大部分数据更新需要同时更新索引。</p>
<h2 id="索引的存储">索引的存储</h2>
<p>一条索引记录中包含的基本信息包括：键值（即你定义索引时指定的所有字段的值）+逻辑指针（指向数据页或者另一索引页）。</p>
<img title="索引" src="/imgs/sql/index/index.png" style="display:block;margin:auto" />
<p>当你为一张空表创建索引时，数据库系统将为你分配一个索引页，该索引页在你插入数据前一直是空的。此页此时既是根结点，也是叶结点。每当你往表中插入一行数据，数据库系统即向此根结点中插入一行索引记录。当根结点满时，数据库系统大抵按以下步骤进行分裂：</p>
<pre><code>A）创建两个儿子结点
B）将原根结点中的数据近似地拆成两半，分别写入新的两个儿子结点
C）根结点中加上指向两个儿子结点的指针
</code></pre>
<p>通常状况下，由于索引记录仅包含索引字段值（以及4-9字节的指针），索引实体比真实的数据行要小许多，索引页相较数据页来说要密集许多。一个索引页可以存储数量更多的索引记录，这意味着在索引中查找时在I/O上占很大的优势，理解这一点有助于从本质上了解使用索引的优势。</p>
<h2 id="索引的类型">索引的类型</h2>
<p>索引有聚集索引和非聚集索引两种类型。</p>
<pre><code>A）聚集索引，表数据按照索引的顺序来存储的。对于聚集索引，叶子结点即存储了真实的数据行，不再有另外单独的数据页。
B）非聚集索引，表数据存储顺序与索引顺序无关。对于非聚集索引，叶结点包含索引字段值及指向数据页数据行的逻辑指针，该层紧邻数据页，其行数量与数据表行数据量一致。
</code></pre>
<p>在一张表上只能创建一个聚集索引，因为真实数据的物理顺序只可能是一种。如果一张表没有聚集索引，那么它被称为“堆集”（Heap）。这样的表中的数据行没有特定的顺序，所有的新行将被添加的表的末尾位置。</p>
<h2 id="聚集索引">聚集索引</h2>
<p>在聚集索引中，叶结点也即数据结点，所有数据行的存储顺序与索引的存储顺序一致。</p>
<img title="索引" src="/imgs/sql/index/index2.png" style="display:block;margin:auto" />
<h3 id="聚集索引与查询操作">聚集索引与查询操作</h3>
<p>如上图，我们在名字字段上建立聚集索引，当需要在根据此字段查找特定的记录时，数据库系统会根据特定的系统表查找的此索引的根，然后根据指针查找下一个，直到找到。例如我们要查询“Green”，由于它介于[Bennet,Karsen]，据此我们找到了索引页1007，在该页中“Green”介于[Greane, Hunter]间，据此我们找到叶结点1133（也即数据结点），并最终在此页中找以了目标数据行。</p>
<p>此次查询的IO包括3个索引页的查询（其中最后一次实际上是在数据页中查询）。这里的查找可能是从磁盘读取(Physical Read)或是从缓存中读取(Logical Read)，如果此表访问频率较高，那么索引树中较高层的索引很可能在缓存中被找到。所以真正的IO可能小于上面的情况。</p>
<h3 id="聚集索引与插入操作">聚集索引与插入操作</h3>
<p>最简单的情况下，插入操作根据索引找到对应的数据页，然后通过挪动已有的记录为新数据腾出空间，最后插入数据。</p>
<p>如果数据页已满，则需要拆分数据页（页拆分是一种耗费资源的操作，一般数据库系统中会有相应的机制要尽量减少页拆分的次数，通常是通过为每页预留空间来实现）：</p>
<pre><code>A）在该使用的数据段（extent）上分配新的数据页，如果数据段已满，则需要分配新段。
B）调整索引指针，这需要将相应的索引页读入内存并加锁。
C）大约有一半的数据行被归入新的数据页中。
D）如果表还有非聚集索引，则需要更新这些索引指向新的数据页。
</code></pre>
<p>特殊情况：</p>
<pre><code>A）如果新插入的一条记录包含很大的数据，可能会分配两个新数据页，其中之一用来存储新记录，另一存储从原页中拆分出来的数据。
B）通常数据库系统中会将重复的数据记录存储于相同的页中。
C）类似于自增列为聚集索引的，数据库系统可能并不拆分数据页，页只是简单的新添数据页。
</code></pre>
<h3 id="聚集索引与删除操作">聚集索引与删除操作</h3>
<p>删除行将导致其下方的数据行向上移动以填充删除记录造成的空白。<br>
如果删除的行是该数据页中的最后一行，那么该数据页将被回收，相应的索引页中的记录将被删除。如果回收的数据页位于跟该表的其它数据页相同的段上，那么它可能在随后的时间内被利用。如果该数据页是该段的唯一一个数据页，则该段也被回收。</p>
<p>对于数据的删除操作，可能导致索引页中仅有一条记录，这时，该记录可能会被移至邻近的索引页中，原索引页将被回收，即所谓的“索引合并”。</p>
<h2 id="非聚集索引">非聚集索引</h2>
<p>非聚集索引与聚集索引相比：</p>
<pre><code>A）叶子结点并非数据结点
B）叶子结点为每一真正的数据行存储一个“键-指针”对
C）叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行。
D）类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的。
</code></pre>
<p>聚集索引是一种稀疏索引，数据页上一级的索引页存储的是页指针，而不是行指针。而对于非聚集索引，则是密集索引，在数据页的上一级索引页它为每一个数据行存储一条索引记录。</p>
<p>对于根与中间级的索引记录，它的结构包括：</p>
<pre><code>A）索引字段值
B）RowId（即对应数据页的页指针+指针偏移量）。在高层的索引页中包含RowId是为了当索引允许重复值时，当更改数据时精确定位数据行。
C）下一级索引页的指针
</code></pre>
<p>对于叶子层的索引对象，它的结构包括：</p>
<pre><code>A）索引字段值
B）RowId
</code></pre>
<img title="索引" src="/imgs/sql/index/index3.png" style="display:block;margin:auto" />
<h3 id="非聚集索引与查询操作">非聚集索引与查询操作</h3>
<p>针对上图，如果我们同样查找“Green”，那么一次查询操作将包含以下IO：3个索引页的读取+1个数据页的读取。同样，由于缓存的关系，真实的IO实际可能要小于上面列出的。</p>
<h3 id="非聚集索引与插入操作">非聚集索引与插入操作</h3>
<p>如果一张表包含一个非聚集索引但没有聚集索引，则新的数据将被插入到最末一个数据页中，然后非聚集索引将被更新。如果也包含聚集索引，该聚集索引将被用于查找新行将要处于什么位置，随后，聚集索引、以及非聚集索引将被更新。</p>
<h3 id="非聚集索引与删除操作">非聚集索引与删除操作</h3>
<p>如果在删除命令的Where子句中包含的列上，建有非聚集索引，那么该非聚集索引将被用于查找数据行的位置，数据删除之后，位于索引叶子上的对应记录也将被删除。如果该表上有其它非聚集索引，则它们叶子结点上的相应数据也要删除。</p>
<p>如果删除的数据是该数所页中的唯一一条，则该页也被回收，同时需要更新各个索引树上的指针。</p>
<p>由于没有自动的合并功能，如果应用程序中有频繁的随机删除操作，最后可能导致表包含多个数据页，但每个页中只有少量数据。</p>
<h2 id="索引覆盖">索引覆盖</h2>
<p>索引覆盖是这样一种索引策略：当某一查询中包含的所需字段皆包含于一个索引中，此时索引将大大提高查询性能。</p>
<p>包含多个字段的索引，称为复合索引。索引最多可以包含31个字段，索引记录最大长度为600B。如果你在若干个字段上创建了一个复合的非聚集索引，且你的查询中所需Select字段及Where,Order By,Group By,Having子句中所涉及的字段都包含在索引中，则只搜索索引页即可满足查询，而不需要访问数据页。由于非聚集索引的叶结点包含所有数据行中的索引列值，使用这些结点即可返回真正的数据，这种情况称之为“索引覆盖”。</p>
<p>在索引覆盖的情况下，包含两种索引扫描：</p>
<pre><code>A）匹配索引扫描
B）非匹配索引扫描
</code></pre>
<h3 id="匹配索引扫描">匹配索引扫描</h3>
<p>此类索引扫描可以让我们省去访问数据页的步骤，当查询仅返回一行数据时，性能提高是有限的，但在范围查询的情况下，性能提高将随结果集数量的增长而增长。</p>
<p>针对此类扫描，索引必须包含查询中涉及的的所有字段，另外，还需要满足：Where子句中包含索引中的“引导列”（Leading Column），例如一个复合索引包含A,B,C,D四列，则A为“引导列”。如果Where子句中所包含列是BCD或者BD等情况，则只能使用非匹配索引扫描。</p>
<h3 id="非配置索引扫描">非配置索引扫描</h3>
<p>正如上述，如果Where子句中不包含索引的导引列，那么将使用非配置索引扫描。这最终导致扫描索引树上的所有叶子结点，当然，它的性能通常仍强于扫描所有的数据页。</p>
<h1>参考</h1>
<blockquote>
<p><a href="http://www.cnblogs.com/morvenhuang/archive/2009/03/30/1425534.html">数据库进阶系列之一：漫谈数据库索引</a></p>
</blockquote>
]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>sql</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳极光联盟</title>
    <url>/2020/07/22/%E6%B7%B1%E5%9C%B3%E6%9E%81%E5%85%89%E8%81%94%E7%9B%9F/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="db2262e0194c37a6351e752a415ef82415c929083e887d2065cf09abfd9ffc76">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>lstm</tag>
        <tag>时间复杂度</tag>
        <tag>时序特征</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>oppo广东移动通信有限公司</title>
    <url>/2020/08/12/oppo%E5%B9%BF%E4%B8%9C%E7%A7%BB%E5%8A%A8%E9%80%9A%E4%BF%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="5bc513b888122c33ec9025c8bbe171814cd1a116a3dd9bb93301042f056df8d3">5d20b81ac1f5d0b70e9113cacf134e1eec55447497a63f53873908500ac625f1423b695f187cdd796fd2f19623632ee80d0d0fc920ec205d622fbbf7d3e84307afb94b23027f08fbd819934a81c8f862bc8f830430a140a71b223ae5b4dd7f6bea314de1aa37ebde269bb3d4396150edd161d4fd10813ea2b4f15752a5e046620f53d5a66c954351cfbd0d939287eced586708f25efff26517d890f4d0b538f7e125568fdbda089412a46ca3e75955241cf8568fc208f64396078cc3cc317a28574016851ff50975dbca310114419f9c80f0e9ba55bee1d035d0d4a589a2b74cc3a77cbeb3519aaffab41609a241e1c1aaa88a105d99bb15c325335dbdb813b8b6b88768b6f2ed298fd6c08a23c5cf25d3be4a49b59c4eb171c8da9b6254e3989ca77576488f720afdfefc9fab071091e851ff63d2d6ebb1e9f059146ef31f3b15da43c34e77808ef5108bb70111c491db899ba47bb0c1966a76d269c218ab35e00634bc2f289dc39ce98173c355536e143d21f52b1a03b212852a9f0bc5a81676bbe059fe9fb8021889754ffccdcd05cbd3e6d259e618ca61e3f08d69ee7889f80089e3f3b820b085016e20162c128c1244ebf7f24fe6010e2a15f0d985c952bba1075c2575a03ad2b86be7ee4a8f4680335fae896e79ba56dcb505a7cab29ba352cea380ec08e6c8a8cddb654dc8a37fa917837c59d97e447f50e37ca861277552defc6c9e710b0151861f14b1d64a611cb560da516940a921a7cc8db067803f29b779d58f5137aca45a2ef99a95a49527a16c5323261214cac27136f2f9362e056f4da959adc271a8b2494ebe3ab61bd654860ac3e1e4834ac84953c91e94e2c198660f6346a2c074a4bf708d8ace3b9e8f601c3e3bc204b696625ae4f5d319136d23f2144a0fdfb7970a0c301e3237732bafa70c0701cd65833f60de540de735e3c4ef1a50ab472b3ff0ec174ac29bd2503e19a11e59002020258b25c97d5a2a7699736b474e856f71b995dd675a52fda97ed628841ef5339e5ab86e542e39553ceb1e2edbb2d5a3c07dd4bd1784419a1ac5e6c8c10e25ae6b565cdfde6b542273cfaa3dd9ebfd10c39bac8f8463a6846367e907273f4ba62a76feb366d12500b3fba640631f193d01bde1f9d12859ae982d006567cd95609257a885defd8efbce2d41f0a1ff9ba5bad3dd70d59b7cf7970d225b270519590e8c14b5c9503f32c8b93a9bd7e0e2fd25f9a72419a828b5af26325d92d3f5f141902cff424bf029b2c187c66125ff3011640b3434ff9171937d5ea3dd1bef58b320e2ac7c15feb3a56e78bc32c8dd776e5eb4e329a643fb262dfb367f2e8b02c721df7de98cf9dfa022b48d7cb2c38e92872a560454b9252afa669a76b59cf047bd7fbab5aa0ac9cc5119caff6aa168fe66a47bad9a4e00f22455d58a9b57110ead63be4e43cc3a1ea830183d91e30e1de39cf601bf29a8237ac912a77dff572e6bead69191ba1fcfad679c660c345a4c06161156964a68c74863f04320d65df1010a9150b6616e8bd6312907592bacfd40ea90f520ad63d243fe3f1a00dd04324dab92280ac13578dfa8c5a0cfc57c1a78558559654a1148f2fb60fe9273bc494490254f48745343b92db341c852485fd3b73f0f57cb6e2f11148992759ff5be11fc068fc87fad5001bcd12ba2c608135708e02eaedea5084363749f0de57354bcb8b65df92113120723ed33879a692bf3bc42ec85b7b50910193a7e7f3dc822dd80cfee11eaa2001a2e054a5fdaf4438e7a0f5f808ae16d3d656e02e6362b7e88c285b985bfa4a1d867737e4699126cf362832d8e9bb6c7d232b021822f238e56fa48ad53edadda4c9fe9a91b23e77a23304c4dbd567c16b72e6b0b8c48b77cb8fedf100975364061b953c50e5407d34325f0308c66d1c621df6519c983f56528d542c7c349db0984b4fb5574c7b85cd71989aa54e8b0a6a25bb32fec1801bd8473790927bd96f3fa98b8e469e0c25441ff367d22b5ca3aa7d8bdb5959ffe0063380840c7ec0b581468e5cb2033e16f5139411c36f46faab3833234f18712b000cee50b3f957b29afc0e5f94260e895a75c8afa50</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳追一科技有限公司</title>
    <url>/2020/08/03/%E6%B7%B1%E5%9C%B3%E8%BF%BD%E4%B8%80%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="04777bb9e89e17960c4cde28b630a4b74263962794b11db67fd6ce6afee77020">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>rs todo</tag>
        <tag>nlp todo</tag>
      </tags>
  </entry>
  <entry>
    <title>宇龙计算机通信科技（深圳）有限公司</title>
    <url>/2020/08/11/%E5%AE%87%E9%BE%99%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%80%9A%E4%BF%A1%E7%A7%91%E6%8A%80%EF%BC%88%E6%B7%B1%E5%9C%B3%EF%BC%89%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="0a96818458728a8c88715045e42ac9018b456f08823b688ada48c1539925ab42">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>rs</tag>
      </tags>
  </entry>
  <entry>
    <title>粤港澳大湾区数字经济研究院</title>
    <url>/2020/08/08/%E7%B2%A4%E6%B8%AF%E6%BE%B3%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%95%B0%E5%AD%97%E7%BB%8F%E6%B5%8E%E7%A0%94%E7%A9%B6%E9%99%A2/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="27ab009649b551519813e92b0b51adfe0f70d9608d0f567c6077f81a3525688f">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>维沃移动通信有限公司</title>
    <url>/2020/08/12/%E7%BB%B4%E6%B2%83%E7%A7%BB%E5%8A%A8%E9%80%9A%E4%BF%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="4d7aef68e1bd06e344c08efec3c104e1972ac7ce5bc9f1cd7e52b9a2532704e8">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>深圳乐信软件技术有限公司</title>
    <url>/2020/08/05/%E6%B7%B1%E5%9C%B3%E4%B9%90%E4%BF%A1%E8%BD%AF%E4%BB%B6%E6%8A%80%E6%9C%AF%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="1bb10ad5e3de6abc98007679c0cf7b194e831a20d1c0d6cf5f6db6eb9daa12eb">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>manjaro配置</title>
    <url>/2022/03/19/manjaro%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录manjaro使用过程中的一些配置！</p>
<span id="more"></span>
<h1>zsh 配置</h1>
<h2 id="安装">安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh</span><br><span class="line">sh install.sh</span><br></pre></td></tr></table></figure>
<h2 id="themes">themes:</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.zshrc</span><br><span class="line">ZSH_THEME=<span class="string">&quot;ys2&quot;</span></span><br></pre></td></tr></table></figure>
<p>其中<code>~/.oh-my-zsh/themes/ys2.zsh-theme</code>的部分内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Conda info</span></span><br><span class="line"><span class="built_in">local</span> conda_info=<span class="string">&#x27;$(conda_prompt_info)&#x27;</span></span><br><span class="line"><span class="function"><span class="title">conda_prompt_info</span></span>() &#123;</span><br><span class="line">  <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$CONDA_DEFAULT_ENV</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> -n <span class="string">&quot;(<span class="variable">$CONDA_DEFAULT_ENV</span>) &quot;</span></span><br><span class="line">  <span class="keyword">else</span> </span><br><span class="line">    <span class="built_in">echo</span> -n <span class="string">&quot;(base) &quot;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PROMPT=<span class="string">&quot;</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[blue]%&#125;#%&#123;<span class="variable">$reset_color</span>%&#125; \</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[yellow]%&#125;%~%&#123;<span class="variable">$reset_color</span>%&#125;\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;hg_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;git_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;svn_info&#125;</span>\</span></span><br><span class="line"><span class="string"><span class="variable">$&#123;venv_info&#125;</span>\</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$fg</span>[yellow]%&#125; <span class="variable">$&#123;conda_info&#125;</span>\</span></span><br><span class="line"><span class="string">\</span></span><br><span class="line"><span class="string">%&#123;<span class="variable">$terminfo</span>[bold]<span class="variable">$fg</span>[red]%&#125;$ %&#123;<span class="variable">$reset_color</span>%&#125;&quot;</span></span><br></pre></td></tr></table></figure>
<p><code>~/.condarc</code>中增加：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">changeps1: False</span><br></pre></td></tr></table></figure>
<h2 id="所有可读目录颜色">所有可读目录颜色</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.dir_colors</span><br><span class="line">OTHER_WRITABLE 01;34</span><br><span class="line"></span><br><span class="line">vim ~/.zshrc</span><br><span class="line"><span class="built_in">eval</span> `<span class="built_in">dircolors</span> <span class="variable">$HOME</span>/.dir_colors`</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>manjaro</tag>
      </tags>
  </entry>
  <entry>
    <title>jupyter notebook配置</title>
    <url>/2022/04/02/jupyter-notebook%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录使用jupyter notebook过程中的配置问题！</p>
<span id="more"></span>
<h1>Jupyter notebook好用的插件</h1>
<ol>
<li>
<p>安装<code>jupyter notebook</code>插件工具栏</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>添加插件选项工具栏到<code>jupyter notebook</code>页面中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter contrib nbextension install</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>选择好用的插件</p>
<ul>
<li>Hinterland：代码自动填充</li>
<li>Skip-Traceback：省略繁杂的错误提示信息</li>
<li>Live Markdown Preview：Markdown实时渲染</li>
<li>Highlighter：摘选高亮</li>
<li>Spell Checker：拼写错误检查</li>
<li>Code prettify：格式化代码</li>
<li>Codefolding：代码折叠</li>
<li>ExecuteTime：代码执行时长</li>
<li>Table of Contents：自动生成目录</li>
<li>Variable Inspector ：这是一个查看变量的插件</li>
</ul>
</li>
</ol>
<h1>更改Code字体</h1>
<ol>
<li>
<p>找到控制字体的css文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/home/huangyedi2012/.conda/envs/pytorch/lib/python3.9/site-packages/notebook/static/components/codemirror/lib</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改css文件：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.CodeMirror</span> pre<span class="selector-class">.CodeMirror-line</span>,</span><br><span class="line"><span class="selector-class">.CodeMirror</span> pre<span class="selector-class">.CodeMirror-line-like</span> &#123;</span><br><span class="line">  <span class="comment">/* Reset some styles that the rest of the page might have set */</span></span><br><span class="line">  -moz-<span class="attribute">border-radius</span>: <span class="number">0</span>; -webkit-<span class="attribute">border-radius</span>: <span class="number">0</span>; <span class="attribute">border-radius</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">border-width</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">background</span>: transparent;</span><br><span class="line">  <span class="attribute">font-family</span>: <span class="string">&#x27;Courier New&#x27;</span>;</span><br><span class="line">  <span class="attribute">font-size</span>: inherit;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">white-space</span>: pre;</span><br><span class="line">  <span class="attribute">word-wrap</span>: normal;</span><br><span class="line">  <span class="attribute">line-height</span>: <span class="number">1.5</span>;</span><br><span class="line">  <span class="attribute">color</span>: inherit;</span><br><span class="line">  <span class="attribute">z-index</span>: <span class="number">2</span>;</span><br><span class="line">  <span class="attribute">position</span>: relative;</span><br><span class="line">  <span class="attribute">overflow</span>: visible;</span><br><span class="line">  -webkit-tap-highlight-<span class="attribute">color</span>: transparent;</span><br><span class="line">  -webkit-<span class="attribute">font-variant-ligatures</span>: contextual;</span><br><span class="line">  <span class="attribute">font-variant-ligatures</span>: contextual;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
        <tag>notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>python import导包路径</title>
    <url>/2022/02/10/python-import%E5%AF%BC%E5%8C%85%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>本文主要分析python中import文件路径的问题。</p>
<span id="more"></span>
<h1>基本概念</h1>
<ul>
<li><code>module</code>：一个<code>.py</code>文件</li>
<li><code>package</code>：一个包含<code>.py</code>文件的文件夹</li>
</ul>
<h1>搜索路径</h1>
<p>python导入其他包或者模块时，会根据<code>sys.path</code>中的路径进行搜索，这些路径主要为一下三种:</p>
<ol>
<li>当前执行脚本所在路径</li>
<li>python内置标准库路径，PYTHONPATH</li>
<li>安装的第三方模块路径。</li>
</ol>
<h1>import步骤</h1>
<p>python所有加载的module信息都存放在<code>sys.modules</code>结构中，当import一个模块时:</p>
<ol>
<li>如果是<code>import A</code>，检查<code>sys.modules</code>中是否有A，如果有则不加载，如果没有，则为A创建module对象，并加载A</li>
<li>如果是 <code>from A import B</code>，先为 A 创建 module 对象，再解析A，从中寻找B并填充到 A 的 <code>__dict__ </code>中</li>
</ol>
<h1>相对导入与绝对导入</h1>
<p><strong>绝对导入与相对导入的形式：</strong></p>
<ul>
<li>
<p>绝对导入的格式为 <code>import A.B</code> 或 <code>from A import B</code></p>
</li>
<li>
<p>相对导入格式为 <code>from . import B</code> 或 <code>from ..A import B</code>，.代表当前模块，…代表上层模块，…代表上上层模块，依次类推。</p>
</li>
</ul>
<p><strong>顶层结构：</strong></p>
<p>在没有明确指定包结构的情况下，Python 是根据 <code>__name__ </code>来决定一个模块在包中的结构的:</p>
<ul>
<li>如果是 <code>__main__ </code>则它本身是顶层模块，没有包结构</li>
<li>如果是A.B.C 结构，那么顶层模块是 A。</li>
</ul>
<blockquote>
<p><strong><code>__name__ </code>变量</strong>作为python的内置变量，是每个python模块必备的属性，可以取两种值:</p>
<ul>
<li>当你直接执行一段脚本的时候，这段脚本的 **<strong>name</strong>**变量等于 <strong>‘<strong>main</strong>’</strong></li>
<li>当这段脚本被导入其他程序的时候，<strong><strong>name</strong></strong> 变量等于脚本本身的名字。</li>
</ul>
</blockquote>
<p><strong>导包原则：</strong></p>
<ol>
<li>如果是绝对导入，一个模块只能导入自身的子模块或和它的顶层模块同级别的模块及其子模块</li>
<li>如果是相对导入，一个模块必须有包结构且只能导入它的顶层模块内部的模块</li>
</ol>
<p>如果一个模块被直接运行，则它自己为顶层模块，不存在层次结构，找不到其他的相对路径，因此存在相对导入语句的模块，不能直接运行。</p>
<p>Python2.x 缺省为相对路径导入，Python3.x 缺省为绝对路径导入。绝对导入可以避免导入子包覆盖掉标准库模块（由于名字相同，发生冲突）。如果在 Python2.x 中要默认使用绝对导入，可以在文件开头加入如下语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br></pre></td></tr></table></figure>
<p><strong>from _<em>future</em>_ import absolute_import</strong></p>
<p>这句 import 并不是指将所有的导入视为绝对导入，而是指禁用 <code>implicit relative import</code>（隐式相对导入）, 但并不会禁掉 <code>explicit relative import</code>（显示相对导入）。</p>
<p><strong>关于隐式相对导入、显示相对导入和绝对导入</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">thing</span><br><span class="line">├── books</span><br><span class="line">│ ├── adventure.py</span><br><span class="line">│ ├── history.py</span><br><span class="line">│ ├── horror.py</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ └── lovestory.py</span><br><span class="line">├── furniture</span><br><span class="line">│ ├── armchair.py</span><br><span class="line">│ ├── bench.py</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── screen.py</span><br><span class="line">│ └── stool.py</span><br><span class="line">└── __init__.py</span><br></pre></td></tr></table></figure>
<p>如果在 stool 中引用 bench，则有如下几种方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bench <span class="comment"># 此为 implicit relative import</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> bench <span class="comment"># 此为 explicit relative import</span></span><br><span class="line"><span class="keyword">from</span> furniture <span class="keyword">import</span> bench <span class="comment"># 此为 absolute import</span></span><br></pre></td></tr></table></figure>
<ul>
<li>隐式相对就是没有告诉解释器相对于谁，但默认相对与当前模块；</li>
<li>显示相对则明确告诉解释器相对于谁来导入</li>
</ul>
<p>以上导入方式的第三种，才是官方推荐的，第一种是官方强烈不推荐的</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2022/04/26/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>recurrent neural network</title>
    <url>/2022/04/26/recurrent-neural-network/</url>
    <content><![CDATA[<h1>GRU</h1>
]]></content>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>面试问题2</title>
    <url>/2022/07/19/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-cj/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="912937c3238ad27d2a22c8fe6fb5d1661861466165caa810c3d32469c6f24d20">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>面试问题1</title>
    <url>/2022/07/18/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-qw/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="4fe139a1b41bd1b209901981cd154ab30753b1f5217671f92c4a20a775d800dc">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Unified Language Model Pre-training</title>
    <url>/2023/03/30/Unified-Language-Model-Pre-training/</url>
    <content><![CDATA[<p><img src="UniLM.png" alt="UniLM"></p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>unlm</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow常见问题汇总</title>
    <url>/2022/08/31/tensorflow%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>tensorflow 常见问题汇总</p>
<span id="more"></span>
<h2 id="save-flags">save flags</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_configuration</span>(<span class="params">FLAGS, filepath</span>):</span><br><span class="line">    flag_dict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name,value <span class="keyword">in</span> FLAGS.__flags.items():</span><br><span class="line">        value = value.value</span><br><span class="line">        flag_dict[name] = value</span><br><span class="line">    json_str = json.dumps(flag_dict, indent=<span class="number">4</span>)</span><br><span class="line">    tf.logging.info(json_str)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        fw.write(json_str)</span><br></pre></td></tr></table></figure>
<h2 id="logging-twice">logging twice</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logger = tf.get_logger()</span><br><span class="line">logger.propagate=<span class="literal">False</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>logging</tag>
        <tag>config</tag>
      </tags>
  </entry>
  <entry>
    <title>多标签分类</title>
    <url>/2022/09/27/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>多标签分类相关算法</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>mlc</tag>
      </tags>
  </entry>
  <entry>
    <title>cmder配置</title>
    <url>/2023/06/26/cmder%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>本文记录cmder的一些配置情况！</p>
<span id="more"></span>
<h1>配置环境变量</h1>
<p>设置以下环境变量：</p>
<ul>
<li><code>CMDER_ROOT=D:\ProgramData\cmder_mini</code></li>
<li><code>PATH=%CMDER_ROOT%</code></li>
</ul>
<h1>添加到右键菜单</h1>
<p>以管理员身份打开cmd</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> %CMDER_ROOT%</span><br><span class="line">Cmder.exe /REGISTER ALL</span><br></pre></td></tr></table></figure>
<h1>解决中文乱码</h1>
<p>在设置中找到<code>startup -&gt; Environment</code>，追加以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> PATH=%ConEmuBaseDir%\Scripts;%PATH%</span><br><span class="line"><span class="built_in">set</span> LANG=zh_CN.UTF-8</span><br><span class="line"><span class="built_in">set</span> LC_ALL=zh_CN.utf8</span><br><span class="line">chcp utf-8</span><br></pre></td></tr></table></figure>
<h1>修改提示符符号</h1>
<p>Cmder 中的提示符符号默认为 λ，可能会导致某些bug，修改提示符时，需要修改 <code>%CMDER_ROOT%\config\cmder_prompt_config.lua</code>，搜索<code>λ</code>，替换成<code>$</code></p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">prompt_lambSymbol = <span class="string">&quot;$&quot;</span></span><br><span class="line">prompt_singleLine = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h1>添加自定义命令</h1>
<p>修改<code>%CMDER_ROOT%\config\user-aliases.cmd</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ll=<span class="built_in">ls</span> -lh --show-control-chars --color $*</span><br></pre></td></tr></table></figure>
<h1>Win11下vim不可用问题</h1>
<ol>
<li>
<p><code>任务栏</code>  --&gt; <code>debug</code> --&gt; <code>properties</code> --&gt; <code>使用旧版控制台</code></p>
</li>
<li>
<p><code>Setting</code> --&gt; <code>Startup</code> --&gt; <code>Command line</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%windir%\system32\bash.exe ~ -cur_console:p:n</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1>防止字体重叠</h1>
<p>去掉<code>Setting -&gt; Main -&gt; Fonts</code>中的<code>Monospace</code>勾选项</p>
<h1>将Pycharm中的terminal替换成cmder</h1>
<p>打开pycharm设置，settings&gt;tool&gt;terminal中修改shell path。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;cmd.exe&quot;</span> /k <span class="string">&quot;&quot;</span>D:\ProgramData\cmder\vendor\init.bat<span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>cmder</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>active learning 主动学习</title>
    <url>/2022/10/18/active-learning-%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><strong>Deep Active Learning：</strong> Pool-based AL selects most informative data iteratively from a large pool of unlabeled i.i.d. data samples until either the basic learner(s) reaches a certain level of performance or a fixed budget is exhausted.</p>
<span id="more"></span>
<h1>Querying Strategies</h1>
<h2 id="uncertainty-based">uncertainty-based</h2>
<p>Uncertainty-based DAL selects data samples with <strong>high aleatoric uncertainty or epistemic uncertainty</strong>.</p>
<ul>
<li><strong>Aleatoric uncertainty</strong> refers to the natural uncertainty in data due to influences on data generation processes that are inherently random.</li>
<li><strong>Epistemic uncertainty</strong> comes from the modeling/learning process and is caused by a lack of knowledge.</li>
</ul>
<p><strong>Typical methods：</strong></p>
<ol>
<li><strong>Maximum Entropy (Entropy)</strong>  selects data x that maximize the predictive entropy.</li>
<li><strong>Margin</strong> selects data x whose two most likely labels have smallest difference in posterior probabilities.</li>
<li><strong>Least Confidence (LeastConf)</strong> selects data x whose most likely label ŷ has lowest posterior probability</li>
<li><font color='red'>**Bayesian Active Learning by Disagreements (BALD) ** chooses data points that are<br>
expected to maximize the information gained from the model parameters ω, i.e. the mu-<br>
tual information between predictions and model posterior: αBALD (x, M) = HM [y|x] −<br>
Ep(ω|Dl) [HM[y|x, ω]].</font></li>
<li><font color='red'><strong>Mean Standard Deviation (MeanSTD)</strong> maximizes the mean  standard deviation of the predicted probabilities over all k classes: αMeanSTD (x, M) = 1 k Pk pVarq(ω)[p(y = k|x, ω)].</font></li>
<li>DeepFool Active Learning method (AdvDeepFool)</li>
<li>Generative Adversarial Active Learning (GAAL)</li>
<li>Bayesian Generative Active Deep Learning (BGADL)</li>
<li>Batch Active learning by Diverse Gradient Embeddings (BADGE)</li>
<li>Loss Prediction Loss (LPL)</li>
</ol>
<h2 id="representativeness-diversity-based">representativeness/diversity-based</h2>
<p>Representative/diversity-based strategies select batches of samples representative of the unlabeled set and are based on the intuition that the selected representative examples, once labeled, can act as a surrogate for the entire dataset.</p>
<p>Typical methods：</p>
<ol>
<li>KMeans</li>
<li>CoreSet</li>
<li>Cluster-Margin</li>
<li>Active-DPP</li>
</ol>
<h2 id="combined-strategies">combined strategies</h2>
<p>Due to the demand for larger batch size (representative/diversity) and more precise decision boundaries for higher model performance (uncertainty) in DAL, combined strategies have become the dominant approaches to DAL. It aims to achieve a trade-off between uncertainty and representativeness/diversity in query selection.</p>
<h1>参考文献</h1>
<blockquote>
<p><a href="https://arxiv.org/pdf/2203.13450.pdf">Zhan, Xueying, et al. “A comparative survey of deep active learning.” arXiv preprint arXiv:2203.13450 (2022).</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>active learning</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT算法原理</title>
    <url>/2021/06/28/GBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient Boosting 和Decision Tree。</p>
<span id="more"></span>
<h1>Decision Tree：CART回归树</h1>
<p>GBDT使用的决策树通通都是都是CART回归树。为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是<strong>梯度值</strong>，是连续值所以要用回归树。</p>
<p>对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。</p>
<p>在<strong>分类树</strong>中最佳划分点的判别标准是熵或者基尼系数，都是用<strong>纯度</strong>来衡量的，但是在<strong>回归树</strong>中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是<strong>平方误差</strong>，它能很好的评判拟合程度。</p>
<p><strong>回归树生成算法:</strong></p>
<p><strong>输入:</strong> 训练数据集$ D$</p>
<p><strong>输出:</strong> 回归树 $f(x)$</p>
<p>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<ol>
<li>
<p>选择最优切分变量$ j$与切分点 $s$， 求解:</p>
<p>$$<br>
\min_{j,s}[\min_{c_1}\sum_{x_i \in R_1(j, s)} (y_i-   c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j, s)} (y_i-c_2)^2]<br>
$$</p>
<p>遍历变量$j$，对固定的切分变量$j$扫描切分点 $s$，选择使得上式达到最小值的对 $(j,s)$.</p>
<p><strong>简要解释一下上述公式</strong>：中括号里面的公式是求出每个特征变量在哪一个划分点时损失函数最小，最外面的 $min$ 是在所有特征值，求得使损失函数全局最小的特征及其切分点$(j^<em>, s^</em>)$;</p>
</li>
<li>
<p>用选定的对 $(j,s)$ 划分区域并决定相应的输出值：</p>
<p>$$<br>
R_1(j, s)={x|x^{(j)}\leq s},R_2(j, s)={x|x^{(j)} &gt; s}<br>
$$</p>
<p>$$<br>
\hat {c_m}=\frac{1}{N}\sum_{x_1 \in R_m(j, s)}y_i, x \in R_m, m=1,2<br>
$$</p>
<p>划分区域的输出值就是将该区域的所有样本的<strong>输出值求平均</strong>。</p>
</li>
<li>
<p>继续对两个子区域调用步骤（1）和（2），直至满足停止条件。</p>
</li>
<li>
<p>将输入空间划分为$M$个区域$R_1,R_2…R_M$，得到决策树</p>
</li>
</ol>
<p>$$<br>
f(x)=\sum_{m=1}^M \hat{c_m}I(x \in R_m)<br>
$$</p>
<h1>Gradient Boosting：拟合负梯度</h1>
<p>梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树。</p>
<p><strong>提升树算法:</strong></p>
<ol>
<li>
<p>初始化$ f0(x)=0$</p>
</li>
<li>
<p>对$m=1,2…M$</p>
<ol>
<li>
<p>计算残差<br>
$$<br>
r_{mi}=y_i-f_{m-1}(x_i), i=1, 2,…,M<br>
$$</p>
</li>
<li>
<p>拟合残差$ r_{mi}$ 学习一个回归树，得到 $h_m(x)$</p>
</li>
<li>
<p>更新 $f_m(x)=f_{m-1}(x)+h_m(x)$</p>
</li>
</ol>
</li>
<li>
<p>得到回归树<br>
$$<br>
f_{M}(x)=\sum_{m=1}^Mh_m(x)<br>
$$</p>
</li>
</ol>
<p>上面伪代码中的<strong>残差</strong>是什么？</p>
<p>在提升树算法中，假设我们前一轮迭代得到的强学习器是</p>
<p>$$<br>
f_{t-1}(x)<br>
$$</p>
<p>损失函数是</p>
<p>$$<br>
L(y, f_{t-1}(x))<br>
$$<br>
我们本轮迭代的目标是找到一个弱学习器</p>
<p>$$<br>
h_{t}(x)<br>
$$</p>
<p>当采用平方损失函数时</p>
<p>$$<br>
\begin{aligned}<br>
&amp; L(y, f_{t-1}(x)+h_t(x)) \<br>
&amp; = (y - f_{t-1}(x) - h_t(x))^2 \<br>
&amp; =(r - h_t(x))^2 \<br>
\end{aligned}<br>
$$</p>
<p>这里，</p>
<p>$$<br>
r = y - f_{t-1}(x)<br>
$$<br>
是当前模型拟合数据的<strong>残差（residual）</strong>。所以，对于提升树来说只需要简单地拟合当前模型的残差。</p>
<p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p>
<p>第$t$轮的第$i$个样本的损失函数的负梯度为：<br>
$$<br>
-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}<br>
$$<br>
此时不同的损失函数将会得到不同的负梯度，如果选择平方损失:<br>
$$<br>
L(y, f(x_i))=\frac{1}{2}(y-f(x_i))^2<br>
$$<br>
负梯度为<br>
$$<br>
-[\frac{\partial {L(y, f(x_i))}}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=-[\frac{\partial \frac{1}{2}(y-f(x_i))^2}{\partial {f(x_i)}}]_{f(x)=f_{t-1}(x)}=y-f(x_i)<br>
$$<br>
此时我们发现GBDT的<strong>负梯度就是残差</strong>，所以说对于回归问题，我们要拟合的就是残差。</p>
<blockquote>
<p><strong>gbdt的残差为什么用负梯度代替？</strong></p>
<ol>
<li>
<p>通过一阶泰勒展开证明负梯度方向是下降最快的方向<br>
$$<br>
f(\theta_{k+1}) \approx f(\theta_k)+\frac{\partial{f(\theta_k)}}{\partial{\theta_k}}(\theta_{k+1}-\theta_k)<br>
$$</p>
</li>
<li>
<p>在GB中，对损失函数展开：<br>
$$<br>
L(y,F_m(x))\approx L(y, F_{m-1}(x))+\frac{\partial{L(y, F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}(F_m(x)-F_{m-1}(x))<br>
$$<br>
即有：<br>
$$<br>
L(y,F_m(x))\approx L(y, F_{m-1}(x))+\frac{\partial{L(y, F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}T_m(x)<br>
$$<br>
则在优化$L(y,F(x))$时，<br>
$$<br>
F_m(x)=F_{m-1}(x)-\eta \frac{\partial{L(y, F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}<br>
$$<br>
即，$T_m(x)=-\eta \frac{\partial{L(y, F_{m-1}(x))}}{\partial{ F_{m-1}(x)}}$</p>
</li>
</ol>
<p>所以需要当前的弱学习器来学习负梯度，这里和GBDT中差了一个 $\eta$.</p>
<ol start="3">
<li>
<p>在1和2中都是随机梯度下降，但是不同的是：</p>
<ol>
<li>在参数空间中优化，每次迭代得到参数的增量，这个增量就是负梯度乘上学习率；</li>
<li>在函数空间中优化，每次得到增量函数，这个函数会去拟合负梯度，在GBDT中就是一个个决策树。</li>
</ol>
</li>
</ol>
<p>要得到最终结果，只需要把初始值或者初始的函数加上每次的增量。所以1的优化过程是(假设迭代了M次)：</p>
<p>无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，<strong>残差只是特例</strong>。</p>
</blockquote>
<h1>GBDT算法原理</h1>
<p>上面两节分别将Decision Tree和Gradient Boosting介绍完了，下面将这两部分组合在一起就是我们的GBDT了。</p>
<p><strong>GBDT算法：</strong></p>
<ol>
<li>
<p>初始化弱学习器<br>
$$<br>
f_0(x)=\arg \min_{c}\sum_{i=1}^{N}L(y_i, c)<br>
$$<br>
当为平方损失时，$f_0(x)=\frac{\sum_{i=1}^N y_i}{N}$</p>
</li>
<li>
<p>对$m=1,2,…,M$有：</p>
<p>a. 对每个样本$i=1,2,…,N$，计算负梯度，即残差</p>
<p>$$<br>
r_{im}=-[\frac{\partial{L(y_i, f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}<br>
$$</p>
<p>b. 将上步得到的残差作为样本新的真实值，并将数据$(x_i, x_im), i=1, 2,…,N$作为下棵树的训练数据，得到一颗新的回归树$ f_m(x)$，其对应的叶子节点区域为 $R_jm, j=1, 2,…,J$。其中J为回归树$t$的叶子节点的个数。</p>
<p>c. 对叶子区域$ j=1,2,…J$计算最佳拟合值<br>
$$<br>
\gamma_{jm}=\arg \min_{\gamma}\sum_{x_i \in R_{jm}}L(y_i, f_{m-1}(x_i)+\gamma) (对 \gamma求导并令导数为0即可求得)<br>
$$</p>
<p>d. 更新强学习器<br>
$$<br>
f_m(x)=f_{m-1}(x)+\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})<br>
$$</p>
</li>
<li>
<p>得到最终学习器<br>
$$<br>
f(x)=f_M{x}=f_{0}(x)+\sum_{m=1}^{M}\sum_{j=1}^{J}\gamma_{jm}I(x \in R_{jm})<br>
$$</p>
</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://ranmaosong.github.io/2019/04/27/ML-GBDT/">GBDT算法原理以及实例理解</a></li>
<li><a href="https://www.zhihu.com/question/63560633/answer/581670747">gbdt的残差为什么用负梯度代替？</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
        <tag>cart</tag>
        <tag>boosting tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic Regression</title>
    <url>/2021/06/25/Logistic-Regression/</url>
    <content><![CDATA[<p>本文主要参考Andrew Ng老师的Machine Learning公开课，并用《机器学习实战》中的源码实现。</p>
<span id="more"></span>
<h1>Logistic Regression基本原理</h1>
<h2 id="Logistic分布">Logistic分布</h2>
<p>Logistic Distribution的密度函数和概率分布函数如下：</p>
<p>$$\begin{equation}<br>
f(x)=F’(x) = \frac{e^{-(x-\mu)/\gamma}} { \gamma (1+e^{-(x-\mu)/\gamma})^2 }<br>
\end{equation}$$</p>
<p>$$\begin{equation}<br>
F(x) = P(X \leqslant x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}<br>
\end{equation}$$</p>
<p>上式中$ \mu $是位置参数，$ \gamma &gt; 0 $是形状参数。</p>
<p>下图是不同参数对logistic分布的影响，从图中可以看到可以看到 $ \mu $ 影响的是中心对称点的位置，$ \gamma $越小中心点附近增长的速度越快。而常常在深度学习中用到的非线性变换$ sigmoid $函数是逻辑斯蒂分布的$ \gamma=1,\mu=0 $的特殊形式。</p>
<img title="不同参数对logistic分布的影响（图片来源维基百科）" src="lr-distribution.png" style="display:block;margin:auto" />
<h2 id="二项Logistic-Regression模型">二项Logistic Regression模型</h2>
<img title="分类数据示例" src="lr-classify-data.png"  style="display:block;margin:auto" />
<p>逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。对于上图中的数据，逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。</p>
<blockquote>
<p>分离边界的维数与空间的维数相关。如果是二维平面，分离边界就是一条线（一维）。如果是三维空间，分离边界就是一个空间中的面（二维）。如果是一维直线，分离边界就是直线上的某一点。</p>
</blockquote>
<p>假设输入的特征向量为$ x \in R^n $，$ Y $取值为$ 0，1 $。对于二维的空间，决策边界可以表示为$ w_1x_1+w_2x_2+b=0 $，假如存在一个例子使得$ h_w(x)=w_1x_1+w_2x_2+b&gt;0 $，那么可以判断它类别为$ 1 $，这个过程实际上是<strong>感知机</strong>，即只通过决策函数的符号来判断属于哪一类。</p>
<p>而逻辑回归需要再进一步，它要找到分类概率$ P(Y=1)$与输入向量$ x $的直接关系，然后通过比较概率值来判断类别，而刚好上文中的<code>logistic function</code>能满足这样的要求，它令决策函数的输出值$ w^Tx+b = log \frac{P(Y=1|x)}{1−P(Y=1|x)} $，求解这个式子得到了输入向量$ x $下导致产生两类的概率为：</p>
<p>$$\begin{equation}<br>
P(Y=1|x)=\frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}}<br>
\label{eq:logistic1}<br>
\end{equation}$$</p>
<p>$$\begin{equation}<br>
P(Y=0|x)=\frac{1}{1+e^{w\cdot x+b}}<br>
\end{equation}$$</p>
<p>其中$ w $称为权重，$ b $称为偏置，其中的$ w⋅x+b $看成对$ x $的线性函数，有时候为了书写方便，会将$ b $写入$ w $，即 $ w=(b,w_1,…,w_n) $ ，并取$ x=(1,x_1,…,x_n) $。然后对比上面两个概率值，概率值大的就是$ x $对应的类。</p>
<p>又已知一个事件发生的几率<code>odds</code>是指该事件发生与不发生的概率比值，二分类情况下即$ \frac{P(Y=1|x)}{P(Y=0|x)}=\frac{P(Y=1|x)}{1−P(Y=1|x)} $。取<code>odds</code>的对数就是上面提到的<code>logistic function</code>，$ logistic(P(Y=1|x))=log\frac{P(Y=1|x)}{1−P(Y=1|x)}=w⋅x $。从而可以得到一种对逻辑回归的定义，</p>
<p>**输出$ Y=1 $的对数几率是由输入$ x $的线性函数表示的模型，即逻辑斯蒂回归模型(李航.《统计机器学习》)。**而直接考察公式$\eqref{eq:logistic1}$可以得到另一种对逻辑回归的定义，**线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型(李航.《统计机器学习》)。**因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
<p>有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$ P(Y=1|x)=h_w(x) $，似然函数为$ \prod [h_w(x^{(i)})]^{y^{(i)}}[1-h_w(x^{(i)})]^{(1-y^{(i)})} $，对数似然函数为</p>
<p>$$\begin{eqnarray}<br>
L(w) &amp; = &amp; \sum_{i=1}^{N}\log P(y^{(i)}|x^{(i)};w) \\<br>
&amp; = &amp; \sum_{i=1}^{N}[y^{(i)}\log h_w(x^{(i)})+(1-y^{(i)})\log(1-h_w(x^{(i)}))]<br>
\end{eqnarray}$$</p>
<h1>优化方法</h1>
<p>优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是</p>
<p>$$\begin{eqnarray}<br>
min J(w) &amp;=&amp; \min \frac{1}{m} \sum_{j=1}^{m}Cost(h_w(x^{(i)}),y^{(i)}) \\<br>
&amp;=&amp; \min {-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}\log h_w(x^{(i)})+(1-y^{(i)})\log(1-h_w(x^{(i)}))]}<br>
\end{eqnarray}$$</p>
<h2 id="梯度下降法">梯度下降法</h2>
<blockquote>
<p>最大似然估计就是要求得使$ J(θ) $取最大值时的$ θ $，但因此处的$ Cost(h_w(x^{(i)}),y^{(i)}) $添加了一个负号，所以必须用梯度下降法求解最佳参数。但若此处的$ Cost(h_w(x^{(i)}),y^{(i)}) $没有添加负号，则需要用梯度上升法求解最佳参数。</p>
</blockquote>
<p>先把$ J(w) $对$ w_j $的一阶偏导求出来，且用$ g $表示。$ g $是梯度向量。</p>
<p>$$\begin{eqnarray}<br>
g_j &amp;=&amp; \frac{\partial J(w)}{\partial w_j}\\<br>
&amp;=&amp; -\frac{1}{m}\sum_{i=1}^{m}(\frac{y^{(i)}}{h_w(x^{(i)})} h_w(x^{(i)}) (1-h_w(x^{(i)}))(-x_{j}^{(i)}) + (1-y^{(i)})\frac {1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}) \\<br>
&amp;=&amp; -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)})<br>
\end{eqnarray}$$</p>
<p>梯度下降是通过$ J(w) $对$ w $的一阶导数来找下降方向（负梯度），并且以迭代的方式来更新参数，更新方式为</p>
<p>$$\begin{eqnarray}<br>
w^{k+1}_j &amp;=&amp; w^k_j+α(-g_j)\\<br>
&amp;=&amp;w^k_j+α \frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)}<br>
\label{eq:lr-gd}<br>
\end{eqnarray}$$</p>
<p>$ k $为迭代次数。每次更新参数后，可以通过比较$||J(w^{k+1})−J(w^k)||$或者$ ||w^{k+1}−w^k ||$与某个阈值$ \epsilon $大小的方式来停止迭代，即比阈值小就停止。</p>
<blockquote>
<p>如果采用梯度上升法来推到参数的更新方式，会发现式子与公式$\eqref{eq:lr-gd}$完全一样，所以采用梯度上升发和梯度下降法是一样的。</p>
</blockquote>
<h2 id="随机梯度下降法">随机梯度下降法</h2>
<p>从上面梯度下降法中的公式$\eqref{eq:lr-gd}$中可以看到，每次更新回归系数时都需要遍历整个数据集，如果有数十亿样本和成千上万个特征，则梯度下降法的计算复杂度就太高了。随机梯度下降法一次仅用一个样本点来更新回归系数：</p>
<p>$$\begin{equation}<br>
w^{k+1}_j = w^k_j+α (y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)}<br>
\end{equation}$$</p>
<h2 id="梯度下降过程向量化">梯度下降过程向量化</h2>
<p>约定训练数据的矩阵形式如下，$ x $的每一行为一条训练样本，而每一列为不同的特称取值：</p>
<h1>$$\begin{equation}<br>
x=<br>
\left[<br>
\begin{matrix}<br>
x^{(1)}\\<br>
x^{(2)}\\<br>
\ldots\\<br>
x^{(m)}<br>
\end{matrix}<br>
\right]</h1>
<p>\left[<br>
\begin{matrix}<br>
x_0^{(1)} &amp; x_1^{(1)} &amp; \ldots &amp; x_n^{(1)}\\<br>
x_0^{(2)} &amp; x_1^{(2)} &amp; \ldots &amp; x_n^{(2)}\\<br>
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\<br>
x_0^{(m)} &amp; x_1^{(m)} &amp; \ldots &amp; x_n^{(m)}<br>
\end{matrix}<br>
\right]<br>
,<br>
y=<br>
\left[<br>
\begin{matrix}<br>
y^{(1)}\\<br>
y^{(2)}\\<br>
\ldots\\<br>
y^{(m)}<br>
\end{matrix}<br>
\right]<br>
\end{equation}$$</p>
<p>约定待求的参数θ的矩阵形式为：</p>
<p>$$\begin{equation}<br>
\theta =<br>
\left[<br>
\begin{matrix}<br>
\theta_1\\<br>
\theta_2\\<br>
\ldots\\<br>
\theta_n<br>
\end{matrix}<br>
\right]<br>
\end{equation}$$</p>
<p>先求$ x \cdot \theta $并记为$ A $：</p>
<h1>$$\begin{equation}<br>
A=x \cdot \theta</h1>
<h1>\left[<br>
\begin{matrix}<br>
x_0^{(1)} &amp; x_1^{(1)} &amp; \ldots &amp; x_n^{(1)}\\<br>
x_0^{(2)} &amp; x_1^{(2)} &amp; \ldots &amp; x_n^{(2)}\\<br>
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\<br>
x_0^{(m)} &amp; x_1^{(m)} &amp; \ldots &amp; x_n^{(m)}<br>
\end{matrix}<br>
\right]<br>
\cdot<br>
\left[<br>
\begin{matrix}<br>
\theta_0\\<br>
\theta_1\\<br>
\ldots\\<br>
\theta_n<br>
\end{matrix}<br>
\right]</h1>
<p>\left[<br>
\begin{matrix}<br>
\theta_0x_0^{(1)} + \theta_1x_1^{(1)} + \ldots + \theta_nx_n^{(1)}\\<br>
\theta_0x_0^{(2)} + \theta_1x_1^{(2)} + \ldots + \theta_nx_n^{(2)}\\<br>
\ldots \\<br>
\theta_0x_0^{(m)} + \theta_1x_1^{(m)} + \ldots + \theta_nx_n^{(m)}<br>
\end{matrix}<br>
\right]<br>
\end{equation}$$</p>
<p>求$ h_\theta(x)-y $并记为$ E $：</p>
<h1>$$\begin{equation}<br>
E=h_\theta(x)-y=<br>
\left[<br>
\begin{matrix}<br>
g(A^{(1)})-y^{(1)}\\<br>
g(A^{(2)})-y^{(2)}\\<br>
\ldots \\<br>
g(A^{(m)})-y^{(m)}<br>
\end{matrix}<br>
\right]</h1>
<p>\left[<br>
\begin{matrix}<br>
e^{(1)}\\<br>
e^{(2)}\\<br>
\ldots\\<br>
e^{(m)}<br>
\end{matrix}<br>
\right]<br>
=g(A)-y<br>
\end{equation}$$</p>
<p>由上式可知$ h_\theta(x)-y $可以由$ g(A)-y $一次计算求得。</p>
<p>再来看一下公式$\eqref{eq:lr-gd}$的$\theta$更新过程：</p>
<p>$$\begin{eqnarray}<br>
\theta_j &amp;=&amp; \theta_j + \alpha \sum_{i=1}^{m}(-e^{(i)})x_j^{(i)}\\<br>
&amp;=&amp; \theta_j-\alpha\cdot(x_j^{(1)},x_j^{(2)},\ldots,x_j^{(m)})\cdot E<br>
\end{eqnarray}$$</p>
<p>综合上面的式子有：</p>
<p>$$\begin{equation}<br>
\theta = \theta - \alpha\cdot\frac{1}{m}\cdot x^T\cdot(g(x\cdot\theta)-y)<br>
\end{equation}$$</p>
<h1>正则化</h1>
<p>由于模型的参数个数一般是由人为指定和调节的，所以正则化常常是用来限制模型参数值不要过大，也被称为惩罚项。一般是在目标函数(经验风险)中加上一个正则化项$ \Phi(w) $即</p>
<p>$$\begin{equation}<br>
J(w) = -\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))] + \lambda \Phi(w)<br>
\label{eq:reg}<br>
\end{equation}$$</p>
<p>而这个正则化项一般会采用L1范数或者L2范数。其形式分别为$ \Phi (w)=||x||_1 $和$ \Phi (w)=||x||_2 $。</p>
<p>首先针对L1范数$ \Phi (w)=|x| $，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化当$ w_j &gt; 0 $是取1，当$ w_j &lt; 0 $时取-1.</p>
<p>从而导致的参数$w_j$减去了学习率与公式的乘积，因此当$ w_j &gt; 0 $的时候，$ w_j$会减去一个正数，导致$ w_j $减小，而当$ w_j &lt; 0 $的时候，$ w_j$会减去一个负数，导致$ w_j$又变大，因此这个正则项会导致参数$ w_j$取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。</p>
<p>然后针对L2范数$\phi(w) = \sum_{j=1}^{n}w_j^2$，同样对它求导，得到梯度变化为$\frac{\partial \Phi(w)}{\partial w_j} = 2w_j$(一般会用$\frac{\lambda}{2}$来把这个系数2给消掉)。同样的更新之后使得$ w_j$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。</p>
<p><strong>需要注意的是</strong>，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而公式$\eqref{eq:reg}$中的$\lambda$也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，$\lambda$越大，对参数值惩罚越大，泛化能力越好。</p>
<h1>《机器学习实战》代码</h1>
<p>梯度上升法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradAscent</span>(<span class="params">dataMatIn, classLabels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;梯度上升法&quot;&quot;&quot;</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.1</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    weights = ones((n, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(maxCycles):</span><br><span class="line">        a = dataMatrix * weights</span><br><span class="line">        h = sigmoid(dataMatrix * weights)  <span class="comment"># 100*3 3*1</span></span><br><span class="line">        error = (labelMat - h)</span><br><span class="line">        weights = weights + alpha / m * dataMatrix.transpose() * error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>随机梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stocGradAscent0</span>(<span class="params">dataMatrix, classLabels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机梯度上升法，但是迭代次数不够，且可能存在局部波动现象&quot;&quot;&quot;</span></span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        h = sigmoid(<span class="built_in">sum</span>(dataMatrix[i] * weights))</span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stocGradAscent1</span>(<span class="params">dataMatrix, classLabels, numIter=<span class="number">150</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;改进的随机梯度上升法&quot;&quot;&quot;</span></span><br><span class="line">    m, n = dataMatrix.shape</span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(numIter):</span><br><span class="line">        dataIndex = <span class="built_in">range</span>(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            alpha = <span class="number">4</span> / (<span class="number">1.0</span> + j + i) + <span class="number">0.01</span> <span class="comment"># alpha在每次迭代时都进行了调整</span></span><br><span class="line">            randIndex = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, <span class="built_in">len</span>(dataIndex))) <span class="comment"># 随机选取样本数据</span></span><br><span class="line">            h = sigmoid(<span class="built_in">sum</span>(dataMatrix[randIndex] * weights))</span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            <span class="keyword">del</span> (dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<h1>问题</h1>
<ol>
<li>
<p>LR为什么使用最大似然函数作为损失函数，而不是用MSE？</p>
<p>选用MSE作为损失函数时，求导形式为$\frac{\partial C}{\partial w}=(\hat{y} - y)\sigma’(z)x$，这个梯度是和sigmoid导数有关的，当模型的输出接近0或者1时，$\sigma’(z)$就会非常小，造成梯度消失的问题。</p>
</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<p><a href="http://blog.csdn.net/dongtingzhizi/article/details/15962797">【机器学习笔记1】Logistic回归总结</a><br>
<a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html">【机器学习算法系列之二】浅析Logistic Regression</a><br>
<a href="http://blog.csdn.net/itplus/article/details/21896453">牛顿法与拟牛顿法学习笔记（一）牛顿法</a></p>
</blockquote>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title>同时配置两个github账户</title>
    <url>/2023/06/26/%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AE%E4%B8%A4%E4%B8%AAgithub%E8%B4%A6%E6%88%B7/</url>
    <content><![CDATA[<p>配置两个github账号，使用时互不干扰！</p>
<span id="more"></span>
<h1>生成两个SSH key</h1>
<p>生成两个key的具体命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t ed25519 -C <span class="string">&quot;one@gmail.com&quot;</span></span><br><span class="line">ssh-keygen -t ed25519 -C <span class="string">&quot;two@gmail.com&quot;</span></span><br></pre></td></tr></table></figure>
<p>注意在输入文件名时将两个文件进行区分</p>
<h1>创建config文件并配置</h1>
<p>继续在 ​​<code>.ssh</code>​​ 目录下创建 <code>config</code> 文件，在 <code>config</code> 文件中添加以下内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># one(one@gmail.com)</span></span><br><span class="line">Host github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_ed25519_one</span><br><span class="line">User one</span><br><span class="line">    </span><br><span class="line"><span class="comment"># two(two@gmail.com)</span></span><br><span class="line">Host two.github.com</span><br><span class="line">HostName github.com</span><br><span class="line">PreferredAuthentications publickey</span><br><span class="line">IdentityFile ~/.ssh/id_ed25519_two</span><br><span class="line">User two</span><br></pre></td></tr></table></figure>
<h1>部署 SSH key</h1>
<p>将各自的ssh key写入到github账户</p>
<h1>远程测试</h1>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -T git@one.github.com</span><br><span class="line">ssh -T git@two.github.com</span><br></pre></td></tr></table></figure>
<p>运行命令后如果出现<code>​​Hi xxxx! You’ve successfully authenticated, but GitHub does not provide shell access.​​</code>时，恭喜你，配置成功了~</p>
<h1>使用</h1>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com: one的用户名/learngit.git</span><br><span class="line">git <span class="built_in">clone</span> git@two.github.com: two的用户名/learngit.git</span><br></pre></td></tr></table></figure>
<p>如果设置了全局的email和username，此时提交到两个仓库的内容均为同一个user，如需设置不同的user，可以取消全局设置，仅在各个项目中设置user和email。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取消全局 用户名/邮箱 配置</span></span><br><span class="line">git config --global --<span class="built_in">unset</span> user.name</span><br><span class="line">git config --global --<span class="built_in">unset</span> user.email</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 单独为每个repo设置 用户名/邮箱</span></span><br><span class="line">git config user.name <span class="string">&quot;one_name&quot;</span> ; git config user.email <span class="string">&quot;one_email&quot;</span></span><br><span class="line">git config user.name <span class="string">&quot;two_name&quot;</span> ; git config user.email <span class="string">&quot;two_email&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>SMOTE算法</title>
    <url>/2021/06/22/SMOTE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>SMOTE（Synthetic Minority Oversampling Technique），合成少数类过采样技术．它是基于随机过采样算法的一种改进方案。</p>
<span id="more"></span>
<h1>算法思想</h1>
<p>SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。</p>
<h1>算法流程</h1>
<ol>
<li>
<p>对于少数类中每一个样本$x$，以欧氏距离为标准计算它到少数类样本集中所有的样本距离，得到$k$近邻</p>
</li>
<li>
<p>根据样本不平衡比例设置一个采样比例以确定采样倍率$N$，对于每一个少数类样本$x$，从其$k$近邻中随机选择若干个样本，假设选择的近邻为$x_n$。</p>
</li>
<li>
<p>对于每一个随机选出的近邻$x_n$，分别与原样本按照如下的公式构建新的样本:<br>
$$<br>
x_{new} = x + rand(0,1)\times (\tilde{x}-x)<br>
$$</p>
</li>
</ol>
<p><img src="smote_sample.png" alt=""></p>
<h1>算法伪代码</h1>
<p><img src="smote.png" alt=""></p>
<h1>算法缺陷</h1>
<ol>
<li>$k$邻近的选择：如何选择$k$才能使算法最优是未知的，需要反复的测试。</li>
<li>分布边缘化为题：SMOTE无法克服非平衡数据集的数据分布问题，如果一个负类样本处在负类样本集的分布边缘,则由此负类样本和相邻样本产生的“人造”样本也会处在这个边缘,且会越来越边缘化,从而模糊了正类样本和负类样本的边界,而且使边界变得越来越模糊。这种边界模糊性,虽然使数据集的平衡性得到了改善,但加大了分类算法进行分类的难度．</li>
</ol>
<h1>引用</h1>
<blockquote>
<ol>
<li><a href="https://www.jianshu.com/p/13fc0f7f5565">SMOTE算法</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>样本</tag>
      </tags>
  </entry>
  <entry>
    <title>最大似然估计（MLE） &amp; 最大后验概率估计（MAP）</title>
    <url>/2021/06/28/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89-%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1%EF%BC%88MAP%EF%BC%89/</url>
    <content><![CDATA[<p><strong>概率</strong>用于在已知一些参数的情况下，预测接下来的观测所得到的结果；</p>
<p><strong>似然性</strong>则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。</p>
<span id="more"></span>
<blockquote>
<p>概率：参数 + 观测 --&gt; 结果；<br>
似然：观测 + 结果 --&gt; 参数；</p>
<p>参数可以理解为描述事件性质的未知数，知道这些参数，就能完全描述一个事件；<br>
可以用 CTR 预估来解释这个过程，最开始我们只有观测（pv）和结果（clk，是否点击），我们假设模型未 LR，那么通过最大似然估计可以得到 LR 的相关参数，最后在线上使用的时候，可以直接根据模型（LR）+ 观测数据（pv）来预估结果（点击的可能性）；</p>
<p>其实就是一个似然估计参数，然后概率预估结果；</p>
</blockquote>
<h1>最大似然估计（Maximum Likelihood Estimation, MLE） or 极大似然估计</h1>
<p>最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计理想地认为，对于极少的样本观测，我们观测到的样本很可能就是发生概率最大的。</p>
<p>假设$x_1,x_2,…,x_n$为独立同分布采样， $\theta$为模型参数，$f$为已知的模型。遵循上述的独立同分布假设，产生上述采样的概率可以表示为：<br>
$$<br>
f(x_1,x_2,…x_n)=f(x_1|\theta)\times f(x_2|\theta)\times … \times f(x_n|\theta)<br>
$$<br>
此时，$x_1,x_2,…,x_n$已知，而模型参数$\theta$未知，因此似然可以定义为<br>
$$<br>
L(\theta|x_1,x_2,…,x_n):=f(x_1,x_2,…,x_n|\theta)=\prod_{i=1}^{n}f(x_i\theta)<br>
$$</p>
<blockquote>
<p>注：上面如果写等式其实是有误解的，最大似然估计是不考虑先验概率的，只是根据观测直接预估模型的参数。可以结合贝叶斯公式，如果去除贝叶斯公式中的先验概率，那么上面两个等式是可以认为是相等的；</p>
</blockquote>
<p>由于小数连乘操作可能造成下溢的问题，所以一般会对似然两边同时取对数进行计算，得到如下的形式：<br>
$$<br>
logL(\theta|x_1,x_2,…,x_n)=\sum_{i=1}^{n}logf(x_i|\theta)<br>
$$<br>
对参数$\theta$的最大似然估计为<br>
$$<br>
\hat{\theta}<em>{mle} = argmax</em>{\theta\in\Theta}\hat{l}(\theta|x_1,x_2,…,x_n)<br>
$$<br>
其中，$\hat{l}=\frac{1}{n}logL$为平均对数似然（其实不求平均也没问题，因为之后是对参数 $\theta$ 求导，和$n$无关)</p>
<p>之后求参数导，导数等于 0，求参数即可；</p>
<h1>最大后验概率估计（Maximum a Posteriori estimation，MAP）</h1>
<p>最大似然估计是求$\theta$ 使得似然函数$P(x_0|\theta)$最大，最大后验估计是求$\theta$使得函数$P(x_n|\theta)P(\theta)$最大，$\theta$自己出现的先验概率也最大（其实就是考虑了参数的先验概率）</p>
<p>MAP 其实是在最大化：<br>
$$<br>
P(\theta|x_n)=\frac{P(x_n|\theta)P(\theta)}{P(x_0)}<br>
$$<br>
观测数据 $x_0$ 是确定的（比如抛硬币，出现：反正正正正反正正正反，把实验做 1000 次，“反正正正正反正正正反”出现了$n$次，那么 $P(x_0)=n/1000$ ，总之，这是一个可以由数据集得到的值），因此可以把分母去掉；</p>
<p>最大化后验概率$P(\theta|x_0)$的意义也很明确，$x_0$已经出现了，要求$\theta$取得什么值是的 $P(x_n|\theta)$最大；</p>
<h1>最大似然估计 vs 最大后验概率估计</h1>
<p>最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；</p>
<p>为什么先验概率加进去有用？</p>
<p>比如抛硬币，先验概率认为，正面朝上概率的最大可能是 0.5；但是在实际抛硬币的过程中，可能会出现抛 10 次 7 次正面的情况，这种情况下，最大似然估计会认为正面朝上的概率最大可能是 0.7，最大后验概率估计会综合考虑实验和先验概率，因此认为正面朝上的概率最大可能是 0.5-0.7 之间的一个值，相当于先验概率对其做了一个校正；</p>
<p>最大似然估计可以认为是频率学派的观点，最大后验概率估计可以认为是贝叶斯学派的观点；</p>
<p><strong>后验概率 := 似然 * 先验概率</strong></p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/46737512">最大似然估计（MLE） &amp; 最大后验概率估计（MAP）</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习损失函数</title>
    <url>/2021/06/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>损失函数旨在表示出logit和label的差异程度，不同的损失函数有不同的表示意义，也就是在最小化损失函数过程中，logit逼近label的方式不同，得到的结果可能也不同。</p>
<span id="more"></span>
<p>损失函数（loss function）是用来<strong>估量模型的预测值$f(x)$与真实值$Y$的不一致程度</strong>，它是一个非负实值函数,通常使用$L(Y, f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：<br>
$$<br>
\theta^*=arg  min_{\theta}{\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i,\theta))+\lambda\Phi(\theta)}<br>
$$</p>
<h1>分类损失</h1>
<h2 id="Entropy">Entropy</h2>
<h2 id="Cross-Entropy">Cross Entropy</h2>
<h2 id="K-L-Divergence">K-L Divergence</h2>
<h2 id="Dice-Loss">Dice Loss</h2>
<h2 id="Focal-Loss">Focal Loss</h2>
<h2 id="Tversky-Loss">Tversky Loss</h2>
<h1>回归损失</h1>
<h2 id="L1-Loss-Mean-Absolute-Error-MAE">L1 Loss (Mean Absolute Error MAE)</h2>
<p>它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。<br>
$$<br>
L = \sum_{i=1}^{n}{|Y_i - f(x_i)|}<br>
$$<br>
<strong>优点：</strong> 收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。</p>
<p><strong>缺点：</strong> 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。</p>
<h2 id="L2-Loss-Mean-Squred-Error-MSE">L2 Loss(Mean Squred Error MSE)</h2>
<p>它衡量的是预测值与真实1值之间距离的平方和，作用范围同为0到正无穷。<br>
$$<br>
L=\sum_{i=1}^{n}{(Y_i-f(x_i))^2}<br>
$$<br>
均方误差可用于线性回归的一个原因是：我们假设观测中包含噪声，其中噪声服从正态分布：<br>
$$<br>
y= \boldsymbol{w}^T \boldsymbol{x} + b +\epsilon<br>
$$<br>
其中， $\epsilon ~ N(0,\sigma^2)$。</p>
<p>通过给定的$\boldsymbol{x}$观测到特定$y$的似然（likelihood）：<br>
$$<br>
P(y|\boldsymbol{x})=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left( -\frac{1}{2\sigma^2}(y-\boldsymbol{w}^T \boldsymbol{x} - b)^2 \right)<br>
$$<br>
根据最大似然估计法，参数$\boldsymbol{w}$和$b$的最优值是使整个数据集的似然最大的值：<br>
$$<br>
P(\boldsymbol{y}|\boldsymbol{X})=\prod_{i=1}^{n}p(y^{(i)}|\boldsymbol{x}^{(i)})<br>
$$</p>
<p>最小化负对数似然函数有：<br>
$$<br>
-log(P(\boldsymbol{y}|\boldsymbol{X}))=\sum_{i=1}^{n}\frac{1}{2}log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\left(y^{(i)}-\boldsymbol{w}^T\boldsymbol{x}^{i}-b \right)^2<br>
$$<br>
而$\sigma$是某个固定的常数，于是上述公式第二部分与均方差误差一致。因此，在高斯噪声的假设下，最小化均方差等价于对线性模型的极大似然估计。</p>
<h2 id="Smooth-L1-Loss">Smooth L1 Loss</h2>
<h2 id="IoU-Loss">IoU Loss</h2>
<h2 id="GloU-Loss">GloU Loss</h2>
<h2 id="DloU-Loss">DloU Loss</h2>
<h2 id="CloU-Loss">CloU Loss</h2>
<h2 id="F-EIoU-Loss">F-EIoU Loss</h2>
<h2 id="CDloU-Loss">CDloU Loss</h2>
<h1>LogLoss对数损失函数</h1>
<p>log损失函数的标准形式<br>
$$<br>
L(Y,P(Y|X)) = - logP(Y|X)<br>
$$</p>
<p>损失函数$L(Y, P(Y|X))$表达的是样本$X$在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，<strong>就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。</strong></p>
<p>取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。</p>
<p>todo:</p>
<p>$$<br>
max(L) --&gt; min(-L)<br>
$$</p>
<p>各种损失函数的适用场景</p>
<h1>参考文献</h1>
<blockquote>
<p>1.<a href="https://www.cnblogs.com/guoyaohua/p/9217206.html">一文读懂机器学习常用损失函数（Loss Function）</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>loss</tag>
      </tags>
  </entry>
  <entry>
    <title>树模型原理与区别</title>
    <url>/2021/06/28/%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E4%B8%8E%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>RF,GBDT,XGBoost,lightGBM都属于集成学习（Ensemble Learning），集成学习的目的是通过结合多个基学习器的预测结果来改善基本学习器的泛化能力和鲁棒性。</p>
<span id="more"></span>
<h1>随机森林（RandomForest）</h1>
<p><strong>原理：</strong></p>
<p>Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择。</p>
<p>因此可以概括RF包括四个部分：</p>
<p>1、随机选择样本（放回抽样）；</p>
<p>2、随机选择特征属性；</p>
<p>3、构建决策树；</p>
<p>4、随机森林投票（平均）， 因此防止过拟合能力更强，降低方差。</p>
<p><strong>优点：</strong></p>
<ol>
<li>随机森林算法能解决分类与回归两种类型的问题，表现良好，由于是集成学习，方差和偏差都比较低，泛化性能优越；</li>
<li>随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出特征的重要性程度，这是一个非常实用的功能。</li>
<li>可以应对缺失数据；</li>
<li>当存在分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法；</li>
<li>高度并行化，易于分布式实现</li>
<li>由于是树模型 ，不需要归一化即可之间使用</li>
</ol>
<p><strong>缺点：</strong></p>
<p>随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。</p>
<h1>GBDT (Gradient Boosting Decision Tree)</h1>
<p><strong>原理：</strong></p>
<p>GradientBoosting算法关键是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。</p>
<p>GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是<strong>CART回归树</strong>，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树) 因为Gradient Boosting 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。</p>
<p><strong>优点：</strong></p>
<ol>
<li>它能灵活的处理各种类型的数据；</li>
<li>在相对较少的调参时间下，预测的准确度较高。</li>
</ol>
<p><strong>缺点：</strong></p>
<p>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</p>
<h1>XGBoost</h1>
<p>XGBoost与GBDT的区别： 在了解了XGBoost原理后容易理解二者的不同</p>
<ol>
<li>
<p><strong>损失函数的改变：</strong>（导数和正则项的认识）</p>
<p>传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；</p>
<p>传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；</p>
<p>XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；</p>
</li>
<li>
<p><strong>工具的优化</strong>：（趋势值和并行的认识）</p>
<p>shrinkage（缩减），相当于学习速率（XGBoost中的eta）。</p>
</li>
<li>
<p><strong>列抽样</strong>：XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；</p>
</li>
<li>
<p><strong>对缺失值的处理</strong>：对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；</p>
</li>
<li>
<p><strong>XGBoost工具支持并行</strong></p>
<p>注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。</p>
<p>XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。</p>
<p>这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。</li>
<li>预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/62207593">RandomForest、GBDT、XGBoost、lightGBM 原理与区别</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/142413825">机器学习 | XGBoost详解</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>todo</tag>
        <tag>GBDT</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title>社区发现-Fast Unfolding算法</title>
    <url>/2021/06/28/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0-Fast-Unfolding%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>在社交网络中，有些用户之间联系较为紧密，而另外一些用户之间的关系则较为稀疏。在网络中，我们可以将联系较为紧密的部分用户看成一个社区，在这个社区内部，用户之间联系紧密，而在两个社区之间，联系较为稀疏。</p>
<span id="more"></span>
<h1>社区划分的评价标准</h1>
<p>利用算法将整个网络划分成多个社区之后，需要一个评价指标来衡量这个划分结果的好坏。fast unfolding算法采用的是**模块度（Modularity）**Q值来衡量。</p>
<p><strong>模块度</strong>可以定义为：社区内部的总边数和网络中总边数的比例减去一个期望值，该期望值是将网络设定为随机网络时同样的社区分配所形成的社区内部的总边数和网络中总边数的比例的大小。<br>
$$<br>
Q = \frac{1}{2m}\sum_{vw}\left[A_{vw}-\frac{k_v k_w}{2m}\right]\delta(c_v,c_w)<br>
$$</p>
<p>其中，$A_{vw}$为网络中邻接矩阵中的一个元素：</p>
<p>$$<br>
A_{vw}=<br>
\begin{cases}<br>
1&amp; 点v和w是相连的\\<br>
0&amp; 其它<br>
\end{cases}<br>
$$</p>
<p><strong>社区内部的边数和网络的总边数的比例：</strong></p>
<p>$m$为整个网络中的边数<br>
$$<br>
m=\frac{1}{2}\sum_{vw}A_{vw}<br>
$$</p>
<p>$c_v$表示点$v$所属的社区，当$i$,$j$存在于同一个社区中时，$\delta(i,j) = 1$，否则为0。</p>
<p>于是，社区内部的边数和网络的总边数的比例为：<br>
$$<br>
\frac{\sum_{vw}A_{vw}\delta(c_v,c_w)}{\sum_{vw}A_{vw}} = \frac{1}{2m}\sum_{vw}A_{vw}\delta(c_v,c_w)<br>
$$</p>
<p><strong>随机网络的总边数和网络中总边数的比例：</strong></p>
<p>定义$k_v$表示点$v$的度，即<br>
$$<br>
k_v = \sum_w A_{vw}<br>
$$</p>
<p>则将网络设定成随机网络，并进行相同的社区分配操作形成的社区内部的总边数和网络中总边数的比例的大小为$\frac{k_vk_w}{2m}$。</p>
<blockquote>
<p>网络代表所有与原网络有一样的度序列的网络平均。下图说明了如何保持节点的度序列来随机化网络连边。</p>
<p><img src="fast_unfolding0.jpg" alt="随机化网络"></p>
<ul>
<li>将网络的边都断成两段。度为k的节点有$k$个“半边”</li>
<li>每条“半边”都随机寻找其他的“半边”配成一个整边</li>
<li>容易发现，这样得到的新的网络跟原始的网络有同样的度序列</li>
<li>节点$i$有$k_i$个“半边”，每个半边恰好连接的是节点$j$的“半边”的概率为$\frac{k_j}{2m}$</li>
<li>平均来说，新的网络中，节点i与节点j的期望连边数为$\frac{k_ik_j}{2m}$。</li>
</ul>
</blockquote>
<p><strong>模块度变形</strong>：</p>
<p>定义$e_{ij}$为社区$i$与社区$j$之间的边数占网络中所有边数的占比，即<br>
$$<br>
e_{ij} = \frac{1}{2m} \sum_{vw}A_{vw}\delta(c_v,i)\delta(c_w,j)<br>
$$</p>
<p>定义$a_i$为连接到社区$i$的边数占网络中所有边数的占比，即</p>
<p>$$<br>
a_i = \frac{1}{2m} \sum_{v}k_{v}\delta(c_v,i)<br>
$$</p>
<p>同时，由于$\delta(c_v,c_w)=\sum_i\delta(c_v,i)\delta(c_w,i)$. 则模块度的计算可以简化为：</p>
<p>$$<br>
\begin{eqnarray*}<br>
Q &amp; = &amp; \frac{1}{2m}\sum_{vw}\left[A_{vw}-\frac{k_v k_w}{2m}\right]\sum_i\delta(c_v,i)\delta(c_w,i) \\<br>
&amp; = &amp; \sum_i \left[\frac{1}{2m}\sum_{vw}A_{vw}\delta(c_v,i)\delta(c_w,i)-\frac{1}{2m}\sum_v k_v\delta(c_v,i)\frac{1}{2m}\sum_w k_w\delta(c_w,i)\right]\\<br>
&amp; = &amp; \sum_i (e_{ii}-a_i^2)<br>
\end{eqnarray*}<br>
$$</p>
<h1>Fast Unfolding算法</h1>
<p>在社区发现问题中，以前的研究人员提出了许多的方法，例如标签传播算法（Label Propagation Algorithm）、Fast Unfolding等。考虑到现有数据的规模和算法的复杂度等因素，本文选用的是fast unfolding。</p>
<p>Fast Unfolding算法的主要目标是不断划分社区使得划分后的整个网络的模块度不断增大。算法主要包括两个过程，过程示例如下。</p>
<p><img src="fast_unfolding.png" alt="fast unfolding示意图"></p>
<ol>
<li>
<p><strong>Modularity Optimization</strong>，这一过程主要讲节点与邻近的社区进行合并，使得网络的模块度不断变大。</p>
<p>定义$\sum_{in}$为社区$C$内所有边的权重和，$\sum_{tot}$为与社区$C$内的点连接的边的权重和，$k_i$为所有连接到节点$i$上的边的权重和，$k_{i,in}$为节点$i$与社区$C$内的节点连接的边的权重和，$m$是网络中所有边的权重和。</p>
<p>则将节点$i$划分到社区$C$中产生的模块度的变化$\Delta Q$可用下式计算<br>
$$<br>
\begin{eqnarray*}<br>
\Delta Q &amp; = &amp; \left[\frac{\sum_{in} + k_{i,in}}{2m} - \left(\frac{\sum_{tot}+k_i}{2m}\right)^2\right]-\left[\frac{\sum_{in}}{2m} - \left(\frac{\sum_{tot}}{2m}\right)^2 - \left(\frac{k_i}{2m}\right)^2\right]\\<br>
&amp; = &amp; \frac{k_{i,in}}{2m} - \frac{k_i\sum_{tot}}{2m^2}<br>
\end{eqnarray*}<br>
$$</p>
<p>根据上式，我们只需要知道社区中与该节点连接的边的权重之和，以及社区中的点连接的边的权重和就可以计算模块度的变化量。</p>
</li>
<li>
<p><strong>Commnunity Aggregation</strong>这一过程将第一步中的社区汇聚成一个点，重构网络结果。</p>
<p>这一步中，将原来的两个社区之间的边的<strong>权重和</strong>作为新的节点之间的权重，将社区内的权重和作为新节点上的<strong>环向边</strong>的权重。</p>
<p>可以看到，做Graph folding的意义在于调整Modularity Optimization的基本单位。这么做的主要原因是</p>
<ul>
<li>基于邻居的Modularity Optimization优化在后期变化非常缓慢，需要通过调整变量的粒度来加速算法的收敛。</li>
<li>Graph folding还使得算法的视野更远，即每一步Modularity Optimization将涉及到更多的非邻居节点。</li>
</ul>
</li>
</ol>
<p>fast unfolding算法将重复迭代以上过程，直至网络的结构不变（$\Delta Q$小于某个阈值）。</p>
<h1>Fast unfolding算法的应用</h1>
<ol>
<li>寻找黑浓度较高的社区，判定成黑社区</li>
<li>黑浓度较低的社区，判定成灰规则</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<p><a href="http://blog.csdn.net/wangyibo0201/article/details/52048248">模块度Q——复杂网络社区划分评价标准</a><br>
<a href="http://ece-research.unm.edu/ifis/papers/community-moore.pdf">Finding community structure in very large networks</a><br>
<a href="https://arxiv.org/pdf/0803.0476.pdf">Fast unfolding of communities in large networks</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>社区发现</tag>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Dropout</title>
    <url>/2021/08/23/Dropout/</url>
    <content><![CDATA[<p>Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。</p>
<span id="more"></span>
<h1><strong>Dropout简介</strong></h1>
<h2 id="Dropout出现的原因"><strong>Dropout出现的原因</strong></h2>
<p>训练深度神经网络的时候，总是会遇到两大缺点:</p>
<ol>
<li>容易过拟合</li>
<li>费时</li>
</ol>
<p>过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p>
<h2 id="什么是Dropout">什么是Dropout</h2>
<p>Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p>
<p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图所示。</p>
<p><img src="dropout.jpg" alt="dropout"></p>
<h1>Dropout训练和预测时的不同</h1>
<p><strong>在训练阶段：</strong></p>
<ol>
<li>首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变</li>
<li>然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）</li>
<li>在p是神经元抛弃概率时，训练后神经元参数需要进行$\frac{1}{1-p}$缩放</li>
</ol>
<blockquote>
<p>当模型使用dropout layer时，训练的时候只有占比为$p$的隐层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大$\frac{1}{p}$，为了避免这种情况，就需要测试的时候将输出结果乘以 $p$使下一层的输入规模保持不变。</p>
<p>而利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大$\frac{1}{p}$倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。</p>
</blockquote>
<p><strong>在测试阶段：</strong></p>
<p>直接利用所有训练好的神经元权重计算输出，不需要dropout</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>样本不均衡问题</title>
    <url>/2021/06/21/%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>类别不平衡问题指的是数据集中各个类别的样本数量极不均衡。</p>
<span id="more"></span>
<h1>定义</h1>
<p>通常把样本类别比例超过3:1的数据成为不平衡数据。</p>
<h1>影响</h1>
<p>多数数据样本带有的信息量比少数样本信息量大，会使得我们的分类模型存在很严重的偏向性。</p>
<p><strong>直观的例子</strong>：根据1000个正样本和1000个负样本正确训练出了一个精确率90%，召回率90%的分类器，且通过实验验证没有欠采样过采样的问题。直到有一天，数据发生了一点变化，还是原来的数据类型和特征，只是每天新数据中正负样本变成了100个正样本，10000个负样本。注意，先前精确率90%的另一种表达是负样本有10%的概率被误检为正样本。模型不变，现在误检的负样本数10000 * 0.1=1000个，正样本被检出100 * 0.9（召回）=90个，这个时候召回率不变仍为90%，但是新的精确率=90 / (1000+90)=8.26%</p>
<h1>解决方法</h1>
<ol>
<li><strong>扩大数据集</strong>：更多的数据往往意味着更多的信息。</li>
<li><strong>数据重采样</strong>：
<ol>
<li>过采样：对小类数据进行采样，增加小类数据样本量。<strong>随机过采样容易产生模型过拟合的问题，即使得模型学习到的信息过于特别(Specific)而不够泛化(General)</strong></li>
<li>欠采样：对大类数据进行采样，减少大类数据样本量。</li>
</ol>
</li>
<li><strong>人工构造样本</strong>：<a href="/2021/06/22/SMOTE%E7%AE%97%E6%B3%95/" title="SMOTE算法">SMOTE算法</a>（Synthetic Minority Oversampling Technique），基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。</li>
</ol>
<h1>个人思考</h1>
<p>对于不同的业务场景，模型的目标不一样，如癌症识别业务适合高召回的模型，而恶意识别适合高准确的模型。</p>
<h1>引用</h1>
<blockquote>
<ol>
<li><a href="https://blog.csdn.net/songhk0209/article/details/71484469">解决样本不平衡问题的奇技淫巧 汇总</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56882616">炼丹笔记一：样本不平衡问题</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>样本</tag>
      </tags>
  </entry>
  <entry>
    <title>概率分布函数和概率密度函数</title>
    <url>/2021/06/26/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E5%92%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>如何理解概率分布函数和概率密度函数的问题。</p>
<span id="more"></span>
<h1>离散型</h1>
<h2 id="变量定义">变量定义</h2>
<p>**离散型随机变量：**如果随机变量的值可以都可以逐个列举出来，则为离散型随机变量。</p>
<h2 id="概率函数">概率函数</h2>
<p><strong>概率函数</strong>，就是用函数的形式来表达概率。<br>
$$<br>
p_i = P\{X=x_i\}<br>
(i=1,2,3,4,5,6)<br>
$$</p>
<h2 id="概率分布">概率分布</h2>
<p><strong>概率分布</strong>，用于表述随机变量取值的概率规律。</p>
<p><img src="%E7%A6%BB%E6%95%A3%E5%9E%8B%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83.png" alt="离散型概率分布"></p>
<h2 id="概率分布函数">概率分布函数</h2>
<p>又称累<strong>积概率函数</strong><br>
$$<br>
F(x)=P(X\le x) = \sum_{x_k \le x}p_k<br>
$$</p>
<h1>连续型</h1>
<h2 id="变量定义-2">变量定义</h2>
<p>**连续性随机变量：**如果随机变量X的取值无法逐个列举则为连续型变量。</p>
<h2 id="概率密度函数">概率密度函数</h2>
<p>对标离散型变量的概率函数：<br>
$$<br>
P(a \le X \le b) = F(b) - F(a) = \int ^b_a(x)dx<br>
$$</p>
<h2 id="概率分布函数-2">概率分布函数</h2>
<p>概率分布函数为概率密度函数的积分<br>
$$<br>
F(x)=P(X \le x) = \int_{-\infty}^x(x)dx<br>
$$<br>
<img src="%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0.png" alt="概率分布函数与概率密度函数"></p>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程</title>
    <url>/2021/06/28/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！</p>
<span id="more"></span>
<h1>数据描述</h1>
<ul>
<li>统计值：max, min, mean, std等</li>
<li>集中趋势</li>
<li>分布形状</li>
</ul>
<h1>特征处理</h1>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="缺失值处理">缺失值处理</h3>
<ol>
<li>缺失值删除
<ul>
<li>删除样本</li>
<li>删除特征</li>
</ul>
</li>
<li>缺失值填充
<ul>
<li>固定值填充，如0，999，-999</li>
<li>均值填充</li>
<li>众数填充</li>
<li>上下数据填充</li>
<li>插值法填充</li>
<li>KNN填充</li>
<li>random forest填充</li>
<li>不填充： LightGBM和XGBoost都能对NaN数据进行学习，不需要处理缺失值</li>
</ul>
</li>
</ol>
<h3 id="异常值处理">异常值处理</h3>
<ol>
<li>基于统计的异常点检测算法 例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。</li>
<li>基于距离的异常点检测算法 主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离(曼哈顿距离)、欧氏距离和马氏距离等方法。</li>
<li>基于密度的异常点检测算法 考察当前点周围密度，可以发现局部异常点。</li>
</ol>
<h2 id="特征转换">特征转换</h2>
<h3 id="连续型特征">连续型特征</h3>
<ol>
<li>
<p><strong>函数转换</strong>：有时我们的模型的假设条件是要求自变量或因变量服从某特殊分布（如正太分布），或者说自变量或因变量服从该分布时，模型的表现较好。</p>
</li>
<li>
<p><strong>特征缩放</strong>：某些模型（像岭回归）要求你必须将特征值缩放到相同的范围值内。通过缩放可以避免某些特征比其他特征获得大小非常悬殊的权重值。</p>
</li>
<li>
<p><strong>无量纲化</strong>：无量纲化使不同规格的数据转换到同一规格。</p>
<ul>
<li>
<p><strong>标准化</strong>：</p>
<ul>
<li>均值方差法</li>
<li>z-score标准化</li>
<li>StandardScaler标准化</li>
</ul>
</li>
<li>
<p><strong>归一化</strong>：</p>
<ul>
<li>最大最小归一化</li>
<li>对数函数转化（log）</li>
<li>反余切转化</li>
</ul>
</li>
<li>
<p><strong>区间缩放法</strong>：区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放。<br>
$$<br>
x^{\prime}=\frac{x-Min}{Max-Min}<br>
$$</p>
</li>
</ul>
</li>
<li>
<p><strong>二值化（定量特征）</strong>：特征的二值化处理是将数值型数据输出为布尔类型。其核心在于设定一个阈值，当样本书籍大于该阈值时，输出为1，小于等于该阈值时输出为0。</p>
</li>
<li>
<p><strong>离散化分箱处理</strong>：将数值型属性转换成类别型更有意义，同时将一定范围内的数值划分成确定的块，使算法减少噪声的干扰，避免过拟合。</p>
<ul>
<li>
<p>等值划分</p>
</li>
<li>
<p>等频划分</p>
</li>
</ul>
</li>
</ol>
<h3 id="离散化特征">离散化特征</h3>
<ol>
<li>
<p><strong>数值化处理</strong>：将类别属性转换成一个标量，最有效的场景应该就是二分类的情况。这种情况下，并不需要排序，并且你可以将属性的值理解成属于类别1或类别2的概率。 多分类问题：选取多分类，编码到[0，classnum)。</p>
<p>该方法局限性较大：</p>
<ul>
<li>不适用于建立预测具体数值的模型，比如线性回归，只能用于分类，</li>
<li>即使用于分类，也有一些模型不适合，</li>
<li>可能结果的精度不如one-hot编码。</li>
</ul>
</li>
<li>
<p><strong>哑编码</strong>：</p>
<ul>
<li>
<p>独热编码（one-hot)：数据集中的每个实例，只有一个是1（其他的为0）</p>
<ul>
<li>优点：简单，且保证无共线性。</li>
<li>缺点：太稀（稀疏矩阵）</li>
</ul>
</li>
<li>
<p>顺序性哑变量：将一个变量的k个值生成k个哑变量，保护了特征的顺序关系。</p>
<table>
<thead>
<tr>
<th>status</th>
<th>向量表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>bad</td>
<td>(1,0,0)</td>
</tr>
<tr>
<td>normal</td>
<td>(1,1,0)</td>
</tr>
<tr>
<td>good</td>
<td>(1,1,1)</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ol>
<h1>特征选择</h1>
<h2 id="特征检验">特征检验</h2>
<h3 id="单变量">单变量</h3>
<ol>
<li>正态性检验</li>
<li>显著性分析</li>
</ol>
<h3 id="多变量">多变量</h3>
<ol>
<li>一致性检验</li>
<li>多重共线性</li>
</ol>
<h2 id="特征选择">特征选择</h2>
<h3 id="Filter：过滤法">Filter：过滤法</h3>
<p>按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。过滤式特征选择的评价标准分为四种，即距离度量、信息度量、关联度度量以及一致性度量。</p>
<p>**优点：**算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。</p>
<p><strong>缺点</strong>：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。</p>
<ol>
<li>
<p><strong>方差选择法</strong></p>
<p>计算各个特征的方差，选择方差大于阈值的特征 。</p>
</li>
<li>
<p><strong>相关系数法</strong></p>
<p>计算各个特征对目标值的相关系数以及相关系数的P值。<br>
$$<br>
\rho_{\boldsymbol{x},\boldsymbol{y}}=\frac{\text{cov}(\boldsymbol{x},\boldsymbol{y})}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}}=\frac{E[(\boldsymbol{x}-\mu_\boldsymbol{x},\boldsymbol{y}-\mu_\boldsymbol{y})]}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}}<br>
$$<br>
使用条件：</p>
<ol>
<li>两个变量间有线性关系；</li>
<li>变量是连续变量；</li>
<li>变量均符合正态分布，且二元分布也符合正态分布；</li>
<li>两变量独立；</li>
<li>两变量的方差不为 0；</li>
</ol>
</li>
<li>
<p><strong>互信息法</strong></p>
<p>互信息（mutual information）是用来评价<strong>一个事件的出现对于另一个事件的出现所贡献的信息量。</strong><br>
$$<br>
I(X;Y) = \sum\limits_{y \in \mathcal{Y}}\sum\limits_{x \in \mathcal{X}} p(x,y) ,\text{log}\left(\frac{p(x,y)}{p(x)p(y)}\right)<br>
$$<br>
而如果 𝑥 和 𝑦 是相互独立的随机变量，则 𝑝(𝑥,𝑦)=𝑝(𝑥)𝑝(𝑦) ，那么上式为 0。因此若 𝐼(𝑋;𝑌)越大，则表示两个变量相关性越大，于是就可以用互信息来筛选特征。</p>
</li>
<li>
<p><strong>卡方检验（Chi-Square）</strong></p>
<p>卡方检验恰好可以进行<strong>独立性检验</strong>，所以其适用于特征选择。</p>
<blockquote>
<p>卡方分布，其定义如下：</p>
<p>设随机变量$x_1, x_2 … x_n ,,\quad \text{i.i.d} \sim N(0,1)$，即独立同分布于标准正态分布，那么这 𝑛个随机变量的平方和：<br>
$$<br>
X = \sum\limits_{i=1}^n x_i^2<br>
$$<br>
构成一个新的随机变量，其服从自由度为 𝑛 的卡方分布 ( 𝜒2 分布) ，记为 $X \sim \chi^2_n$。</p>
</blockquote>
<p>计算检验统计量 𝜒2 ( 𝜒2χ表示卡方值) ，𝜒2 越大，表示观测值和理论值相差越大，当 𝜒2 大于某一个临界值时，就能获得统计显著性的结论。<br>
$$<br>
\chi^2 = \sum\frac{(观测频数 - 期望频数)^2}{期望频数}= \sum_{i=1}^{r} \sum_{j=1}^{c} {(O_{i,j} - E_{i,j})^2 \over E_{i,j}}<br>
$$</p>
</li>
</ol>
<h3 id="Wrapper：包装法">Wrapper：包装法</h3>
<p>根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</p>
<h3 id="Embedded：嵌入法">Embedded：嵌入法</h3>
<p>先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/111296130">深度了解特征工程</a></li>
<li><a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">常用的特征选择方法之 Pearson 相关系数</a></li>
<li>[<a href="https://www.cnblogs.com/massquantity/p/10486904.html">特征选择： 卡方检验、F 检验和互信息</a>](<a href="https://www.cnblogs.com/massquantity/p/10486904.html">https://www.cnblogs.com/massquantity/p/10486904.html</a>)</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>ABCNN</title>
    <url>/2021/07/08/ABCNN/</url>
    <content><![CDATA[<p>ABCNN是一种基于卷积神经网络CNN和Attention机制的算法，应用于文本分类等业务场景中。</p>
<span id="more"></span>
<h1>主要思路</h1>
<h2 id="BCNN">BCNN</h2>
<p><strong>BCNN结构</strong>：使用基本CNN网络（无attention机制），每次处理一对句子，输出层解决sentence pair task.</p>
<p><img src="bcnn.png" alt="bcnn"></p>
<p>假设输入两个句子 $s_0$ 和 $s_1$，句子长度分别为 $l_0$ 和 $l_1$。输入的长度取 $s = max(l_0,l_1)$ ,短的补0。</p>
<h3 id="Input-layer">Input layer</h3>
<p>每个句子被填充为相同长度$s$，使用词向量映射每个句子为矩阵$ d_0\times s$维矩阵。</p>
<h3 id="Convolution-layer">Convolution layer</h3>
<p>卷积核取$d_0 \times w$,沿词方向做卷积，左右做padding，分别加长度$w-$1 补0。则卷积后得到特征长度为$s + 2 \times (w-1) - w + 1 = s+w-1$。叠加$d_1$ 个卷积核，最终得到新的词向量，维度为$d_1 \times (s+w-1)$ 。卷积核的高须为词向量的维度。</p>
<p>设词向量分别为$v_1,v_2,…,v_s$。记$c_i = [v_{i-w+1},v_{i-w+2},…,v_i]$，$c_i$的维度为$d_0\times w$。$p_i$为卷积$c_i$后的特征，则有：</p>
<p>$$<br>
p=tanh(Wc_i + b)<br>
$$</p>
<p>其中，$W$为卷积核集合，即为卷积权重，其维度为$d_1\times d_0\times w$，$b_i$为对应的偏置。</p>
<h3 id="Pooling-layer">Pooling layer</h3>
<p>沿词的方向，步长为$step$,长度为 $wp $ ,取需要池化的向量$p_i$,维度为 $d_0\times w$,经过池化函数后，维度变为$d_0\times 1 $。池化函数作用的是词向量沿着词维度的特征集合。池化函数可取均值，最大值等。</p>
<p><strong>all-ap</strong>：对所有特征列做column-wise的pooling操作，得到 $s \times 1$维度的输出向量</p>
<p><strong>w-ap</strong>：卷积核长度设置为$w$，对窗口$w$内的特征列做column-wise的polling操作，得到$s+w-1$的输出向量。</p>
<h3 id="Output-layer">Output layer</h3>
<p>就是对整个特征层做池化。输出特征向量后，把两个句子的特征向量拼接起来得到特征。</p>
<h2 id="ABCNN">ABCNN</h2>
<p>ABCNN（Attention-Based BCNN）有3种结构，ABCNN-1，ABCNN-2，ABCNN-3。</p>
<h3 id="ABCNN-1">ABCNN-1</h3>
<p><img src="abcnn_1.png" alt="ABCNN-1"></p>
<p>结构中增加了一个抽象层级，就是在原有单词级上增加了一个短语级的抽象。单词级的抽象文中重新命名为unit,作为低级别的表示，短语级的作为更高一级的表示。图中那个红色的与BCNN网络中的输入是一样的，是句子的词向量矩阵，两个句子。第一个句子5个单词，第二个句子7个单词。</p>
<p>蓝色的为短语级高一级的词向量表示。蓝色表示是由Attention Matrix A和红色词向量计算生成</p>
<p>Attention Matrix $A$是由左右两个句子的情况生成。$A$中的 $i$ 列值是由左边句子（五个单词）中第 $i$ 个单词（unit）的向量与右边句子的Attention值分布$j$行值是由右边句子中第$j$个单词的向量与左边句子的Attention值分布</p>
<p>注意力矩阵$A$定义句子间词的关系，即：<br>
$$<br>
A_{i,j}=score(F_{0,r}[:,i],F_{1,r}[:,j])<br>
$$<br>
其中，$F_{i,r[:,k]}$定义为第$i$个句子的第$k$个词向量。<br>
$$<br>
score(x,y)=\frac{1}{1+|x-y|}<br>
$$<br>
生成注意力矩阵后，利用$F_{0,a}=W_0A^T，F_{1,a}=W_1A$ 得到句子对应的注意力特征矩阵。叠加到句子的特征矩阵中，进行卷积。</p>
<h3 id="ABCNN-2">ABCNN-2</h3>
<p><img src="abcnn_2.png" alt="ABCNN-2"></p>
<p>ABCNN-2架构是在以初始词向量形式输入并经过卷积后的输出的向量表示中（两个句子分别变成了7col和9col），计算出Attention Matrix A，计算方法还是计算两个句子对应单词向量的欧式距离，生成向量矩阵，方法同ABCNN-1的架构。</p>
<p>第二步：计算卷积向量权重，给每个单词计算一个权重值。左边句子（7col）的每个单词对应的Attention权重是由Matrix A中列向量求和的值作为权重值，col-wise sum，右边句子中每个单词的权重值是Matrix A中行向量求和值作为权重值，row-wise sum.<br>
$$<br>
a_{0,j}=\sum A[j,:]t<br>
$$<br>
句子原始输入是词向量矩阵，左边的是$5<em>d$，右边是$7</em>d$，$w$是3<br>
$$<br>
F_{i,r}^c \in R^{d\times(s_i+w-1)}<br>
$$<br>
将卷积输出的特征矩阵，基于Attention权重值，做池化。$i$取值为0,1，$j$取值为句子长度。<br>
$$<br>
F_{i,r}^p[:,j]=\sum_{k=j:j+2}a_{i,k}F_{i,r}^c[:,k], j = 1…s_i<br>
$$<br>
<strong>ABCNN-1和ABCNN-2比较:</strong></p>
<ol>
<li>ABCNN-1中Attention是间接的影响卷积，ABCNN-2中通过Attention权重直接影响池化。</li>
<li>ABCNN-1需要两个权重矩阵需要学习，并且输入特征矩阵要两次， 相比ABCNN-2网络需要更多的参数学习，容易过拟合。</li>
<li>ABCNN-2执行更大粒度的池化，如果在卷积层输入的是词粒度的，那么ABCNN-2在池化时就已经是短语粒度的（经过卷积了），池化时的w和卷积的w保持一致。</li>
</ol>
<h3 id="ABCNN-3">ABCNN-3</h3>
<p><img src="abcnn_3.png" alt="ABCNN-3"></p>
<p>将ABCNN-1和ABCNN-2结合，作为架构，这样保留word level信息，也增加了phrase level的信息。更加高层次的特征抽象。</p>
<h1>工程网络图</h1>
<h2 id="BCNN-2">BCNN</h2>
<pre class="mermaid">graph BT 

subgraph Input Layer
X1("x1 [None, 300, 40]")
X2("x2 [None, 300, 40]")

X1 --> X1_expand("x1_expand [None,300, 40,1]")
X2 --> X2_expand("x2_expand [None,300, 40,1]")
end

X1_expand --all_pool--> left_ap_0("left_ap_0 [None, 300]")
X2_expand --all_pool--> right_ap_0("right_ap_0 [None, 300]")

subgraph CNN Layer 1
X1_expand --"padding (3,3)"--> left_pad_1("left_pad [None, 300, 46, 1]")
X2_expand --"padding (3,3)"--> right_pad_1("right_pad [None, 300, 46, 1]")

left_pad_1 --"convolution (300, 4, 50)"--> left_conv_trans_1("left_conv_trans [None, 50, 43, 1]")
right_pad_1 --"convolution (300, 4, 50)"--> right_conv_trans_1("right_conv_trans [None, 50, 43, 1]")

left_conv_trans_1 --"w_pool (1, 4)"--> left_wp_1("left_wp [None, 50, 40, 1]")
left_conv_trans_1 --"all_pool (1, 43)"--> left_ap_1("left_ap [None, 50]")


right_conv_trans_1 --"w_pool (1, 4)"--> right_wp_1("left_wp [None, 50, 40, 1]")
right_conv_trans_1 --"all_pool (1, 43)"--> right_ap_1("left_ap [None, 50]")
end

subgraph CNN Layer 2
left_wp_1 --"padding (3,3)" --> left_pad_2("left_pad [None, 50, 46, 1]")
right_wp_1 --"padding (3,3)" --> right_pad_2("right_pad [None, 50, 46, 1]")

left_pad_2 --"convolution (50, 4, 50)"--> left_conv_trans_2("left_conv_trans [None, 50, 43, 1]")
right_pad_2 --"convolution (50, 4, 50)"--> right_conv_trans_2("right_conv_trans [None, 50, 43, 1]")

left_conv_trans_2 -- "w_pool (1,4)" --> left_wp_2("left_wp [None, 50, 40, 1]")
left_conv_trans_2 -- "all_pool (1, 43)" --> left_ap_2("left_ap [None, 50]")


right_conv_trans_2 -- "w_pool (1,4)" --> right_wp_2("right_wp [None, 50, 40, 1]")
right_conv_trans_2 -- "all_pool (1, 43)" --> right_ap_2("right_ap [None, 50]")
end

left_ap_0 --cosine -------> sims_0("sims_0 [None, ]")
right_ap_0 --cosine -------> sims_0


left_ap_1 --cosine -----> sims_1("sims_1 [None, ]")
right_ap_1 --cosine -----> sims_1


left_ap_2 --cosine --> sims_2("sims_2 [None, ]")
right_ap_2 --cosine --> sims_2

subgraph Output Layer

sims_0 --stack--> stack_sim("stack_sim [None, 3]")
sims_1 --stack--> stack_sim
sims_2 --stack--> stack_sim

features("features [None, 4]") --concat--> output_features("output_features [None, 7]")
stack_sim --concat--> output_features

output_features --"full connected" --> output("output [None, 2]")
output --softmax--> softmax("softmax [None, ]")

end</pre>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://aclanthology.org/Q16-1019.pdf">文献：ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/50160263">注意力机制之ABCNN</a></li>
<li><a href="https://www.jianshu.com/p/bb366027978a">文献阅读笔记：ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>文本匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>ESIM</title>
    <url>/2021/07/08/ESIM/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>模型</tag>
        <tag>文本匹配</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM原理</title>
    <url>/2021/07/28/LSTM%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://www.cnblogs.com/stephen-goodboy/p/12773466.html">RNN和LSTM模型详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/109519044">为什么LSTM会减缓梯度消失？</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>lstm</tag>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention详解</title>
    <url>/2021/07/23/Attention%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>Attention机制模仿生物观察行为的内部过程，将内部经验和外部对齐，从而增加部分区域的观察精细度。</p>
<span id="more"></span>
<h1>Attention机制</h1>
<p>在一般的Encoder-Decoder框架中，模型会将所有输入的$X$都转化成语义表示$C$，这将导致Decoder出来的每个字都是同权的考虑了输入中的所有词。例如<code>Tom chase Jerry</code>目标翻译的结果是：<code>汤姆追逐杰瑞</code>。在未考虑注意力机制的模型中，<code>汤姆</code>这个词的翻译收到<code>Tom</code>、<code>chase</code>和<code>Jerry</code>三个词同权重的影响。但实际上，<code>汤姆</code>这个词的翻译应该受到<code>Tom</code>这个词的影响最大！</p>
<p>在带有Attention机制的Encoder-Decoder模型需要从序列中学习到每一个元素的重要成都，然后按照重要程度将元素合并。因此，注意力机制可以看作是 Encoder 和 Decoder 之间的接口，它向 Decoder 提供来自每个 Encoder 隐藏状态的信息。通过该设置，模型能够选择性地关注输入序列的有用部分，从而学习它们之间的“对齐”。这就表明，在 Encoder 将输入的序列元素进行编码时，得到的不在是一个固定的语义编码 C ，而是存在多个语义编码，且不同的语义编码由不同的序列元素以不同的权重参数组合而成。一个简单地体现 Attention 机制运行的示意图如下：</p>
<p><img src="encoder-decoder.png" alt="Encoder-Decoder"></p>
<p>在 Attention 机制下，语义编码 $C$ 就不在是输入序列 $X$ 的直接编码了，而是各个元素按其重要程度加权求和得到的，即<br>
$$<br>
C_i=\sum^{T_x}<em>{j=0}{a</em>{ij}f(x_j)}<br>
$$<br>
其中，$i$表示时刻，$j$ 表示序列中的第 $j$ 个元素， $T_x$ 表示序列的长度， $f(⋅)$表示对元素 $x_j$x的编码。$a_{ij}$可以看作是一个概率，反映了元素 $h_j$ 对 $C_i$ 的重要性，可以使用 softmax 来表示：<br>
$$<br>
a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}<br>
$$<br>
这里$e_{ij}$ 正是反映了待编码的元素和其它元素之间的匹配度，当匹配度越高时，说明该元素对其的影响越大，则 $a_{ij}$ 的值也就越大。</p>
<p>因此，得出$a_{ij}$ 的过程如下图：</p>
<p><img src="attention_matrix.png" alt="Attention Matrix"></p>
<p>其中，$h_i$表示 Encoder 的转换函数，$F(h_j,H_i)$ 表示预测与目标的匹配打分函数。将以上过程串联起来，则注意力模型的结构如下图所示：</p>
<p><img src="attention_architecture.png" alt="Attention Architecture"></p>
<h1>Attention原理</h1>
<p>Attention 机制的一个重点就是获得 attention value，即机器翻译中的语义编码 $C_i$。在上一节中我们知道该值是通过输入元素按照不同的权重参数组合而成的，所以我们可以将其定义为一个 attention 函数，比较主流的 attention 函数的机制是采用键值对查询的方式，其工作实质如下图所示：</p>
<p><img src="attention_qkv.png" alt="Attention QKV"></p>
<p>在自然语言任务中，往往 Key 和 Value 是相同的。需要注意的是，计算出来的 attention value 是一个向量，代表序列元素 $x_j$ 的编码向量，包含了元素 $x_j$ 的上下文关系，即同时包含全局联系和局部联系。全局联系很好理解，因为在计算时考虑了该元素与其他所有元素的相似度计算；而局部联系则是因为在对元素 $x_j$ 进行编码时，重点考虑与其相似度较高的局部元素，尤其是其本身。</p>
<p><strong>Step 1：准备隐藏状态</strong></p>
<p>首先准备第一个 Decoder 的隐藏层状态（红色）和所有可用的 Encoder 隐藏层状态（绿色）。在示例中，有 4 个 Encoder 隐藏状态和 1 个 Decoder 隐藏状态。</p>
<p><img src="attention1.gif" alt="Attention1"></p>
<p><strong>Step 2：得到每一个 Encoder 隐藏状态的得分</strong></p>
<p>分值（score）由 <code>score</code> 函数来获得，最简单的方法是直接用 Decoder 隐藏状态和 Encoder 中的每一个隐藏状态进行点积。</p>
<p><img src="attention2.gif" alt="Attention2"></p>
<p><strong>Step 3：将所有得分送入 softmax 层</strong></p>
<p>该部分实质上就是对得到的所有分值进行归一化，这样 <code>softmax</code> 之后得到的所有分数相加为 1。而且能够使得原本分值越高的隐藏状态，其对应的概率也越大，从而抑制那些无效或者噪音信息。</p>
<p><img src="attention3.gif" alt="Attention3"></p>
<p><strong>Step 4：用每个 Encoder 的隐藏状态乘以 softmax 之后的得分</strong></p>
<p>通过将每个编码器的隐藏状态与其softmax之后的分数(标量)相乘，我们得到 对齐向量 或标注向量。这正是对齐产生的机制。</p>
<p><img src="attention4.gif" alt="Attention4"></p>
<p><strong>Step 5：将所有对齐的向量进行累加</strong></p>
<p>对对齐向量进行求和，生成 <em>上下文向量</em> 。上下文向量是前一步的对齐向量的聚合信息。</p>
<p><img src="attention5.gif" alt="Attention5"></p>
<p><strong>Step 6：把上下文向量送到 Decoder 中</strong></p>
<p>通过将上下文向量和 Decoder 的上一个隐藏状态一起送入当前的隐藏状态，从而得到解码后的输出。</p>
<p><img src="attention6.gif" alt="Attention6"></p>
<p>最终得到完整的注意力层结构如下图所示：</p>
<p><img src="attention.png" alt="Attention"></p>
<h1>其他理解</h1>
<p>Q就是词的查询向量，K是“被查”向量，V是内容向量。</p>
<p>简单来说一句话：Q是最适合查找目标的，K是最适合接收查找的，V就是内容，这三者不一定要一致，所以网络这么设置了三个向量，然后学习出最适合的Q, K, V，以此增强网络的能力。</p>
<p>主要要理解Q，K的意义，可以<strong>类比搜索</strong>的过程：</p>
<p>假设我们想查一篇文章，我们不会直接把文章的内容打上去，而是会在搜索框输入该文章的<strong>关键字</strong>，如果我们搜不到，我们往往会再换一个关键字，直到搜到为止，那么可以让我们搜到的关键字就是<strong>最适合查找目标文章的关键字</strong>。这个<strong>最适合查找目标文章的关键字就是Q。</strong></p>
<p>那么搜索引擎拿到我们输入的关键字Q之后，就会把Q和库里面的文章对比，当然搜索引擎为了节省资源加快对比速度，提前把库里面的文章进行了处理提取了<strong>关键信息</strong>，关键信息有很多，那么那个关键信息能够使得搜索命中率高，那个就是<strong>最适合接收查找的关键信息，<strong>这个</strong>最适合接收查找的关键信息就是K</strong>。</p>
<p>使用Q和K计算了相似度之后得到score，这就是相似度评分，之后有了相似度评分，就可以把内容V加权回去了。</p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&amp;mid=2247485860&amp;idx=1&amp;sn=e926a739784090b3779711164217b968&amp;chksm=c06981f9f71e08efb5f57441444f71a09f1d27fc667af656a5ad1173e32ad394201d02195a3a&amp;mpshare=1&amp;scene=1&amp;srcid=0618HMAYi4gzzwWfedLoOuSD&amp;key=cb6098335ab487a8ec84c95399379f16f975d33ce91588d73ecf857c54b543666b5927e231ad3a9b17bff0c20fff20fc49c262912dca050dee9465801de8a4cdc79e3d8f4fbc058345331fb691bcbacb&amp;ascene=1&amp;uin=MTE3NTM4MTY0NA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=ikhBXxX7PL%2Fal9hbIGXbRFA96ei74EF%2BcP8KdbP6UcV6mIpOfPWzVuju%2Bqw86q5r">动画图解Attention机制，让你一看就明白</a></li>
<li><a href="https://www.cnblogs.com/ydcode/p/11038064.html">浅谈Attention机制的理解</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解</title>
    <url>/2021/07/23/Transformer%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://www.cnblogs.com/mantch/p/11591937.html">Transformer各层网络结构详解！</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Elastic Search</title>
    <url>/2021/07/02/Elastic-Search/</url>
    <content><![CDATA[<p>Elasticsearch（简称ES）是一个分布式、可扩展、实时的搜索与数据分析引擎。</p>
<span id="more"></span>
<p>todo:</p>
<ol>
<li>索引过程</li>
<li>检索过程</li>
<li>分数怎么计算的</li>
</ol>
<h1>传统数据库的问题</h1>
<p>随着访问量的上升，几乎大部分使用 MySQL 架构的网站在数据库上都开始出现了性能问题。</p>
<p><strong>读写分离</strong><br>
由于数据库的写入压力增加，读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从复制技术来达到读写分离，以提高读写性能和读库的可扩展性。Mysql 的 master-slave 模式成为这个时候的网站标配了。</p>
<p><strong>分表分库</strong><br>
开始流行使用分表分库来缓解写压力和数据增长的扩展问题。这个时候，分表分库成了一个热门技术，也是业界讨论的热门技术问题。</p>
<p><strong>MySQL 的扩展性瓶颈</strong><br>
大数据量高并发环境下的 MySQL 应用开发越来越复杂，也越来越具有技术挑战性。分表分库的规则把握都是需要经验的。分库分表的子库到一定阶段又面临扩展问题。还有就是需求的变更，可能又需要一种新的分库方式。</p>
<h1>搜索引擎原理</h1>
<p>一次检索大致可分为四步：</p>
<ol>
<li>
<p><strong>查询分析</strong><br>
正常情况下用户输入正确的查询，比如搜索“里约奥运会”这个关键词，用户输入正确完成一次搜索，但是搜索通常都是全开放的，任何的用户输入都是有可能的，很大一部分还是非常口语化和个性化的，有时候还会存在拼写错误，用户不小心把“淘宝”打成“涛宝”，这时候需要用自然语言处理技术来做拼写纠错等处理，以正确理解用户需求。</p>
</li>
<li>
<p><strong>分词技术</strong><br>
这一步利用自然语言处理技术将用户输入的查询语句进行分词，如标准分词会把“lucene全文检索框架”分成 lucene | 全 | 文｜检｜索｜框｜架｜， IK分词会分成： lucene｜全文｜检索｜框架｜，还有简单分词等多种分词方法。</p>
</li>
<li>
<p><strong>关键词检索</strong><br>
提交关键词后在倒排索引库中进行匹配，倒排索引就是关键词和文档之间的对应关系，就像给文档贴上标签。比如在文档集中含有 “lucene” 关键词的有文档1 、文档 6、文档9，含有 “全文检索” 关键词的有文档1 、文档6 那么做与运算，同时含有 “lucene” 和 “全文检索” 的文档就是文档1和文档6，在实际的搜索中会有更复杂的文档匹配模型。</p>
</li>
<li>
<p><strong>搜索排序</strong><br>
对多个相关文档进行相关度计算、排序，返回给用户检索结果。</p>
</li>
</ol>
<h1>倒排索引</h1>
<p>倒排索引，也常被称为反向索引，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射，它是文档检索系统中最常用的数据结构。</p>
<p>下面我们通过具体实例深入理解倒排索引，通过简单文档以小见大，体验倒排索引的建过程。</p>
<table>
<thead>
<tr>
<th>文档ID</th>
<th>文档内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>人工智能成为互联网大会焦点</td>
</tr>
<tr>
<td>2</td>
<td>谷歌推出开源人工智能系统工具</td>
</tr>
<tr>
<td>3</td>
<td>互联网的未来在人工智能</td>
</tr>
<tr>
<td>4</td>
<td>谷歌开源机器学习工具</td>
</tr>
</tbody>
</table>
<p>对于文档内容，先要经过词条化处理。与英文不同的是，英文通过空格分隔单词，中文的词与词之间没有明确的分隔符号，经过分词系统进行中文分词以后把矩阵切分成一个个的词条。</p>
<table>
<thead>
<tr>
<th>词项</th>
<th>文档频率</th>
<th>倒排记录表</th>
</tr>
</thead>
<tbody>
<tr>
<td>人工</td>
<td>3</td>
<td>1,2,3</td>
</tr>
<tr>
<td>智能</td>
<td>3</td>
<td>1,2,3</td>
</tr>
<tr>
<td>成为</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>互联网</td>
<td>2</td>
<td>1,3</td>
</tr>
<tr>
<td>大会</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>焦点</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>谷歌</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr>
<td>推出</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>开源</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr>
<td>系统</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>工具</td>
<td>2</td>
<td>2,4</td>
</tr>
<tr>
<td>的</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>未来</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>在</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>机器</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>学习</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>我们需要对单词进行排序，像 B+ 树一样，可以在页里实现二分查找。</p>
<p><img src="InvertIndex.png" alt="inverted index"></p>
<p>Lucene 的倒排索引，增加了最左边的一层「字典树」term index，它不存储所有的单词，只存储单词前缀，通过字典树找到单词所在的块，也就是单词的大概位置，再在块里二分查找，找到对应的单词，再找到单词对应的文档列表。</p>
<p>Lucene 的实现会要更加复杂，针对不同的数据结构采用不同的字典索引，使用了FST模型、BKDTree等结构。</p>
<p>真实的倒排记录也并非一个链表，而是采用了SkipList、BitSet等结构。</p>
<h1>ElasticSearc索引</h1>
<h2 id="索引的不变性"><strong>索引的不变性</strong></h2>
<p>由于倒排索引的结构特性，在索引建立完成后对其进行修改将会非常复杂。再加上几层索引嵌套，更让索引的更新变成了几乎不可能的动作。<br>
所以索性设计成不可改变的：倒排索引被写入磁盘后是不可改变的，它永远不会修改。</p>
<p><strong>优点</strong>：</p>
<ol>
<li>不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。</li>
<li>一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。</li>
<li>其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。</li>
<li>写入单个大的倒排索引允许数据压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。</li>
</ol>
<p><strong>缺点：</strong></p>
<p>主要事实是它是不可变的，你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。</p>
<h2 id="动态更新索引">动态更新索引</h2>
<p>怎样在保留不变性的前提下实现倒排索引的更新？答案是: <strong>用更多的索引</strong>。</p>
<p>Elasticsearch 基于 Lucene, 引入了 <em>按段搜索</em> 的概念。 每一 <em>段</em> 本身都是一个倒排索引， 但 <em>索引</em> 在 Lucene 中除表示所有 <em>段</em> 的集合外， 还增加了 <em>提交点</em> 的概念 — 一个列出了所有已知段的文件，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段。</p>
<p>在 lucene 中查询是基于 segment。每个 segment 可以看做是一个独立的 subindex，在建立索引的过程中，lucene 会不断的 flush 内存中的数据持久化形成新的 segment。多个 segment 也会不断的被 merge 成一个大的 segment，在老的 segment 还有查询在读取的时候，不会被删除，没有被读取且被 merge 的 segement 会被删除。</p>
<p><img src="index_segment.png" alt="Index Segment"></p>
<p><strong>索引更新过程</strong>：</p>
<ol>
<li>
<p>数据先写入内存buffer，在写入buffer的同时将数据写入translog日志文件，注意：此时数据还没有被成功es索引记录，因此无法搜索到对应数据；</p>
</li>
<li>
<p>如果buffer快满了或者到一定时间，es就会将buffer数据refresh到一个新的segment file中，但是此时数据不是直接进入segment file的磁盘文件，而是先进入os cache的。这个过程就是<strong>refresh</strong>。一旦buffer中的数据被refresh操作，刷入os cache中，就代表这个数据就可以被搜索到了。</p>
<p>每隔1秒钟，es将buffer中的数据写入一个新的segment file，因此每秒钟会产生一个新的磁盘文件segment file，这个segment file中就存储最近1秒内buffer中写入的数据。</p>
<p>这就是为什么es被称为准实时（NRT，near real-time）：因为写入的数据默认每隔1秒refresh一次，也就是数据每隔一秒才能被 es 搜索到，之后才能被看到，所以称为准实时。</p>
<p>只要数据被输入os cache中，buffer就会被清空，并且数据在translog日志文件里面持久化到磁盘了一份，此时就可以让这个segment file的数据对外提供搜索了。</p>
</li>
<li>
<p>重复1~2步骤，新的数据不断进入buffer和translog，不断将buffer数据写入一个又一个新的segment file中去，每次refresh完，buffer就会被清空，同时translog保留一份日志数据。随着这个过程推进，translog文件会不断变大。当translog文件达到一定程度时，就会执行commit操作。</p>
</li>
<li>
<p>commit操作发生第一步，就是将buffer中现有数据refresh到os cache中去，清空buffer。将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。将现有的translog清空，然后再次重启启用一个translog，此时commit操作完成。</p>
</li>
</ol>
<h1>文档的更新与删除</h1>
<h2 id="删除">删除</h2>
<p>段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。</p>
<p>磁盘上的每个segment都有一个.del文件与它相关联。当发送删除请求时，该文档未被真正删除，而是在.del文件中标记为已删除。此文档可能仍然能被搜索到，但会从结果中过滤掉。当segment合并时，在.del文件中标记为已删除的文档不会被包括在新的segment中，也就是说merge的时候会真正删除被删除的文档。</p>
<h2 id="更新">更新</h2>
<p>创建新文档时，Elasticsearch将为该文档分配一个版本号。对文档的每次更改都会产生一个新的版本号。当执行更新时，旧版本在.del文件中被标记为已删除，并且新版本在新的segment中写入索引。旧版本可能仍然与搜索查询匹配，但是从结果中将其过滤掉。</p>
<h1>ElasticSearch集群原理</h1>
<p><img src="es_cluster.png" alt="es-cluster"></p>
<p><strong>Document</strong>：文档，指一行数据；</p>
<p><strong>Index</strong>：索引，是多个document的集合（和sql数据库的表对应)；</p>
<p><strong>Shard</strong>：分片，当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的</p>
<p><strong>Replia</strong>：副本，为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。</p>
<p><strong>Node</strong>：节点，形成集群的每个服务器称为节点，一个节点可以包含多个shard</p>
<p><strong>Cluster</strong>：集群，ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。</p>
<h2 id="集群节点角色">集群节点角色</h2>
<p><img src="cluster_role.png" alt="cluster role"></p>
<p>ES集群的服务器主要分为以下三种角色：</p>
<ol>
<li>
<p><strong>master节点</strong>：负责保存和更新集群的一些元数据信息，之后同步到所有节点，所以每个节点都需要保存全量的元数据信息，包括集群的配置信息、集群的节点信息、模板template设置、索引以及对应的设置、mapping、分词器和别名、索引关联到的分片以及分配到的节点等配置</p>
<blockquote>
<p><strong>master选举</strong></p>
<p><strong>选举策略</strong></p>
<p>如果集群中存在master，认可该master，加入集群，如果集群中不存在master，从具有master资格的节点中选id最小的节点作为master</p>
<p><strong>选举时机</strong></p>
<p>集群启动：后台启动线程去ping集群中的节点，按照上述策略从具有master资格的节点中选举出master</p>
<p>现有的master离开集群：后台一直有一个线程定时ping master节点，超过一定次数没有ping成功之后，重新进行master的选举</p>
<p><strong>避免脑裂</strong></p>
<p>脑裂问题是采用master-slave模式的分布式集群普遍需要关注的问题，脑裂一旦出现，会导致集群的状态出现不一致，导致数据错误甚至丢失。</p>
<p>ES避免脑裂的策略：过半原则，可以在ES的集群配置中添加一下配置，避免脑裂的发生</p>
</blockquote>
</li>
<li>
<p><strong>data节点</strong>：负责数据存储和查询</p>
</li>
<li>
<p><strong>coordinator节点</strong>：路由索引请求、聚合搜索结果集、分发批量索引请求</p>
</li>
</ol>
<h2 id="路由机制">路由机制</h2>
<p>当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？</p>
<p>首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：</p>
<p>shard = hash(routing) % number_of_primary_shards</p>
<p>routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。</p>
<p>这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。</p>
<h2 id="新建、索引、删除文档">新建、索引、删除文档</h2>
<p>新建、索引和删除请求都是写操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。</p>
<p><img src="document_operation.png" alt="document operation"></p>
<p>以下是在主副分片和任何副本分片上面 成功新建，索引和删除文档所需要的步骤顺序：</p>
<ol>
<li>客户端向 Node 1 发送新建、索引或者删除请求。</li>
<li>节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。</li>
<li>Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。</li>
</ol>
<h2 id="查询文档">查询文档</h2>
<p>可以从主分片或者从其它任意副本分片检索文档</p>
<p><img src="document_query.png" alt="doucment query"></p>
<p>以下是从主分片或者副本分片检索文档的步骤顺序：</p>
<ol>
<li>客户端向 Node 1 发送获取请求。</li>
<li>节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。</li>
<li>Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。</li>
</ol>
<p>在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。</p>
<p>在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。</p>
<h2 id="更新文档">更新文档</h2>
<p><img src="document_update.png" alt="document update"></p>
<p>以下是部分更新一个文档的步骤：</p>
<ol>
<li>客户端向 Node 1 发送更新请求。</li>
<li>它将请求转发到主分片所在的 Node 3 。</li>
<li>Node 3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。</li>
<li>如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， Node 3 向协调节点也返回成功，协调节点向客户端返回成功。</li>
</ol>
<h2 id="分布式检索">分布式检索</h2>
<p>搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档: 这些文档有可能在集群的任何分片上。一个搜索请求必须询问我们关注的索引（index or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档。</p>
<p>但是找到所有的匹配文档仅仅完成事情的一半。在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch 。</p>
<h3 id="查询阶段"><strong>查询阶段</strong></h3>
<p>在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的优先队列。</p>
<p><img src="query.png" alt="query"></p>
<p>查询阶段包含以下三个步骤:</p>
<ol>
<li>客户端发送一个 <code>search</code> 请求到 <code>Node 3</code> ， <code>Node 3</code> 会创建一个大小为 <code>from + size</code> 的空优先队列。</li>
<li><code>Node 3</code> 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 <code>from + size</code> 的本地有序优先队列中。</li>
<li>每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 <code>Node 3</code> ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li>
</ol>
<p>当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。</p>
<p>协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。</p>
<h3 id="取回阶段"><strong>取回阶段</strong></h3>
<p>查询阶段标识哪些文档满足搜索请求，但是我们仍然需要取回这些文档，这是取回阶段的任务。</p>
<p><img src="fetch.png" alt="fetch"></p>
<p>分布式阶段由以下步骤构成：</p>
<ol>
<li>协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。</li>
<li>每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。</li>
<li>一旦所有的文档都被取回了，协调节点返回结果给客户端。</li>
</ol>
<h1>常见问题</h1>
<h2 id="分片的设定">分片的设定</h2>
<p>分片数过小，数据写入形成瓶颈，无法水平拓展</p>
<p>分片数过多，每个分片都是一个lucene的索引，分片过多将会占用过多资源</p>
<p>如何计算分片数</p>
<p>需要注意分片数量最好设置为节点数的整数倍，保证每一个主机的负载是差不多一样的，特别的，如果是一个主机部署多个实例的情况，更要注意这一点，否则可能遇到其他主机负载正常，就某个主机负载特别高的情况。</p>
<p>一般我们根据每天的数据量来计算分片，保持每个分片的大小在 50G 以下比较合理。如果还不能满足要求，那么可能需要在索引层面通过拆分更多的索引或者通过别名 + 按小时 创建索引的方式来实现了。</p>
<h2 id="相关度评分">相关度评分</h2>
<p>Lucene（或 Elasticsearch）使用 <em>布尔模型（Boolean model）</em> 查找匹配文档，并用一个名为 <em>实用评分函数（practical scoring function）<em>的公式来计算相关度。这个公式借鉴了</em>词频/逆向文档频率（term frequency/inverse document frequency）</em> 和 <em>向量空间模型（vector space model）</em>，同时也加入了一些现代的新特性，如协调因子（coordination factor），字段长度归一化（field length normalization），以及词或查询语句权重提升。</p>
<h3 id="布尔模型">布尔模型</h3>
<p><em>布尔模型（Boolean Model）</em> 只是在查询中使用 <code>AND</code> 、 <code>OR</code> 和 <code>NOT</code> （与、或和非）这样的条件来查找匹配的文档，以下查询：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">full AND text AND search AND (elasticsearch OR lucene)</span><br></pre></td></tr></table></figure>
<p>会将所有包括词 <code>full</code> 、 <code>text</code> 和 <code>search</code> ，以及 <code>elasticsearch</code> 或 <code>lucene</code> 的文档作为结果集。</p>
<p>这个过程简单且快速，它将所有可能不匹配的文档排除在外。</p>
<h3 id="词频-逆向文档频率（TF-IDF）">词频/逆向文档频率（TF/IDF）</h3>
<p>当匹配到一组文档后，需要根据相关度排序这些文档，不是所有的文档都包含所有词，有些词比其他的词更重要。一个文档的相关度评分部分取决于每个查询词在文档中的 <em>权重</em> 。</p>
<p>词的权重由三个因素决定，有兴趣可以了解下面的公式，但并不要求记住。</p>
<h4 id="词频">词频</h4>
<p>词在文档中出现的频度是多少？频度越高，权重 <em>越高</em> 。 5 次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tf(t in d) = √frequency </span><br></pre></td></tr></table></figure>
<blockquote>
<p>词 <code>t</code> 在文档 <code>d</code> 的词频（ <code>tf</code> ）是该词在文档中出现次数的平方根。</p>
</blockquote>
<h4 id="逆向文档频率">逆向文档频率</h4>
<p>词在集合所有文档里出现的频率是多少？频次越高，权重 <em>越低</em> 。常用词如 <code>and</code> 或 <code>the</code> 对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如 <code>elastic</code> 或 <code>hippopotamus</code> 可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">idf(t) = 1 + log ( numDocs / (docFreq + 1)) </span><br></pre></td></tr></table></figure>
<blockquote>
<p>词 <code>t</code> 的逆向文档频率（ <code>idf</code> ）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。</p>
</blockquote>
<h4 id="字段长度归一值">字段长度归一值</h4>
<p>字段的长度是多少？字段越短，字段的权重 <em>越高</em> 。如果词出现在类似标题 <code>title</code> 这样的字段，要比它出现在内容 <code>body</code> 这样的字段中的相关度更高。字段长度的归一值公式如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">norm(d) = 1 / √numTerms </span><br></pre></td></tr></table></figure>
<blockquote>
<p>字段长度归一值（ <code>norm</code> ）是字段中词数平方根的倒数。</p>
</blockquote>
<p>字段长度的归一值对全文搜索非常重要，许多其他字段不需要有归一值。无论文档是否包括这个字段，索引中每个文档的每个 <code>string</code> 字段都大约占用 1 个 byte 的空间。对于 <code>not_analyzed</code> 字符串字段的归一值默认是禁用的，而对于 <code>analyzed</code> 字段也可以通过修改字段映射禁用归一值：</p>
<p>对于有些应用场景如日志，归一值不是很有用，要关心的只是字段是否包含特殊的错误码或者特定的浏览器唯一标识符。字段的长度对结果没有影响，禁用归一值可以节省大量内存空间。</p>
<h4 id="结合使用">结合使用</h4>
<p>以下三个因素——词频（term frequency）、逆向文档频率（inverse document frequency）和字段长度归一值（field-length norm）——是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的 <em>权重</em> 。</p>
<p>当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。</p>
<h3 id="向量空间模型">向量空间模型</h3>
<p>在向量空间模型里，向量空间模型里的每个数字都代表一个词的 <em>权重</em> ，与 词频/逆向文档频率（term frequency/inverse document frequency） 计算方式类似。</p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://www.jianshu.com/p/52b92f1a9c47">理解ElasticSearch工作原理</a></li>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/scoring-theory.html">相关度评分背后的理论</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>后台开发</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>es</tag>
        <tag>倒排索引</tag>
      </tags>
  </entry>
  <entry>
    <title>word embedding</title>
    <url>/2021/07/06/word-embedding/</url>
    <content><![CDATA[<p>word embedding 是文本表示的一类方法，将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。</p>
<span id="more"></span>
<p>这种方法有几个明显的优势：</p>
<ol>
<li>他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。</li>
<li>语意相似的词在向量空间上也会比较相近。</li>
<li>通用性很强，可以用在不同的任务中。</li>
</ol>
<h1>神经概率语言模型</h1>
<p>神经网络语言模型的提出解决了N-gram模型当𝑛较大时会发生数据稀疏的问题。与N-gram语言模型相同，神经网络语言模型（NNLM）也是对𝑛元语言模型进行建模，估计$P(x_i|x_{x-n+1},x_{x-n+2},…,x_{i-1})$的概率，与统计语言模型不同的是，神经网络语言模型不通过计数的方法对𝑛元条件概率进行估计，而是直接通过一个神经网络对其建模求解。</p>
<p>语言模型的的<strong>训练样本</strong>：$(Context(w),w)$</p>
<p><img src="%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.png" alt="语言模型"></p>
<p>神经网络语言模型的结构入上图所示，可以分为输入层、投影层、隐藏层和输出层：</p>
<ol>
<li><strong>输入层</strong>：词$x$的上下文，如果用N-gram的方法就是词$x$的前$n−1$个词了。每一个词都作为一个长度为$V$的one-hot向量传入神经网络中。</li>
<li><strong>投影层</strong>:投影层也叫embedding层，在投影层中，存在一个look-up表$C$，$C$被表示成一个$V∗m$的自由参数矩阵，其中$V$是词典的大小，而$m$代表每个词投影后的维度。表$C$中每一行都作为一个词向量存在，这个词向量可以理解为每一个词的另一种分布式表示。每一个one-hot向量都经过表$C$的转化变成一个词向量。$n−1$个词向量首尾相接的拼起来，转化为$(n−1)m$的列向量输入到下一层。</li>
<li><strong>隐藏层</strong>：隐藏层的作用是进行非线性变换。$𝑧=𝑡𝑎𝑛ℎ(𝑊𝑋+𝑏)$</li>
<li><strong>输出层</strong>：用softmax进行概率计算，计算词表$V$的每个词的概率。$𝑃(𝑥_𝑖)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑧)$</li>
</ol>
<p>函数$F(w,Context(w),\theta)$中待确定的参数代表以下两个方面：</p>
<ul>
<li>
<p>词向量：$v(w) \in R^m, w \in C$以及填充向量</p>
</li>
<li>
<p>神经网络的参数：权重参数$W、U$和偏置参数$p、q$</p>
<p>$W \in R^{n_h \times (n-1)m}$</p>
<p>$p \in R^{n_h}$</p>
<p>$U \in R^{V \times n_h}$</p>
<p>$q \in R^V$</p>
</li>
</ul>
<p>词向量在神经概率语言模型中扮演的角色：训练时，它用来帮助构造目标函数的辅助参数，训练完成后，它只是语言模型的一个副产品。</p>
<h1>Word2Vec</h1>
<p>word2vec中到的两个重要模型：</p>
<ul>
<li>**CBOW模型（Continuous Bag-of-Words Model）：**已知上下文$w_{t-2},w_{t-1},w_{t+1},w{t+2}$的前提下，预测当前词$w_t$</li>
<li>**Skip-gram模型（Continuous Skip-gram Model)：**已知当前词$w_t$的前提下，预测上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$</li>
</ul>
<p><img src="word2vec.png" alt="word2vec"></p>
<h2 id="基于Hierarchical-Softmax的模型">基于Hierarchical Softmax的模型</h2>
<ul>
<li>Huffman树</li>
</ul>
<h2 id="基于Negative-Sampling的模型">基于Negative Sampling的模型</h2>
<p>随机负采样</p>
<h1>Glove</h1>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://www.cnblogs.com/huangyc/p/9861453.html">语言模型</a></li>
<li><a href="https://blog.csdn.net/heyc861221/article/details/80126134">漫谈词向量</a></li>
<li><a href="https://blog.csdn.net/qq_27586341/article/details/90146342">word2vec原理（一）： 词向量、CBOW与Skip-Gram模型基础</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>word embedding</tag>
        <tag>word2vec</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title>bert原理</title>
    <url>/2021/07/23/bert%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers）是一种<strong>预训练模型</strong>，旨在通过考虑所有层中的<strong>双侧上下文</strong>信息来得到<strong>深度的双向表示</strong>。该表示连接上一层输出层后，仅需微调训练就可以在很多 NLP 任务中取得惊人的效果。</p>
<span id="more"></span>
<h1>Bert结构</h1>
<h2 id="Bert的输入">Bert的输入</h2>
<p>在BERT中，输入的向量是由三种不同的embedding求和而成，分别是：</p>
<ol>
<li>token embedding：单词本身的向量表示</li>
<li>segment embedding：用于区分两个句子的向量表示</li>
<li>position embedding：单词位置信息的编码表示</li>
</ol>
<p><img src="bert_input.png" alt="bert input"></p>
<h3 id="Token-embedding">Token embedding</h3>
<p>token embedding 层是要将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示。</p>
<p><img src="token_embedding.png" alt="token embedding"></p>
<p>输入文本在送入token embeddings 层之前要先进行tokenization处理。此外，两个特殊的token会被插入到tokenization的结果的开头 ([CLS])和结尾 ([SEP]) 。它们视为后面的分类任务和划分句子对服务的。</p>
<p>Token Embeddings 层会将每一个wordpiece token转换成768维的向量。这样，例子中的6个token就被转换成了一个(6, 768) 的矩阵或者是(1, 6, 768)的张量（如果考虑batch_size的话）。</p>
<h3 id="Segment-embedding">Segment embedding</h3>
<p>BERT 能够处理对输入句子对的分类任务。这类任务就像判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入到模型中。那BERT如何去区分一个句子对中的两个句子呢？答案就是segment embeddings.</p>
<p><img src="segment_embedding.png" alt="segment embedding"></p>
<p>Segment Embeddings 层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0。</p>
<h3 id="Position-Embeddings">Position Embeddings</h3>
<p>Transformers无法编码输入的序列的顺序性，加入position embeddings会让BERT理解输入句子的位置信息。</p>
<p><strong>如何设计positional embedding？</strong></p>
<ol>
<li>为每个时间步（单词在句子中的位置）输出唯一的编码</li>
<li>即便句子长度不一，句子中两个时间步之间的距离应该是“恒定”的</li>
<li>模型可以轻易泛化到更长的句子上</li>
<li>PE必须是确定的</li>
</ol>
<p>论文中采用的positional embedding：<strong>偶数位置，使用正弦编码，在奇数位置，使用余弦编码</strong><br>
$$<br>
PE(pos,2i)=sin(\frac{pos}{10000^{2i/d_{model}}})<br>
$$</p>
<p>$$<br>
PE(pos,2i+1)=cos(\frac{pos}{10000^{2i/d_{model}}})<br>
$$</p>
<p>BERT能够处理最长512个token的输入序列。论文作者通过让BERT在各个位置上学习一个向量表示来将序列顺序的信息编码进来。这意味着Position Embeddings layer 实际上就是一个大小为 (512, 768) 的lookup表，表的第一行是代表第一个序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子“Hello world” 和“Hi there”, “Hello” 和“Hi”会由完全相同的position embeddings，因为他们都是句子的第一个词。同理，“world” 和“there”也会有相同的position embedding。</p>
<blockquote>
<p>transformer中设计的Position是一个固定向量，但是Google的BERT中的位置编码是可学习的，通过一个可学习的嵌入层来实现的。</p>
<ul>
<li>固定位置编码：
<ul>
<li>优点：计算简单、无需额外训练，能够处理任意长度的序列。</li>
<li>缺点：可能无法针对特定任务进行优化，灵活性较差。</li>
</ul>
</li>
<li>可学习位置编码：
<ul>
<li>优点：能够根据具体任务数据进行优化，灵活性更高，通常在实际任务中表现更好。</li>
<li>缺点：需要额外的训练参数，模型复杂度略有增加。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="网络结构">网络结构</h2>
<p>BERT的主要结构是transformer，一个BERT预训练模型的基础结构是标准transformer结构的encoder部分，一个标准transformer结构如下图所示，其中右边的部分就是BERT中使用的encoder部分。</p>
<p><img src="bert_architecture.png" alt="bert architecture"></p>
<p>一个transformer的encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生，BERT的每一层由一个这样的encoder单元构成。在比较大的BERT模型中，有24层encoder，每层中有16个Attention，词向量的维度是1024。在比较小的BERT模型中，有12层encoder，每层有12个Attention，词向量维度是768。在所有情况下，将feed-forward/filter 的大小设置为 4H（H为词向量的维度），即H = 768时为3072，H = 1024时为4096。</p>
<h3 id="Multi-Headed-Attention">Multi-Headed Attention</h3>
<h4 id="Self-Attention">Self Attention</h4>
<ol>
<li>
<p><strong>self-attention出现的原因</strong></p>
<ul>
<li>
<p>为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中<strong>并行加速计算</strong>的问题</p>
</li>
<li>
<p>由于每个目标词是直接与句子中所有词分别计算相关度(attention)的，所以解决了传统的RNN模型中长距离依赖的问题，通过attention，可以将两个距离较远的词之间的距离拉近为1直接计算词的相关度，而传统的RNN模型中，随着距离的增加，词之间的相关度会被削弱。</p>
</li>
</ul>
</li>
<li>
<p><strong>单个self-attention 的计算过程</strong></p>
<p>self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路</p>
<ol>
<li>
<p>首先，self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64。</p>
<p><img src="self_attention_1.png" alt="self attention1"></p>
</li>
<li>
<p>计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2。</p>
<p><img src="self_attention_2.png" alt="self attention2"></p>
</li>
<li>
<p>接下来，把点成的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大。</p>
<p><img src="self_attention_3.png" alt="self attention3"></p>
</li>
<li>
<p>下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attention在当前节点的值。</p>
<p><img src="self_attention_4.png" alt="self attention4"></p>
</li>
</ol>
<p>在实际的应用场景，为了提高计算速度，我们采用的是矩阵的方式，直接计算出Query, Key, Value的矩阵，然后把embedding的值与三个矩阵直接相乘，把得到的新矩阵 Q 与 K 相乘，乘以一个常数，做softmax操作，最后乘上 V 矩阵。</p>
<p><strong>这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。</strong></p>
<p><img src="self_attention_matrix1.png" alt="self attention matrix 1"></p>
<p><img src="self_attention_matrix2.png" alt="self attention matrix 2"></p>
</li>
</ol>
<blockquote>
<p>self attention中Q、K、V都是通过一个线性变换得到，其维度可自定义，但一般定义成$embedding_size \times embedding_size/head$​</p>
</blockquote>
<h4 id="Multi-Headed-Attention-2">Multi-Headed Attention</h4>
<p>multi-headed attention机制理解起来很简单，<strong>就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组</strong>，所以最后得到的结果是8个矩阵。</p>
<p><img src="multi_headed_attention1.png" alt="multi-headed attention1"></p>
<p><img src="multi_headed_attention2.png" alt="multi-headed attention2"></p>
<p>Multi-Head Self-Attention<strong>将多个不同单头的</strong>Self-Attention输出<strong>Concat</strong>成一条，然后再经过一个全连接层降维输出，如下图所示，右边的部分即为一个multi-head attention的计算过程，其中的h指的是attention的个数，即上面例子中的n。</p>
<p><img src="multi_headed_attention3.jpg" alt="multi headed attention"></p>
<blockquote>
<p>经过concat和全连接层降维后，multi-headed attention输出的向量维度与输入的向量维度一致</p>
</blockquote>
<h3 id="Add-Norm">Add &amp; Norm</h3>
<h4 id="Add">Add</h4>
<p>Add是对得到的$X_{Attention}$以及X做一个相加。<br>
$$<br>
X_{Attention} = X_{Attention} + X<br>
$$<br>
<img src="skip_connect.jpg" alt="skip connect"></p>
<p>Add的目的和ResNet的跳跃连接目的一样，使用<strong>残差</strong>，相当于每次更新时，导数项上加了一个恒等项1，即使原来的导数很小，这时误差仍然可以有效的反向传播，可以<strong>减少梯度消失和梯度爆炸的问题</strong>。</p>
<h4 id="Layer-Normalization">Layer Normalization</h4>
<p><strong>Normalization的目的是将数据送入激活函数之前进行归一化，避免输入数据落在激活函数的饱和区（两端）。</strong></p>
<p>但是Self-Attention为什么选择Layer Normalization而不是Batch Normalization？</p>
<p><strong>Bach Normalization与Layer Normalization的区别：</strong></p>
<p>假设我们有10行3列的数据，即我们的batchsize = 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。</p>
<p>那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种**“列缩放”**。</p>
<p>而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种**“行缩放”**。</p>
<p>在NLP领域中，如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的<strong>第一个</strong>词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是<strong>针对每个位置</strong>进行缩放，这<strong>不符合NLP的规律</strong>。而LN则是针对一句话进行缩放的，且<strong>LN一般用在第三维度</strong>，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p>
<p><img src="layer_normalization.png" alt="layer normalization"></p>
<h3 id="Feed-Forword-Layer">Feed Forword Layer</h3>
<p>这里就是将Multi-Head Attention得到的提炼好的向量再投影到一个更大的空间（论文里将空间放大了4倍）在那个大空间里可以更方便地提取需要的信息（使用Relu激活函数），最后再投影回token向量原来的空间。<br>
$$<br>
FFN(x)=ReLu(xW_1+b_1)W_2+b_2<br>
$$</p>
<h1>模型训练</h1>
<h2 id="训练任务">训练任务</h2>
<h3 id="Masked-language-Model">Masked language Model</h3>
<p>随机掩盖掉一些单词，然后通过上下文预测该单词。BERT中有15%的wordpiece token会被随机掩盖，这15%的token中80%用<code>[MASK]</code>这个token来代替，10%用随机的一个词来替换，10%保持这个词不变。这种设计使得模型具有捕捉上下文关系的能力，同时能够有利于token-level tasks例如序列标注。</p>
<p><img src="mask_lm.png" alt="mask lm"></p>
<ol>
<li>
<p>为什么选中的15%的wordpiece token不能全部 用<code> [MASK]</code>代替，而要用 10% 的 random token 和 10% 的原 token？</p>
<p><code>[MASK]</code>是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果 <code>[MASK] </code>以外的部分全部都用原 token，模型会学到『如果当前词是<code> [MASK]</code>，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。</p>
<p>以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token 的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在 finetune 阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。</p>
</li>
<li>
<p>最后怎么利用[MASK] token做的预测？</p>
<p>最终的损失函数只计算被mask掉的token的，每个句子里 <code>[MASK] </code>的个数是不定的。实际代码实现是每个句子有一个 maximum number of predictions，取所有<code>[MASK]</code>的位置以及一些 PADDING 位置的向量拿出来做预测（总共凑成 maximum number of predictions 这么多个预测，是定长的），然后再用掩码把 PADDING 盖掉，只计算<code>[MASK]</code>部分的损失。</p>
</li>
</ol>
<h3 id="Next-Sentence-Prediction">Next Sentence Prediction</h3>
<p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
<p><img src="next_sentence_prediction.png" alt="Next Sentence Prediction"></p>
<h1>Fine Tuning</h1>
<h2 id="句子情感分类">句子情感分类</h2>
<p>如果是做单个句子的情感分类。输入中添加[CLS]，输出在最开始的地方添加一个线性分类器即可。为什么在最开始就可以，因为BERT是基于Transformer，而Transformer是基于self-attention，在每个位置都会得到整个句子的信息，所以不需要放到最后面。</p>
<p><img src="bert_case1.png" alt="Bert case 1"></p>
<h2 id="机器翻译">机器翻译</h2>
<p>做机器翻译时，在每个输出后添加一个线性分类器，输出每个词语对应的翻译。</p>
<p><img src="bert_case2.png" alt="Bert case2"></p>
<h2 id="同义句判断">同义句判断</h2>
<p>将相似对拼接一起输入到模型中，最后一层外接一个线性分类器。</p>
<p><img src="bert_case3.png" alt="Bert case3"></p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf">原始论文</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/u011412768/article/details/108015783">BERT原理和结构详解</a></p>
</li>
<li>
<p><a href="https://tech.meituan.com/2019/11/14/nlp-bert-practice.html">美团BERT的探索和实践</a></p>
</li>
<li>
<p><a href="https://www.cnblogs.com/d0main/p/10447853.html">【译】为什么BERT有3个嵌入层，它们都是如何实现的</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/359366717">Transformer 中的 positional embedding</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/42833949">【模型解读】resnet中的残差连接，你确定真的看懂了？</a></p>
</li>
<li>
<p><a href="http://www.cxyzjd.com/article/herosunly/94720139">李宏毅 BERT 学习笔记_herosunly的博客</a></p>
</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>segment embedding</tag>
        <tag>position embedding</tag>
        <tag>self attention</tag>
        <tag>残差连接</tag>
        <tag>layer normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>共线性特征</title>
    <url>/2021/07/20/%E5%85%B1%E7%BA%BF%E6%80%A7%E7%89%B9%E5%BE%81/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>todo</tag>
        <tag>共线性</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2021/07/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>推荐系统本质上就是一个信息过滤系统，通常分为：召回、排序、重排序这3个环节，每个环节逐层过滤，最终从海量的物料库中筛选出几十个用户可能感兴趣的物品推荐给用户。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络（CNN）</title>
    <url>/2021/07/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89/</url>
    <content><![CDATA[<p>**卷积神经网络（Convolutional Neural Network, CNN）**是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p>
<span id="more"></span>
<p>卷积神经网络主要结构有：卷积层、池化层、和全连接层。通过堆叠这些层结构形成一个卷积神经网络。将原始图像转化为类别得分，其中卷积层和全连接层拥有参数，激活层和池化层没有参数。参数更新通过反向传播实现。</p>
<p><img src="cnn.png" alt="cnn"></p>
<p>卷积神经网络通常包含以下几种层：</p>
<ul>
<li>
<p><strong>卷积层（Convolutional layer）</strong>，卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是<strong>提取输入的不同特征</strong>，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p>
</li>
<li>
<p><strong>线性整流层（Rectified Linear Units layer, ReLU layer）</strong>，这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）。</p>
</li>
<li>
<p><strong>池化层（Pooling layer）</strong>，通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。</p>
</li>
<li>
<p><strong>全连接层（ Fully-Connected layer）</strong>, 把所有局部特征结合变成全局特征，用来计算最后每一类的得分。</p>
<p>其中A1为上一层的输出，D1为用随机数生成的一组dropout向量，然后将其与保留概率prob做比较得到一个布尔向量，再将其与A1做乘积即可得到失活后的A1，按理说dropout到这里应该也就完成了，但最后还有一个将A1除以保留概率的操作。所以这里有个疑问，为什么在dropout之后还要做个rescale的除法？</p>
</li>
</ul>
<p>​     其实，这种实现dropout的方法也叫<strong>Inverted Dropout</strong>，是一种经典的dropout实现方法。先不说Inverted Dropout，我们来看正常dropout应该是怎样的：当我们使用了dropout后，在模型训练阶段只有占比为p部分的神经元参与了训练，那么在预测阶段得到的结果会比实际平均要大1/p，所以在测试阶段我们需要将输出结果乘以p来保持输出规模不变。这种原始的dropout实现方式也叫<strong>Vanilla Dropout</strong>。Vanilla操作有一个重大缺陷，那就是预测过程需要根据训练阶段所使用的dropout策略做调整，比较麻烦，所以一般情况下都不会使用这种方法。</p>
<p>​     既如此，相必大家也知道了，我们目前用的都是Inverted Dropout方法，为了能够在神经网络训练完成后安安心心的做预测，我们可以把全部心思都放在训练阶段，所有的设置都在训练阶段完成。<strong>所以为了保证神经网络在丢弃掉一些神经元之后总体信号强度不变和预测结果稳定，也有一种说法叫保证Bernoulli二项分布的数学期望不变，我们在Inverted Dropout方法中对dropout之后的做了除以p的rescale操作。</strong></p>
<p>​     反向传播时同理，梯度计算时需要除以保留概率：</p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://blog.csdn.net/liangchunjiang/article/details/79030681">https://blog.csdn.net/liangchunjiang/article/details/79030681</a></li>
<li><a href="https://easyai.tech/ai-definition/cnn/">https://easyai.tech/ai-definition/cnn/</a></li>
<li><a href="https://juejin.cn/post/6920928949576925191">https://juejin.cn/post/6920928949576925191</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1745012">https://cloud.tencent.com/developer/article/1745012</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title>智能客服系统</title>
    <url>/2021/07/01/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>智能客服机器人慢慢成为了很多企业售后环节的标配产品，它能在一定程度上减轻客服人员的压力。</p>
<span id="more"></span>
<pre class="mermaid">graph TB
O[用户问题] --> A[文本预处理]
A --> B{中控管理}
B --> C[QA机器人]
B --> D[KBQA问答]
B --> E[闲聊机器人]
B --> F[任务机器人]

C --> C1(精准匹配)
C1 --> C2{意图匹配}
C2 --关键词匹配--> C31(会话开始/会话结束)
C2 --关键词匹配--> C32(政治词/黄暴词)
C2 --关键词匹配--> C33(人工客服)
C2 --> C24(QA问题)
C31--> C41(特定回复)
C32 --> C42(特定回复)
C33 --> C43(接入人工坐席)
C24 -->  C44(规则匹配)
C44 --> C51(Elastic Search粗排)
C51 --> C62("Embedding + VSM计算相似性")
C51 --> C63(XGBoost)
C51 --> C64("表达式(Bert)")
C51 --> C65("交互式式(Bert/ABCNN)")

D ---> D1(匹配商品属性)
D1 ---> D2(获取商品属性值)
D2 ---> D3(生成回复)

E ----> E1(Elastic Search粗排)
E1 ----> E2(Embedding+VSM计算相似性)

F --> F1(natural language understanding自然语言理解)
F1 --意图识别/槽位提取---> F2(dialogue state tracking对话状态跟踪器)
subgraph Dialogue Management
F2 --维护/更新对话状态--> F3(dialgoue policy learning对话策略)
end
F3 --决定对话策略 ---> F4(natural language generation自然语言生成)</pre>
<h1>文本预处理</h1>
<ul>
<li>原句</li>
<li>去除空格、标点符号等</li>
<li>简繁转化</li>
<li>同义词改写</li>
</ul>
<h1>QA机器人</h1>
<h2 id="ElasticSearch粗排">ElasticSearch粗排</h2>
<p>ES库中保存有每个用户自己的知识库，因精排比较用户query和知识库中相似问比较耗时，为提高检索性能，因此添加es检索模块，先将与query较相似的相似问检索出来，将精排匹配次数限制在一定数量级。</p>
<p>详细见文章：<a href="/2021/07/02/Elastic-Search/" title="Elastic Search">Elastic Search</a></p>
<h1>闲聊机器人</h1>
<h1>任务机器人</h1>
]]></content>
      <categories>
        <category>business</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>todo</tag>
        <tag>dialogue</tag>
        <tag>taskbot</tag>
      </tags>
  </entry>
  <entry>
    <title>数据倾斜</title>
    <url>/2021/07/19/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
    <content><![CDATA[<p>简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢。</p>
<span id="more"></span>
<h1>数据倾斜</h1>
<p>数据倾斜一般有两种情况：</p>
<ul>
<li><strong>变量值很少：</strong> 单个变量值的占比极大，常见的字段如性别、学历、年龄等。</li>
<li><strong>变量值很多：</strong> 单个变量值的占比极小，常见的字段如收入、订单金额之类的。</li>
</ul>
<h1>常用优化方法</h1>
<ol>
<li>**增加jvm内存：**这适用于变量值非常少的情况，这种情况下，往往只能通过硬件的手段来进行调优；</li>
<li>**增加reduce的个数：**这适用于变量值非常多的情况，这种情况下最容易造成的结果就是大量相同key被partition到一个分区，从而一个reduce执行了大量的工作；</li>
<li>**重新设计key：**在map阶段时给key加上一个随机数，有了随机数的key就不会被大量的分配到同一节点(小几率)，待到reduce后再把随机数去掉即可</li>
<li>**使用combiner合并：**combinner是在map阶段，reduce之前的一个中间阶段，在这个阶段可以选择性的把大量的相同key数据先进行一个合并，可以看做是local reduce，然后再交给reduce来处理，减轻了map端向reduce端发送的数据量(减轻了网络带宽)，也减轻了map端和reduce端中间的shuffle阶段的数据拉取数量(本地化磁盘IO速率)；（hive.map.aggr=true）</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1519028">一文带你搞清楚什么是“数据倾斜”</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>Map Reduce</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习中的梯度消失、梯度爆炸问题</title>
    <url>/2021/07/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/33006526">详解深度学习中的梯度消失、爆炸原因及其解决方法</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习评价指标</title>
    <url>/2021/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<p>本文将详细介绍机器学习分类任务的常用评价指标：准确率（Accuracy）、精确率（Precision）、召回率（Recall）、P-R曲线（Precision-Recall Curve）、F1 Score、混淆矩阵（Confuse Matrix）、ROC、AUC。</p>
<span id="more"></span>
<h1>混淆矩阵</h1>
<table>
<thead>
<tr>
<th></th>
<th>预测结果</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>真实类别</strong></td>
<td>正例</td>
<td>负例</td>
</tr>
<tr>
<td>正例</td>
<td>真阳性（True Positive） TP</td>
<td>假阴性（Ffalse Negative）FN</td>
</tr>
<tr>
<td>负例</td>
<td>假阳性（False Positive） FP</td>
<td>真阴性（True Negative）TN</td>
</tr>
</tbody>
</table>
<h1>准确率（Accuracy）</h1>
<p>准确率是分类问题中最为原始的评价指标，准确率的定义是<strong>预测正确的结果占总样本的百分比</strong>，其公式如下：<br>
$$<br>
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}<br>
$$</p>
<h1>精确率（Precision）</h1>
<p><strong>精准率</strong>（Precision）又叫<strong>查准率</strong>，它是<strong>针对预测结果</strong>而言的，它的含义是<strong>在所有被预测为正的样本中实际为正的样本的概率</strong>：<br>
$$<br>
Precision = \frac{TP}{TP+FP}<br>
$$</p>
<h1>召回率（Recall）</h1>
<p><strong>召回率</strong>（Recall）又叫<strong>查全率</strong>，它是<strong>针对原样本</strong>而言的，它的含义是<strong>在实际为正的样本中被预测为正样本的概率</strong>：<br>
$$<br>
Recall = \frac{TP}{TP+FN}<br>
$$</p>
<h1>F1-Score</h1>
<p>在不同的应用场景下，我们的关注点不同，例如，在预测股票的时候，我们更关心精准率，即我们预测升的那些股票里，真的升了有多少，因为那些我们预测升的股票都是我们投钱的。而在预测病患的场景下，我们更关注召回率，即真的患病的那些人里我们预测错了情况应该越少越好。</p>
<p>精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，召回率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样准确率就很低了。</p>
<p>在实际工程中，我们往往需要结合两个指标的结果，去寻找一个平衡点，使综合性能最大化。</p>
<p>在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的<strong>加权调和平均</strong>，即：<br>
$$<br>
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)<br>
$$</p>
<p>$$<br>
F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}<br>
$$</p>
<p>特别地，当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。<br>
$$<br>
F1=\frac{2 \times P \times R}{P+R} = \frac{2 \times TP}{样例总数+TP-TN}<br>
$$</p>
<h1>ROC/AUC曲线</h1>
<h2 id="灵敏度、特异度、真阳率、假阳率">灵敏度、特异度、真阳率、假阳率</h2>
<p>$$<br>
Sensitivity = \frac{TP}{TP+FN}<br>
$$</p>
<p>$$<br>
Specificity=\frac{TN}{FP+TN}<br>
$$</p>
<ul>
<li>其实我们可以发现灵敏度和召回率是一模一样的，只是名字换了而已。</li>
<li>由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1- 特异度），而不是特异度。</li>
</ul>
<p>$$<br>
真阳率（TPR）=Sensitivity = \frac{TP}{TP+FN}<br>
$$</p>
<p>$$<br>
假阳率(FPR) = 1 - Specificity = \frac{FP}{FP+TN}<br>
$$</p>
<p><strong>TPR 和 FPR 分别是基于实际表现 1 和 0 出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。</strong></p>
<p>正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90% 是正样本，10% 是负样本。我们知道用准确率是有水分的，但是用 TPR 和 FPR 不一样。这里，TPR 只关注 90% 正样本中有多少是被真正覆盖的，而与那 10% 毫无关系，同理，FPR 只关注 10% 负样本中有多少是被错误覆盖的，也与那 90% 毫无关系，所以可以看出：<strong>如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用 TPR 和 FPR 作为 ROC/AUC 的指标的原因。</strong></p>
<h2 id="ROC曲线">ROC曲线</h2>
<p><strong>ROC（Receiver Operating Characteristic）曲线</strong>，又称接受者操作特征曲线。ROC曲线中的主要两个指标就是<strong>真正率TPR</strong>和<strong>假正率FPR</strong>，上面已经解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。</p>
<p><img src="roc.png" alt="ROC"></p>
<h3 id="阈值问题">阈值问题</h3>
<p>ROC曲线是通过<strong>遍历所有阈值</strong>来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。</p>
<p><img src="roc_gate.gif" alt="roc gate"></p>
<p>我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。</p>
<h3 id="判断模型性能">判断模型性能</h3>
<p>那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是**TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。**参考如下动态图进行理解。</p>
<p><img src="roc_tell.gif" alt="roc tell"></p>
<p>即：<strong>进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。</strong></p>
<h2 id="AUC曲线">AUC曲线</h2>
<p>AUC(Area Under Curve)又称为曲线下面积，是处于ROC Curve下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC = 1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC = 0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。</p>
<h1>参考文献</h1>
<blockquote>
<p>1.<a href="https://www.cnblogs.com/guoyaohua/p/classification-metrics.html">【机器学习】一文读懂分类算法常用评价指标</a></p>
</blockquote>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title>ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction </title>
    <url>/2021/11/25/ASAP-A-Chinese-Review-Dataset-Towards-Aspect-Category-Sentiment-Analysis-and-Rating-Prediction/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络 Neural Networks</title>
    <url>/2021/07/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Neural-Networks/</url>
    <content><![CDATA[<p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。</p>
<span id="more"></span>
<h1>神经元</h1>
<p><img src="SingleNeuron.png" alt="神经元"></p>
<p>这个“神经元”是一个以 $x_1,x_2,x_3$ 及截距 $ +1 $ 为输入值的运算单元，其输出为 $  h_{W,b}(x) = f(W^Tx) = f(\sum_{i=1}^3 W_{i}x_i +b)$ ，其中函数 $ f : \Re \mapsto \Re$ 被称为“激活函数”。在本教程中，我们选用sigmoid函数作为激活函数 $ f(\cdot) $:<br>
$$<br>
f(z) = \frac{1}{1+\exp(-z)}.<br>
$$</p>
<h1>神经网络模型</h1>
<p>所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络：</p>
<p><img src="400px-Network331.png" alt="神经网络模型"></p>
<p>我们使用 $ w^l_{jk} $ 表示从 $(l−1)^{th}$ 层的  $k^{th} $个神经元到  $l^{th} $ 层的  $j^{th} $ 个神经元的链接上的权重。使用  $b^l_j$  表示在 $l^{th}$  层第  $j^{th} $ 个神经元的偏置，中间量 $ z^l \equiv w^l a^{l-1}+b^l$ ，使用  $a^l_j$  表示  $l^{th}$  层第  $j^{th}$  个神经元的激活值。</p>
<p>$l^{th}$  层的第 $j^{th}$ 个神经元的激活值 $a^l_j$ 就和 $l-1^{th}$  层的激活值通过方程关联起来了。</p>
<p>$$<br>
\begin{eqnarray}<br>
a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right)<br>
\label{eq:fp}\tag{fp}<br>
\end{eqnarray}<br>
$$</p>
<p>对方程$\eqref{eq:fp}$ 就可以写成下面这种美妙而简洁的向量形式了</p>
<p>$$<br>
\begin{eqnarray}<br>
a^{l} = \sigma(w^l a^{l-1}+b^l)<br>
\label{eq:mfp}\tag{mfp}<br>
\end{eqnarray}<br>
$$</p>
<h1>反向传播</h1>
<p>反向传播的目标是计算代价函数 $C$ 分别关于  $w$  和  $b$  的偏导数  $\frac{∂C}{∂w}$  和  $\frac{∂C}{∂b}$ 。反向传播其实是对权重和偏置变化影响代价函数过程的理解。最终极的含义其实就是计算偏导数 $\frac{\partial C}{\partial w_{jk}^l}$ 和$\frac{\partial C}{\partial b_j^l}$。但是为了计算这些值，我们首先引入一个中间量， $\delta_j^l$ ，这个我们称为在  $l^{th}$  层第  $j^{th}$  个神经元上的<strong>误差</strong>。</p>
<p>对于$l$层的第  $j^{th}$  个神经元，当输入进来时，对神经元的带权输入增加很小的变化 $\Delta z_j^l$ ，使得神经元输出由 $<br>
\sigma(z_j^l)$  变成  $\sigma(z_j^l + \Delta z_j^l)$ 。这个变化会向网络后面的层进行传播，最终导致整个代价产生  $\frac{\partial C}{\partial z_j^l} \Delta z_j^l$ 的改变。所以这里有一种启发式的认识， $\frac{\partial C}{\partial z_j^l}$  是神经元的误差的度量。</p>
<p>按照上面的描述，我们定义  $l$  层的第  $j^{th}$  个神经元上的误差  $\delta_j^l$  为：<br>
$$<br>
\begin{eqnarray}<br>
\delta^l_j \equiv \frac{\partial C}{\partial z^l_j}<br>
\label{eq:error}\tag{error}<br>
\end{eqnarray}<br>
$$</p>
<h2 id="输出层误差的方程">输出层误差的方程</h2>
<p><strong>输出层误差的方程</strong>， $\delta^L$ ： 每个元素定义如下：<br>
$$<br>
\begin{eqnarray}<br>
\delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j)<br>
\label{eq:bp1}\tag{BP1}<br>
\end{eqnarray}<br>
$$</p>
<p>第一个项  $\frac{\partial C}{\partial a_j^L}$  表示代价随着 $j^{th}$  输出激活值的变化而变化的速度。第二项 $\sigma’(z^L_j)$  刻画了在  $z_j^L$  处激活函数  $\sigma$  变化的速度。</p>
<h2 id="使用下一层的误差-delta-l-1-来表示当前层的误差-delta-l">使用下一层的误差  $\delta^{l+1}$ 来表示当前层的误差  $\delta^{l}$</h2>
<p>**使用下一层的误差  $\delta^{l+1}$ 来表示当前层的误差  $\delta^{l}$：**特别地，<br>
$$<br>
\begin{eqnarray}<br>
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l)<br>
\label{eq:bp2}\tag{BP2}<br>
\end{eqnarray}<br>
$$</p>
<p>其中$(w^{l+1})^T$是$(l+1)^{\rm th}$层权重矩阵$w^{l+1}$的转置。假设我们知道$l+1^{\rm th}$层的误差$\delta^{l+1}$。当我们应用转置的权重矩阵$(w^{l+1})^T$，我们可以凭直觉地把它看作是在沿着网络<strong>反向</strong>移动误差，给了我们度量在$l^{\rm th}$ 层输出的误差方法。然后，我们进行 Hadamard 乘积运算 $\odot \sigma’(z^l)$ 。这会让误差通过 $l$  层的激活函数反向传递回来并给出在第 $l$  层的带权输入的误差  $\delta$ 。</p>
<p><strong>证明：</strong><br>
我们想要以$\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$的形式重写$\delta^l_j = \partial C / \partial z^l_j$。应用链式法则<br>
$$<br>
\begin{eqnarray}<br>
\delta^l_j &amp;=&amp; \frac{\partial C}{\partial z^l_j}\\<br>
&amp;=&amp; \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j}\\<br>
&amp;=&amp; \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k<br>
\end{eqnarray}<br>
$$</p>
<p>为了对最后一行的第一项求值，注意：</p>
<p>$$<br>
\begin{eqnarray}<br>
z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k<br>
\end{eqnarray}<br>
$$</p>
<p>做微分，我们得到</p>
<p>$$<br>
\begin{eqnarray}<br>
\frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma’(z^l_j)<br>
\end{eqnarray}<br>
$$</p>
<p>代入上式即有：</p>
<p>$$<br>
\begin{eqnarray}<br>
\delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma’(z^l_j)<br>
\end{eqnarray}<br>
$$</p>
<h2 id="代价函数关于网络中任意偏置的改变率">代价函数关于网络中任意偏置的改变率</h2>
<p><strong>代价函数关于网络中任意偏置的改变率：</strong> 就是<br>
$$<br>
\begin{eqnarray}<br>
\frac{\partial C}{\partial b^l_j} = \delta^l_j<br>
\label{eq:bp3}\tag{BP3}<br>
\end{eqnarray}<br>
$$</p>
<p>这其实是，误差$\delta^l_j$ 和偏导数值 $\partial C / \partial b^l_j$<strong>完全一致</strong>。</p>
<h2 id="代价函数关于任何一个权重的改变率">代价函数关于任何一个权重的改变率</h2>
<p><strong>代价函数关于任何一个权重的改变率：</strong> 特别地，</p>
<p>$$<br>
\begin{eqnarray}<br>
\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j<br>
\label{eq:bp4}\tag{BP4}<br>
\end{eqnarray}<br>
$$</p>
<h2 id="反向传播算法描述">反向传播算法描述</h2>
<ul>
<li><strong>输入$x$：</strong> 为输入层设置对应的激活值$a^1$</li>
<li><strong>前向传播：</strong> 对每个$l=2,3,…,L$计算相应的$z^l = w^la^{l-1} + b^l$ 和 $a^l = \sigma(z^l)$</li>
<li><strong>输出层误差 $\delta^L$ ：</strong> 计算向量 $\delta^L = \nabla_a C \odot \sigma’(z^L)$</li>
<li><strong>反向误差传播：</strong> 对每个$l=L-1, L-2,…,2$ ，计算$\delta^l = ((w^{l+1})^T\delta^{l+1})\odot \sigma’(z^l)$</li>
<li><strong>输出：</strong> 代价函数的梯度由 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 和 $\frac{\partial C}{\partial b_j^l} = \delta_j^l$ 得出</li>
</ul>
<p>证明见<a href="https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/chap2-5.html">四个基本方程的证明</a>。</p>
<h1>代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won&#x27;t set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        <span class="variable language_">self</span>.sizes = sizes</span><br><span class="line">        <span class="variable language_">self</span>.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            test_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                <span class="variable language_">self</span>.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    j, <span class="variable language_">self</span>.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Update the network&#x27;s weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = <span class="variable language_">self</span>.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        <span class="variable language_">self</span>.weights = [w-(eta/<span class="built_in">len</span>(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.weights, nabla_w)]</span><br><span class="line">        <span class="variable language_">self</span>.biases = [b-(eta/<span class="built_in">len</span>(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``.&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> <span class="variable language_">self</span>.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="variable language_">self</span>.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.biases, <span class="variable language_">self</span>.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = <span class="variable language_">self</span>.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[-<span class="number">1</span>])</span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It&#x27;s a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, <span class="variable language_">self</span>.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(<span class="variable language_">self</span>.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, test_data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network&#x27;s output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation.&quot;&quot;&quot;</span></span><br><span class="line">        test_results = [(np.argmax(<span class="variable language_">self</span>.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h1>参考文献</h1>
<blockquote>
<p><a href="https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/cover.html">神经网络与深度学习</a></p>
</blockquote>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>nn</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention-based LSTM for Aspect-level Sentiment Classification</title>
    <url>/2021/11/25/Attention-based-LSTM-for-Aspect-level-Sentiment-Classification/</url>
    <content><![CDATA[<p>利用Aspect信息和Attention机制，解决Aspect-level的细粒度情感分析问题。</p>
<span id="more"></span>
<h1>主要贡献</h1>
<ol>
<li>提出了基于注意力机制的LSTM模型。当关注不同的aspect时，模型可以关注句子中的不同部分。</li>
<li>设计了两种方式将aspect信息输入在模型中：
<ol>
<li>对词进行向量表示时，将aspect embeddings附加到输入向量后面。</li>
<li>计算注意力权重时，将aspect embeddings和句子的隐藏层输出做concat，一起计算注意力权重。</li>
</ol>
</li>
</ol>
<h1>模型设计</h1>
<h2 id="模型结构图">模型结构图</h2>
<p><img src="atae-lstm.png" alt="atae-lstm"></p>
<h2 id="输入向量">输入向量</h2>
<ol>
<li>
<p>input embeddings：加载glove.6B.300.txt向量</p>
</li>
<li>
<p>aspect embeddings：aspect中字向量的加权平均</p>
</li>
<li>
<p>将input embeddings和aspect embeddings进行拼接，作为LSTM的输入<br>
$$<br>
\begin{bmatrix}<br>
x_t \<br>
x_{aspect}<br>
\end{bmatrix}<br>
$$</p>
</li>
</ol>
<h2 id="Attention层">Attention层</h2>
<ol>
<li>
<p>将LSTM输出向量与aspect向量进行拼接，并计算softmax，得到LSTM输出层的Attention系数<br>
$$<br>
M = tanh<br>
\begin{bmatrix}<br>
W_hH \<br>
W_vv_a \otimes e_N<br>
\end{bmatrix}<br>
$$</p>
<p>$$<br>
\alpha=softmax(w^TM)<br>
$$</p>
</li>
<li>
<p>将隐层向量与各相关系数进行点乘，得到attention向量<br>
$$<br>
r = H \alpha^{T}<br>
$$</p>
</li>
</ol>
<h2 id="任务层">任务层</h2>
<ol>
<li>
<p>将attention向量和隐层输出向量相加<br>
$$<br>
h^*=tanh(W_pr+W_xh_N)<br>
$$</p>
</li>
<li>
<p>最后接一个Dense层和一个softmax进行任务判别<br>
$$<br>
y = softmax(W_xh^*+b_s)<br>
$$</p>
</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://aclanthology.org/D16-1058.pdf">Attention-based LSTM for Aspect-level Sentiment Classification</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>PyCharm开发问题</title>
    <url>/2021/11/25/PyCharm%E5%BC%80%E5%8F%91%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>记录Pycharm中常见开发问题，以便后续查用。</p>
<span id="more"></span>
<h1>解决Pycharm不提示TensorFlow的问题</h1>
<p>TensorFlow 2.0中，Keras为懒加载，因此在Pycharm中不能加载代码提示。</p>
<p>在<code>tensorflow/__init__.py</code>文件最后添加如下导入语句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras <span class="keyword">as</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.util.lazy_loader <span class="keyword">import</span> LazyLoader</span><br><span class="line"><span class="comment"># pylint: disable=g-import-not-at-top</span></span><br><span class="line">keras = LazyLoader(<span class="string">&#x27;keras&#x27;</span>, <span class="built_in">globals</span>(), <span class="string">&#x27;tensorflow.keras&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> LazyLoader</span><br></pre></td></tr></table></figure>
<h1>pycharm 不能debug tensorflow keras的call函数</h1>
<p>增加@tf.autograph.experimental.do_not_convert</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.autograph.experimental.do_not_convert</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>saveModel时需要去除，否则会报<code>Missing support to serialize a method function without a named 'self' argument.</code>错误</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>pycharm</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</title>
    <url>/2021/11/11/SKEP-Sentiment-Knowledge-Enhanced-Pre-training-for-Sentiment-Analysis/</url>
    <content><![CDATA[<p>SKEP将不同类型的情感知识整合在一起，为各种情感分析任务提供统一的情感表示。</p>
<span id="more"></span>
<h1>模型解析</h1>
<p><img src="skep.png" alt="skep"></p>
<h2 id="Unsupervised-Sentiment-Knowledge-Mining">Unsupervised Sentiment Knowledge Mining</h2>
<h3 id="情感词挖掘：PMI-Pointwise-Mutual-Information">情感词挖掘：PMI(Pointwise Mutual Information)</h3>
<p>PMI方法仅依赖于少量的情感种子词，给出了每个种子词的词极性$WP(s)$。首先建立一个候选词对的集合，每个词对包含一个种子词，并满足预定义的词性模式。然后，一个词对的共现用PMI计算如下：<br>
$$<br>
PMI(w_1,w_2) = log \frac{p(w_1,w_2)}{p(w_1)p(w_2)}<br>
$$<br>
其中，$p(.)$表示通过计数估计的概率。</p>
<p>一个词的极性是由其PMI得分和所有正种子及负种子之间的差异决定的：<br>
$$<br>
WP(w) = \sum_{WP(s)=+}{PMI(w,s)} - \sum_{WP(s)=-}{PMI(w,s)}<br>
$$<br>
如果$WP(w)&gt;0$，判定$w$为正向情感词，反之为负向情感词。</p>
<h3 id="方面情感对">方面情感对</h3>
<p>一个情感词与其最近的名词将被视为一个方面-情感对，最大距离被经验地限制为不超过3个token。</p>
<h2 id="Sentiment-Masking">Sentiment Masking</h2>
<p>Sentiment Masking主要是构造掩盖情感信息的语料。此过程包含情感检测和混合情感掩盖两部分：</p>
<h3 id="Sentiment-Detection-with-Knowledge">Sentiment Detection with Knowledge</h3>
<ol>
<li>如果输入序列中的一个词也出现在知识库G中，那么这个词就被视为情感词。</li>
<li>情感词与其最近的名词（距离3以内）作为aspect-sentiment pair候选集，如果这样的 candidate也被发现在挖掘的知识G，那么它被认为是一个方面感情对。</li>
</ol>
<h3 id="Hybrid-Sentiment-Masking">Hybrid Sentiment Masking</h3>
<p>由三种类型token生成：aspect-sentiment pairs， sentiment words， common tokens.</p>
<ol>
<li>
<p>Aspect-sentiment Pair Masking：随机选择最多2个aspect-sentiment pair进行mask，这种masking方法提供了捕获aspect word和sentiment word的组合的方法</p>
</li>
<li>
<p>Sentiment Word Masking：对于未屏蔽的sentiment word，随机选取不超过10%的token进行mask。</p>
</li>
<li>
<p>Common Token Masking：如果上两个步骤中的mask数量不足（低于10%），随机选取一定数量的common tokens进行掩盖。</p>
</li>
</ol>
<h2 id="Sentiment-Pre-training-Objectives">Sentiment Pre-training Objectives</h2>
<p>$\widetilde{X}$为sentiment masking之后产生的序列，模型需要恢复三个被替换的目标：</p>
<p>$$<br>
L = L_{sw} + L_{wp} + L_{ap}<br>
$$</p>
<h3 id="Sentiment-Word（SW）-prediction：-L-sw"><strong>Sentiment Word（SW） prediction：</strong> $L_{sw}$</h3>
<p>利用 transformer encoder的output vector $\widetilde{x_i}$ 来恢复被屏蔽的情感词，$\widetilde{x_i}$被输入到一个softmax层，该层在整个vocab上产生一个归一化的概率向量$\widetilde{y_i}$，因此$L_{sw}$就是最大化下面的损失：<br>
$$<br>
\widehat{y}_i=softmax(\widetilde{x}_iW+b)<br>
$$</p>
<p>$$<br>
L_{sw}=-\sum^{i=n}_{i=1}{m_i\times y_i log{\widehat{y}_i}}<br>
$$</p>
<p>$W$和$b$是输出层参数，当$i$-th位置的词语被屏蔽时，$m_i$=1，否则为0，$\widetilde{y_i}$为原始词$x_i$的独热编码表示。</p>
<h3 id="Word-Polarity（WP）-prediction：-L-wp"><strong>Word Polarity（WP） prediction：</strong>$L_{wp}$</h3>
<p>$L_{wp}$与$L_{sw}$类似，计算每个被mask掉的sentiment token</p>
<h3 id="Aspect-sentiment-Pair（AP）prediction：-L-ap"><strong>Aspect-sentiment Pair（AP）prediction：</strong> $L_{ap}$</h3>
<p>论文中利用多标签分类对aspect-sentiment pair进行预测，使用最终的[CLS]来表示整个序列，利用sigmoid激活函数同时输出多个token的预测结果：<br>
$$<br>
\widehat{y}_a = sigmoid(\widetilde{x}_1W_{ap} + b_{ap})<br>
$$</p>
<p>$$<br>
L_{ap}=-\sum^{a=A}_{a=1}y_alog{\widehat{y}_a}<br>
$$</p>
<p>其中，$x_1$表示[CLS]的输出向量，$A$为被掩盖的aspect-sentiment pairs数量，$\widehat{y}_a$为预测的词语概率分布，$y_a$为目标aspect-sentiment pair的独热编码表示（$y_a$中多个位置为1）</p>
<h2 id="Fine-tuning-for-Sentiment-Analysis">Fine-tuning for Sentiment Analysis</h2>
<p>我们验证了三个典型的情绪分析任务：句子级的情绪分类，方面级情绪分类和观点角色标记。</p>
<h3 id="Sentence-level-Sentiment-Classification">Sentence-level Sentiment Classification</h3>
<p>这个任务是分类句子的情感极性。用【CLS】表示输入语句的整体表示形式。外接一个分类器来对整句进行情感极性分类。</p>
<h3 id="Aspect-level-Sentiment-Classification">Aspect-level Sentiment Classification</h3>
<p>这个任务的目的是当给定一个上下文文本时，分析方面的细粒度情感。因此输入包含两个部分：aspect和上下文。</p>
<p>讲这两部分用[SEP]进行拼接，输入到transformer encoder中，利用[CLS]向量判断是否属于aspect-sentiment pair。</p>
<h3 id="Opinion-Role-Labeling">Opinion Role Labeling</h3>
<p>这个任务是从输入文本中检测细粒度的观点。这个任务被转换成序列标记，它使用BIOS方案进行标记，并添加一个CRF层来预测标签。</p>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://arxiv.org/abs/2005.05635">SKPEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2021/12/01/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>激活函数为神经网络引入了非线性，增强了神经网络的表达能力。</p>
<span id="more"></span>
<h1>sigmoid</h1>
<p>函数表达式:<br>
$$<br>
f(z) = \frac{1}{1+e^{-z}}<br>
$$<br>
函数曲线:</p>
<p><img src="sigmoid.png" alt="sigmoid"></p>
<p>优点:</p>
<ol>
<li>输出范围是0到1，因此，对每个神经元的输出进行了归一化。</li>
<li>用于预测概率作为输出的模型</li>
<li>梯度平滑，不会出现【跳跃】的输出值。</li>
</ol>
<p>缺点:</p>
<ol>
<li>梯度消失:当函数的输出不是0附近时，会降低权重更新效率。</li>
<li>输出总是为正，随着层数的增加，样本的分布会从0-1高斯分布偏移至sigmoid的饱和区域，导致反向传播很难进行，收敛速度较慢。而batch-normalization会把样本强行拉回0-1高斯分布</li>
</ol>
<h1>tanh</h1>
<p>函数表达式:<br>
$$<br>
f(x) = tanh(x) = \frac{2}{1+e^{-2x}}-1<br>
$$<br>
函数曲线:</p>
<p><img src="tanh.png" alt="tanh"></p>
<p>与sigmoid函数的对比:</p>
<p><img src="sigmoid_tanh.png" alt="tanh and sigmoid"></p>
<p>优点:</p>
<ol>
<li>tanh相较于sigmoid函数的优点在于:中心对称，均值为0，能将0-1高斯分布依然映射到0附近的分布，保持零均值特性，所以，收敛速度较sigmoid快一些。</li>
</ol>
<p>缺点:</p>
<ol>
<li>梯度消失:当输入较大或较小时，梯度较小，不利于权重更新</li>
</ol>
<p>在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p>
<h1>relu</h1>
<p>函数表达式:<br>
$$<br>
\sigma(x) = <br>
\begin{cases}<br>
max(0,x)&amp;, x&gt;=0<br>
\<br>
0 &amp;, x&lt;0<br>
\end{cases}<br>
$$<br>
函数曲线:</p>
<p><img src="relu.png" alt="relu"></p>
<p>优点:</p>
<ol>
<li>当输入为正时，不存在梯度饱和问题；</li>
<li>计算速度快，relu函数中只存在线性关系，因此计算速度比sigmoid和tanh快。</li>
</ol>
<p>缺点:</p>
<ol>
<li>当输入为负时，relu完全失效，在反向传播过程中，如果输入是负数，则梯度完全为0。</li>
</ol>
<h1>softmax</h1>
<p>函数是用于多分类问题的激活函数，对于长度为 K 的任意实向量，Softmax 可以将其压缩为长度为 K，值在（0，1）范围内，并且向量中元素的总和为 1 的实向量。</p>
<p>函数表达式：<br>
$$<br>
f(x_i)=\frac{e^{x_i}}{\sum_{j=1}^{K}{e^{x_j}}}<br>
$$</p>
<p>函数曲线:</p>
<p><img src="softmax.png" alt="softmax"></p>
<p>优点:</p>
<ol>
<li>引入指数形式:能够将差距大的数值距离拉的更大</li>
</ol>
<p>缺点:</p>
<ol>
<li>指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当 $x_i$值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。</li>
</ol>
<h1>gelu</h1>
<p>对于每一个输入 $x$，其服从于标准正态分布 $N(0, 1)$，它会乘上一个伯努利分布 $Bernoulli(Φ(x))$，其中$Φ(x) = P(X ≤ x)$。<strong>随着 x 的降低，它被归零的概率会升高</strong>。</p>
<p>函数表达式:<br>
$$<br>
GELU(x) = xP(X \le x) = xΦ(x)<br>
$$<br>
函数曲线:</p>
<p><img src="gelu.png" alt="gelu"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>activation function</tag>
      </tags>
  </entry>
  <entry>
    <title>SemEval-2022 Task 10: Structured Sentiment Analysis</title>
    <url>/2021/11/22/SemEval-2022-Task-10-Structured-Sentiment-Analysis/</url>
    <content><![CDATA[<p>2022年情感识别任务Task10：<a href="https://competitions.codalab.org/competitions/33556">SemEval-2022 Task 10: Structured Sentiment Analysis</a></p>
<span id="more"></span>
<h1>任务描述</h1>
<p>Structured Sentiment Analysis主要是从文本中抽取结构化的情感图(Structured sentiment graph)。 即，从文本中抽取所有的观点元组$O=O_1,…,O_n$，每个观点$O_i$为一个四元组$(h,t,e,p)$</p>
<p>其中$h$为<strong>holder</strong>，$p$为<strong>polarity</strong>，$t$为<strong>target</strong>，$e$为<strong>sentiment expression</strong>， holder通过sentiment expression对target表达polarity。</p>
<p><img src="multi_sent_graph.png" alt="multi_sent_graph"></p>
<h2 id="单语言预测">单语言预测</h2>
<p>在相同的语言上训练和预测结构化情感图，需上传7个预测文本</p>
<h2 id="跨语言预测">跨语言预测</h2>
<p>探索模型在跨语言上的繁华能力。在多语言上进行训练，在 <code>MultiBooked、Datasets (Catalan and Basque)</code> 和 <code>OpeNER Spanish</code> 数据集上进行测试。</p>
<h1>数据集</h1>
<table>
<thead>
<tr>
<th>数据集</th>
<th>描述</th>
<th>训练集句子数</th>
</tr>
</thead>
<tbody>
<tr>
<td>$NoReC_{Fine}$</td>
<td>挪威语: 多领域的专业评论数据</td>
<td>8634</td>
</tr>
<tr>
<td>$MultiB_{EU}$</td>
<td>巴斯克語: 酒店评论数据</td>
<td>1064</td>
</tr>
<tr>
<td>$MultiB_{CA}$</td>
<td>加泰罗尼亚语:酒店评论数据</td>
<td>1174</td>
</tr>
<tr>
<td>$MPQA$</td>
<td>英语: 新闻语料</td>
<td>4500</td>
</tr>
<tr>
<td>$DS_{Unis}$</td>
<td>英语:网课和电子商务评论语料</td>
<td>2253</td>
</tr>
<tr>
<td>$OpeNER_es$</td>
<td>西班牙语:</td>
<td>1439</td>
</tr>
<tr>
<td>$OpeNER_en$</td>
<td>英语:</td>
<td>1745</td>
</tr>
</tbody>
</table>
<h1>评估方法</h1>
<p><strong>Sentiment Graph F1</strong></p>
<p>对元组级别(holder, target, expression, polarity)的记录进行对比，在polarity一致的情况下，在holder/target/expression三种元素上，利用预测元素和真实元素的字符重叠情况作为对应元素的准确情况，并取三个元素准确情况的平均值作为元组的准确情况。<br>
$$<br>
tp_{holder} = \frac{intersect(holder_{predict}, holder_{true})}{holder_{predict}}<br>
$$<br>
$$<br>
tp_{target} = \frac{intersect(target_{predict}, target_{true})}{target_{predict}}<br>
$$<br>
$$<br>
tp_{expression} = \frac{intersect(expression_{predict}, expression_{true})}{expression_{predict}}<br>
$$<br>
$$<br>
tp = \frac{tp_{holder} + tp_{target} + tp_{expression}}{3}<br>
$$</p>
<p>准确率$P$: 所有预测准确的元素占预测元素的比例</p>
<p>召回率$R$：所有预测准确的元素占真实元素的比例</p>
<p>Sentiment Graph F1：<br>
$$<br>
F_1 = \frac{2\times P \times R}{P + R}<br>
$$</p>
<h1>Baseline</h1>
<table>
<thead>
<tr>
<th>方法</th>
<th>darmstadt_unis</th>
<th>mpqa</th>
<th>multibooked_ca</th>
<th>multibooked_eu</th>
<th>norec</th>
<th>opener_en</th>
<th>opener_es</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pipeline sequence labeling + relation classification</td>
<td>0.072</td>
<td>0.011</td>
<td>0.289</td>
<td>0.405</td>
<td>0.201</td>
<td>0.254</td>
<td></td>
</tr>
<tr>
<td>graph_parser</td>
<td>0</td>
<td>0</td>
<td>0.038</td>
<td>0.065</td>
<td>0.011</td>
<td>0.098</td>
<td>0.068</td>
</tr>
</tbody>
</table>
<h2 id="Pipeline-Sequence-labeling-relation-classification">Pipeline Sequence labeling + relation classification</h2>
<p>任务分成两部分: 元素抽取(holder/target/expression) 、情感极性分类</p>
<ul>
<li>元素抽取：训练三个BiLSTM模型进行 holder、target、expression的抽取</li>
<li>情感极性分类：BiLSTM + max pooling获取三个表示向量，将三个向量拼接后接一个线性分类器进行polarity预测。
<ul>
<li>全文向量表示</li>
<li>holder或者target的向量表示</li>
<li>expression的向量表示</li>
</ul>
</li>
</ul>
<h2 id="Graph-Parser">Graph Parser</h2>
<p>看做是一个双词依存图的预测任务( a bilexical dependency graph prediction task）。</p>
<p><img src="bilexical.png" alt="bilexical"></p>
<p>模型paper</p>
<ul>
<li><a href="https://aclanthology.org/2021.acl-long.263/">Structured Sentiment Analysis as Dependency Graph Parsing</a>.</li>
<li><a href="https://aclanthology.org/2020.iwpt-1.3/">End-to-End Negation Resolution as Graph Parsing</a>.</li>
</ul>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义模型的保存与加载</title>
    <url>/2021/11/28/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
    <content><![CDATA[<p>TensorFlow2中自定义Model/Layer的保存与加载。</p>
<span id="more"></span>
<p>保存/加载带有自定义层的模型或子类化模型分成两步：</p>
<ol>
<li>
<p>您应该重写 <code>get_config</code> 和 <code>from_config</code>（可选）方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;将返回一个包含模型配置的 Python 字典。&#x27;&#x27;&#x27;</span></span><br><span class="line">    config = <span class="built_in">super</span>(Attention, <span class="variable language_">self</span>).get_config()</span><br><span class="line">    config.update(&#123;</span><br><span class="line">        <span class="string">&#x27;use_W&#x27;</span>: <span class="variable language_">self</span>.use_W,</span><br><span class="line">        <span class="string">&#x27;return_self_attend&#x27;</span>: <span class="variable language_">self</span>.return_self_attend,</span><br><span class="line">        <span class="string">&#x27;return_attend_weight&#x27;</span>: <span class="variable language_">self</span>.return_attend_weight</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>您应该注册自定义对象，以便 Keras 能够感知它</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">custom_objects = &#123;<span class="string">&#x27;Attention&#x27;</span>: Attention&#125;</span><br><span class="line"><span class="variable language_">self</span>.model = tf.keras.models.load_model(<span class="variable language_">self</span>.config.checkpoint_file, custom_objects=custom_objects)</span><br><span class="line"><span class="built_in">print</span>(<span class="variable language_">self</span>.model.summary())</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>load_model</tag>
        <tag>save_model</tag>
      </tags>
  </entry>
  <entry>
    <title>高维矩阵乘法</title>
    <url>/2021/12/08/%E9%AB%98%E7%BB%B4%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/</url>
    <content><![CDATA[<p>NLP中经常为碰到高维矩阵运算，如Attention中的Q、K、V相乘，在此记录矩阵相乘的运算规则。</p>
<span id="more"></span>
<h1>高维矩阵可视化</h1>
<p><strong>一维:</strong> 首先shape=[4]的一维矩阵非常简单，可以用下图表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<p><img src="yiwei.jpg" alt="一维矩阵"></p>
<p>**二维:**shape=[2,3]的二维矩阵可视化如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
<p><img src="erwei.jpg" alt="二维"></p>
<p>为方便展示三维矩阵，旋转角度如下:</p>
<p><img src="erwei2.jpg" alt="二维"></p>
<p>**三维:**一个shape=[2,2,3]的三维矩阵，可视化如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">  [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line"> </span><br><span class="line"> [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],</span><br><span class="line">  [<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]]</span><br></pre></td></tr></table></figure>
<p><img src="sanwei.jpg" alt="三维"></p>
<p>切片展示如下:</p>
<p><img src="sanwei2.jpg" alt="三维"></p>
<p>**四维:**shape=[2,2,2,3]的四维矩阵可视化如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">  </span><br><span class="line">  [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],</span><br><span class="line">   [<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]],</span><br><span class="line">  </span><br><span class="line">  [[[<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>],</span><br><span class="line">    [<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>]],</span><br><span class="line">   </span><br><span class="line">  [[<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>],</span><br><span class="line">  [<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>]]]]</span><br></pre></td></tr></table></figure>
<p><img src="siwei.jpg" alt="四维"></p>
<h1>高维矩阵运算</h1>
<p>从上面的结论可以看出:<strong>所有大于二维的，最终都是以二维为基础堆叠在一起的！！</strong></p>
<p>所以在矩阵运算的时候，其实最后都可以转成我们常见的二维矩阵运算，遵循的原则是：<strong>在多维矩阵相乘中，需最后两维满足shape匹配原则，最后两维才是有数据的矩阵，前面的维度只是矩阵的排列而已！</strong></p>
<p><strong>相乘必须满足以下两个条件：</strong></p>
<ol>
<li>两个n维数组的前n-2维必须完全相同。例如（3,2,4,2）（3,2,2,3）前两维必须完全一致；</li>
<li>最后两维必须满足二阶矩阵乘法要求。例如（3,2,4,2）（3,2,2,3）的后两维可视为（4,2）x（2,3）满足矩阵乘法。</li>
</ol>
<p>另，由于广播机制，第一维为1的，可以与第一维任何数相乘：</p>
<p>（3,2,4,2）*（1,2,2,3）——&gt;&gt;（3,2,4,3）</p>
<p>（1,2,4,2）*（3,2,2,3）——&gt;&gt;（3,2,4,3）</p>
<p>比如两个三维的矩阵相乘，分别为shape=[2,2,3]和shape=[2,3,2]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = </span><br><span class="line">[[[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>]</span><br><span class="line">  [ <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line">  [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]]</span><br><span class="line"></span><br><span class="line">b = </span><br><span class="line">[[[ <span class="number">1.</span>  <span class="number">2.</span>]</span><br><span class="line">  [ <span class="number">3.</span>  <span class="number">4.</span>]</span><br><span class="line">  [ <span class="number">5.</span>  <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line"> [[ <span class="number">7.</span>  <span class="number">8.</span>]</span><br><span class="line">  [ <span class="number">9.</span> <span class="number">10.</span>]</span><br><span class="line">  [<span class="number">11.</span> <span class="number">12.</span>]]]</span><br></pre></td></tr></table></figure>
<p>计算的时候把a的第一个shape=[2,3]的矩阵和b的第一个shape=[3,2]的矩阵相乘，得到的shape=[2,2]，即</p>
<p><img src="matmul1.jpg" alt="matmul1"></p>
<p>同理，再把a，b个字的第二个shape=[2,3]的矩阵相乘，得到的shape=[2,2]。</p>
<p><img src="matmul2.jpg" alt="matmul2"></p>
<p>最终把结果堆叠在一起，就是2个shape=[2,2]的矩阵堆叠在一起，结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line">  [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">220.</span> <span class="number">244.</span>]</span><br><span class="line">  [<span class="number">301.</span> <span class="number">334.</span>]]]</span><br></pre></td></tr></table></figure>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/337829793">【全面理解多维矩阵运算】多维（三维四维）矩阵向量运算-超强可视化</a></li>
<li><a href="https://blog.csdn.net/weixin_45459911/article/details/107852351">高维数组相乘的运算规则</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>tensor</tag>
        <tag>numpy</tag>
        <tag>matmul</tag>
      </tags>
  </entry>
  <entry>
    <title>江浙沪四日行程</title>
    <url>/2024/04/11/%E6%B1%9F%E6%B5%99%E6%B2%AA%E5%9B%9B%E6%97%A5%E8%A1%8C%E7%A8%8B/</url>
    <content><![CDATA[<p>深圳出发，主要走杭州、乌镇、上海。</p>
<span id="more"></span>
<h2 id="行前准备">行前准备</h2>
<h3 id="天气情况">天气情况</h3>
<p>多云，18~26°C</p>
<p>需要准备薄外套、雨伞等。</p>
<h3 id="景点预约">景点预约</h3>
<p><strong>4月19日</strong></p>
<ul>
<li>西湖旅游公众号
<ul>
<li>钱王祠，￥15</li>
<li>岳王庙：￥25</li>
</ul>
</li>
<li>美团
<ul>
<li>雷峰塔：￥40</li>
<li>京杭大运河夜游票+钱塘江夜游票：￥188</li>
</ul>
</li>
<li>现场购票：
<ul>
<li>三潭印月上岛游：￥55</li>
</ul>
</li>
</ul>
<p><strong>4月20日</strong></p>
<ul>
<li>西湖旅游公众号
<ul>
<li>北高峰索道：￥30</li>
<li>飞来峰：￥45</li>
</ul>
</li>
<li>现场购票：
<ul>
<li>灵隐寺：￥30</li>
<li>法喜寺：￥10</li>
</ul>
</li>
</ul>
<p><strong>4月21日</strong></p>
<ul>
<li>
<p>乌镇景区公众号：</p>
<ul>
<li>西栅景区：￥150</li>
<li>木心美术馆：￥20</li>
</ul>
</li>
<li>
<p>美团/现场：</p>
<ul>
<li>游船：双人景区票+游船：￥580</li>
</ul>
</li>
<li>
<p>上海豫园：</p>
<ul>
<li>豫园：￥40</li>
</ul>
</li>
<li>
<p>黄浦江夜游公众号</p>
<ul>
<li>黄浦江游船：￥135</li>
</ul>
</li>
</ul>
<p><strong>4月22日</strong></p>
<ul>
<li>东方明珠公众号：二球观光+旋转餐厅自助餐：￥368</li>
<li>复旦信息办公众号：复旦大学</li>
<li></li>
</ul>
<h2 id="4月19日（杭州">4月19日（杭州)</h2>
<h3 id="深圳-杭州">深圳-&gt;杭州</h3>
<table>
<thead>
<tr>
<th>航班信息</th>
<th>起降时间</th>
<th>起降机场</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>深航ZH9873</strong></td>
<td>07:20 - 09:10</td>
<td>宝安国际机场T3 - 萧山国际机场T3</td>
</tr>
<tr>
<td><strong>南航CZ3569</strong></td>
<td>08:00 - 10:05</td>
<td>宝安国际机场T3 - 萧山国际机场T4</td>
</tr>
</tbody>
</table>
<h3 id="午餐">午餐</h3>
<ul>
<li>弄堂里(湖滨银泰in77C区店)：臭豆腐、甜糕、龙井虾球、叫花鸡</li>
<li>知味观（湖滨店）：三鲜小笼，脆皮糯米藕，知味馄饨</li>
<li>楼外楼（西湖景区孤山路店）：西湖醋鱼，宋嫂鱼羹，虾爆鳝面</li>
</ul>
<h3 id="下午">下午</h3>
<h4 id="路线总览">路线总览</h4>
<p><img src="0419.jpg" alt="西湖步行路线"></p>
<h4 id="集贤亭">集贤亭</h4>
<p>观赏日落真的很美，湖中亭台，一侧还有荷塘，还有很多野鸭，以及手划船，远处是青山黛色，真的很美。</p>
<p><img src="%E9%9B%86%E8%B4%A4%E4%BA%AD.jpg" alt="集贤亭"></p>
<h4 id="三潭印月">三潭印月</h4>
<p>三潭印月，西湖十景中唯一需要<strong>乘船</strong>才可以参观的景点。三潭印月岛(又称小瀛洲)。俯瞰整个小瀛洲犹如一个“田字，园林富于空间层次变化，造成“湖中湖”“岛中岛”“园中园”的境界。岛南湖中建成有三座石塔，相传最初为苏东坡在杭疏浚西湖时所创设，月明之夜，塔中点燃灯光，真月和假月其影确实难分故得名“三潭印月”。</p>
<p><img src="%E4%B8%89%E6%BD%AD%E5%8D%B0%E6%9C%88.jpg" alt="三潭印月"></p>
<h4 id="南屏晚钟">南屏晚钟</h4>
<p>具体指:南屏山净慈寺傍晚的钟声，西湖十景中问世最早的景目，也是唯—个以声音命名的西湖十景。净慈寺始建于954年，是一座皇家寺庙。寺内的钟楼上层悬大梵钟，下层为净慈寺地藏殿。大梵钟高3米，口径2.3米，重10余吨。钟声敲响，振荡频率传到山上形成悠扬共振齐鸣的钟声。</p>
<p><img src="%E9%9B%B7%E5%B3%B0%E5%A1%94.jpg" alt="雷峰塔（『净慈寺』最高的大殿）"></p>
<h4 id="苏堤春晓">苏堤春晓</h4>
<p>苏堤南起南屏山麓，北到栖霞岭下，全长近3千米。北宋文人苏轼用疏浚西湖时挖出的湖泥堆筑了一条南北走向的长堤，后人为了纪念苏东坡治理西湖的功绩将她命名为苏堤。</p>
<p><img src="%E5%9F%8E%E9%9A%8D%E9%98%81%E6%97%A5%E5%87%BA.jpg" alt="城隍阁日出（西湖苏堤『仁风亭』附近）"></p>
<p><img src="%E7%8E%89%E5%B8%A6%E6%A1%A5.jpg" alt="玉带桥（西湖游船岳庙码头）"></p>
<h4 id="西冷桥">西冷桥</h4>
<p>路的尽头是海，是遇见杭州最美的转角。</p>
<p><img src="%E8%A5%BF%E5%86%B7%E6%A1%A5.jpg" alt="网红转角（西冷桥）"></p>
<h4 id="音乐喷泉">音乐喷泉</h4>
<p>19:30开始，连续播放四首歌曲，总时长15-20分钟，第一排绝佳视野，最晚18：30前要进场。</p>
<p><img src="%E9%9F%B3%E4%B9%90%E5%96%B7%E6%B3%89.jpg" alt="西湖音乐喷泉"></p>
<h4 id="font-color-lightgrey-玉皇山-font"><font color='lightgrey'>玉皇山</font></h4>
<p><font color='lightgrey'>玉皇山至高点，可以俯瞰西湖，上面的江湖一览亭可以看到雷峰塔、西湖文化广场、望宸阁三点一线。</font></p>
<p><img src="%E7%A6%8F%E6%98%9F%E8%A7%82.jpg" alt="福星观"></p>
<h3 id="晚餐">晚餐</h3>
<p>武林夜市</p>
<h3 id="夜晚">夜晚</h3>
<h4 id="京杭大运河夜游-钱塘江夜游">京杭大运河夜游+钱塘江夜游</h4>
<p>城市灯光秀尽收眼底～</p>
<p>武林门码头坐船🚢</p>
<h4 id="住宿">住宿</h4>
<p>北高峰索道附近</p>
<h2 id="4月20日（杭州、乌镇">4月20日（杭州、乌镇)</h2>
<h3 id="上午">上午</h3>
<h4 id="线路总览">线路总览</h4>
<p><img src="0420.jpg" alt="线路总览"></p>
<h4 id="北高峰财神庙">北高峰财神庙</h4>
<p>天下第一财神庙 ，北高峰索道售票处坐索道，上山30元，下山20元，索道营业时间是8:00—16:45。</p>
<p><img src="%E5%A4%A9%E4%B8%8B%E7%AC%AC%E4%B8%80%E8%B4%A2%E7%A5%9E%E5%BA%99.jpg" alt="天下第一财神庙"></p>
<h4 id="飞来峰">飞来峰</h4>
<p>灵隐寺景区位于杭州西湖风景名胜区西部的山峰之间，因其周边山水禅意隐现，被称为“灵隐禅踪”，它由“灵隐寺”（见下篇）和“飞来峰”两部分构成。飞来峰，又名灵鹫峰，山体由高约168米的石灰岩构成，因慧理所述“此乃中天竺国灵鹫山之小岭，不知何以飞来？”而得名。</p>
<p><img src="%E9%A3%9E%E6%9D%A5%E5%B3%B0.jpg" alt="飞来峰"></p>
<h4 id="灵隐寺">灵隐寺</h4>
<p>整体是恢弘大气的感觉，一共有5个殿：大雄宝殿（求财求事业）、天王殿（保平安吉祥）、观音殿（求姻缘子嗣）、药师殿（保健康长寿）、华严殿（求事业学业）， 去之前记得提前准备好纸币和硬币，纸币投入功德箱，硬币放入许愿池。</p>
<p><img src="%E7%81%B5%E9%9A%90%E5%AF%BA.jpg" alt="灵隐寺"></p>
<h4 id="法喜寺">法喜寺</h4>
<p>寺庙整体建筑风格典雅古朴，给人一种宁静祥和的感觉。</p>
<p><img src="%E6%B3%95%E5%96%9C%E5%AF%BA1.jpg" alt="网红机位"></p>
<p><img src="%E6%B3%95%E5%96%9C%E5%AF%BA2.jpg" alt="天王殿的背面"></p>
<p><img src="%E6%B3%95%E5%96%9C%E5%AF%BA3.jpg" alt="天王殿和圆通殿之间的如意池"></p>
<p><img src="%E6%B3%95%E5%96%9C%E5%AF%BA4.jpg" alt="大雄宝殿的背面"></p>
<h4 id="龙井村">龙井村</h4>
<p>800亩高山茶园，鳞次栉比，西湖龙井的原产地。龙井茶3月采摘，明前龙井，是指清明节之前采的茶叶🍃，这时的茶，一芽一叶初展，滋味醇厚，清纯持久。</p>
<p><img src="%E9%BE%99%E4%BA%95%E6%9D%91.jpg" alt="龙井村"></p>
<h4 id="三分叉">三分叉</h4>
<p>三分叉，这里是最精华部分，俯瞰眺望西湖，漫山的茶园连绵不绝，视野很开阔，非常治愈。</p>
<p><img src="%E4%B8%89%E5%88%86%E5%8F%89.jpg" alt="三分叉"></p>
<h4 id="十里琅珰">十里琅珰</h4>
<p>从龙井村牌坊处大约半小时即可达到，可以远眺西湖，从山上往下看漫山遍野的茶树，随便拍拍都有航拍视角</p>
<p><img src="%E5%8D%81%E9%87%8C%E7%90%85%E7%8F%B0.jpg" alt="十里琅珰"></p>
<h4 id="梅家坞">梅家坞</h4>
<p>梅家坞是西湖龙井的核心产区之一。</p>
<p><img src="%E6%A2%85%E5%AE%B6%E5%9D%9E.jpg" alt="孤独的一棵树"></p>
<h3 id="午餐-2">午餐</h3>
<p>梅家坞附近</p>
<h4 id="杭州-乌镇">杭州 -&gt; 乌镇</h4>
<ul>
<li>顺风车</li>
<li>梅花坞 --&gt; 武林广场 --&gt; 乌镇西栅景区</li>
</ul>
<h4 id="住宿-2">住宿</h4>
<p>西栅景区内临水名宿</p>
<p><strong>入住，并预约水上集市早茶客</strong></p>
<h3 id="下午-2">下午</h3>
<h4 id="线路总览-2">线路总览</h4>
<p><img src="%E4%B9%8C%E9%95%87%E8%B7%AF%E7%BA%BF%E5%9B%BE.jpg" alt="乌镇西栅路线"></p>
<h4 id="木心美术馆">木心美术馆</h4>
<p>木心美术馆展出的是画家、作家木心先生的绘画与文学作品，展馆由贝聿铭弟子设计，“风啊、水啊、一顶桥”～外观与木心先生心仪的简约美学十分契合。需要避开周一，周一闭馆。</p>
<p><img src="%E6%9C%A8%E5%BF%83%E7%BE%8E%E6%9C%AF%E9%A6%863.jpg" alt="与门头合影"></p>
<p><img src="%E6%9C%A8%E5%BF%83%E7%BE%8E%E6%9C%AF%E9%A6%861.jpg" alt="旋转楼梯"></p>
<p><img src="%E6%9C%A8%E5%BF%83%E7%BE%8E%E6%9C%AF%E9%A6%862.jpg" alt="斜坡走廊"></p>
<h4 id="白莲塔">白莲塔</h4>
<p>可登高拍摄，塔前的江南水乡，塔后的京杭大运河</p>
<p><img src="%E7%99%BD%E8%8E%B2%E5%A1%94.jpg" alt="白莲塔"></p>
<h4 id="水上集市">水上集市</h4>
<p>拍热闹的水上集市超出片，旁边是明月楼茶馆，市井调调让人很放松。</p>
<p><img src="%E6%B0%B4%E4%B8%8A%E9%9B%86%E5%B8%82.jpg" alt="水上集市"></p>
<p><img src="%E6%B0%B4%E4%B8%8A%E9%9B%86%E5%B8%821.jpg" alt="水上集市"></p>
<h3 id="晚餐-2">晚餐</h3>
<ul>
<li>
<p>茅老太臭豆腐</p>
</li>
<li>
<p><strong>书生羊肉面</strong></p>
</li>
<li>
<p>锦记酱鸭</p>
</li>
<li>
<p>吴妈馄饨</p>
</li>
<li>
<p>永平团粉店</p>
</li>
<li>
<p>滋啦啦油煎铺</p>
</li>
<li>
<p>葱包烩</p>
</li>
<li>
<p>通济酱粽</p>
</li>
</ul>
<h4 id="夜晚-2">夜晚</h4>
<p>**乘船线路：**安渡坊码头 - 如意桥码头</p>
<p><img src="%E4%B9%8C%E9%95%87%E5%A4%9C%E6%99%AF1.jpg" alt="乌镇夜景"></p>
<h2 id="4月21日（乌镇、上海">4月21日（乌镇、上海)</h2>
<h3 id="早餐">早餐</h3>
<p><strong>早茶客</strong>（只要住在景区内的就能去早茶客，在游客中心办理入住后尽快预约，每天限额300名，预约不到也没关系，可以自费体验）</p>
<h3 id="上午-2">上午</h3>
<p>西栅晨景</p>
<h4 id="乌镇-上海">乌镇 -&gt; 上海</h4>
<p>从乌镇西栅1号停车场和乌镇东栅景区有发往上海人民广场的专线车，全程2.5h左右。</p>
<h4 id="住宿-3">住宿</h4>
<p>上海人民广场附近</p>
<h3 id="下午-3">下午</h3>
<h4 id="南京路步行街">南京路步行街</h4>
<p><img src="%E5%8D%97%E4%BA%AC%E8%B7%AF%E6%AD%A5%E8%A1%8C%E8%A1%97.jpg" alt="南京路步行街"></p>
<h4 id="沙美大楼">沙美大楼</h4>
<p>传说中的魔都绝美天台，可以直接打卡魔都三件套，还有一家可以俯瞰外滩的绝美小众露台咖啡。</p>
<p><img src="%E6%B2%99%E7%BE%8E%E5%A4%A7%E6%A5%BC.jpg" alt="沙美大楼"></p>
<h4 id="上海邮政博物馆">上海邮政博物馆</h4>
<p><img src="%E4%B8%8A%E6%B5%B7%E9%82%AE%E6%94%BF%E5%8D%9A%E7%89%A9%E9%A6%86.jpg" alt="上海邮政博物馆"></p>
<h4 id="北京东路">北京东路</h4>
<p><img src="%E5%8C%97%E4%BA%AC%E4%B8%9C%E8%B7%AF.jpg" alt="北京东路"></p>
<h4 id="人民英雄纪念塔">人民英雄纪念塔</h4>
<p><img src="%E4%BA%BA%E6%B0%91%E8%8B%B1%E9%9B%84%E7%BA%AA%E5%BF%B5%E5%A1%94.jpg" alt="人民英雄纪念塔"></p>
<h4 id="九江路">九江路</h4>
<p><img src="%E4%B9%9D%E6%B1%9F%E8%B7%AF.jpg" alt="九江路"></p>
<h4 id="上海城隍庙-豫园">上海城隍庙/豫园</h4>
<p>17.30亮灯 22.00熄灯。</p>
<p><img src="%E8%B1%AB%E5%9B%AD.jpg" alt="豫园"></p>
<p><img src="%E8%B1%AB%E5%9B%AD2.jpg" alt="豫园"></p>
<h3 id="晚餐-3">晚餐</h3>
<p>城隍庙小吃</p>
<h3 id="夜晚-3">夜晚</h3>
<h4 id="黄浦江夜游">黄浦江夜游</h4>
<p>线路：十六铺码头一滨江大道一上海中心一上海国际会议中心一东方明珠—秦皇岛码头(调头)—北外滩一自玉兰大厦—外白渡桥一外滩万国建筑博览群—十六铺码头</p>
<p><img src="%E9%BB%84%E6%B5%A6%E6%B1%9F%E5%A4%9C%E6%B8%B8%E7%BA%BF%E8%B7%AF.jpg" alt="黄浦江夜游"></p>
<h4 id="上海外滩">上海外滩</h4>
<p><img src="%E4%B8%8A%E6%B5%B7%E5%A4%96%E6%BB%A9.jpg" alt="上海外滩"></p>
<p><img src="%E4%B8%8A%E6%B5%B7%E5%A4%96%E6%BB%A91.jpg" alt="上海外滩"></p>
<h4 id="北外滩">北外滩</h4>
<p><img src="%E4%B8%96%E7%95%8C%E4%BC%9A%E5%AE%A2%E5%8E%85.jpg" alt="世界会客厅"></p>
<h2 id="4月22日（上海">4月22日（上海)</h2>
<h3 id="上午-3">上午</h3>
<p>徐汇区Citywalk：徐家汇天主堂 -&gt; 徐家汇书院  -&gt; 美罗城-&gt; 上海交通大学徐汇校区 -&gt; 武康大楼 -&gt; 宋庆龄故居。</p>
<h4 id="徐家汇天主教堂">徐家汇天主教堂</h4>
<p>典型的哥特式建筑，拥有双塔尖顶和玫瑰花窗，外观非常壮观。教堂内部装饰精美，有许多精美的雕塑、彩绘装饰玻璃窗画及油画都值得驻足欣赏。</p>
<p>开放时间：徐家汇天主教堂的开放时间为周二至周六的9:00-16:00。</p>
<p><img src="%E5%85%89%E5%9C%A3%E4%BE%9D%E7%BA%B3%E7%88%B5%E5%A0%82.jpg" alt="徐家汇光圣依纳爵堂"></p>
<p><img src="%E5%BE%90%E5%AE%B6%E6%B1%87%E5%A4%A9%E4%B8%BB%E5%A0%82.jpg" alt="徐家汇天主堂"></p>
<h4 id="徐家汇书院">徐家汇书院</h4>
<p>魔都2023最火的图书馆，也是目前国内最大的单体图书馆，免费入馆，参观无需提前预约。</p>
<p><img src="%E5%BE%90%E5%AE%B6%E6%B1%87%E4%B9%A6%E9%99%A2.jpg" alt="徐家汇书院"></p>
<h4 id="美罗城">美罗城</h4>
<p><img src="%E7%BE%8E%E7%BD%97%E5%9F%8E.jpg" alt="美罗城"></p>
<h4 id="上海交通大学徐汇校区">上海交通大学徐汇校区</h4>
<p>徐汇校区宫殿式大门</p>
<p><img src="%E4%B8%8A%E6%B5%B7%E4%BA%A4%E5%A4%A7.jpg" alt="上海交大"></p>
<h4 id="武康路大楼">武康路大楼</h4>
<p>武康大楼是上海最具代表性的建筑之一，独特的船型结构是让这座大楼成了上海必打卡的项目。</p>
<p><img src="%E6%AD%A6%E5%BA%B7%E8%B7%AF%E5%A4%A7%E6%A5%BC.jpg" alt="武康大楼"></p>
<h4 id="宋庆龄故居">宋庆龄故居</h4>
<p>北京宋庆龄故居，始建于清朝康熙年间，有300多年历史，曾是和珅别院和末代皇帝溥仪的父亲醇亲王的府邸花园，是清代四大王府花园之一。</p>
<p><img src="%E5%AE%8B%E5%BA%86%E9%BE%84%E6%95%85%E5%B1%85.jpg" alt="宋庆龄故居"></p>
<h4 id="font-color-lightgrey-静安寺-font"><font color='lightgrey'>静安寺</font></h4>
<p><font color='lightgrey'>静安区亦由静安寺而闻名于世。静安寺的建筑风格是仿明代以前的建筑风格，典型的代表就是斗拱的形制。</font></p>
<p><img src="%E9%9D%99%E5%AE%89%E5%AF%BA.jpg" alt="正南200米左右天桥就是打卡机位"></p>
<h4 id="东方明珠">东方明珠</h4>
<p><img src="%E4%B8%9C%E6%96%B9%E6%98%8E%E7%8F%A0.jpg" alt="东方明珠"></p>
<p><img src="%E4%B8%9C%E6%96%B9%E6%98%8E%E7%8F%A01.jpg" alt="东方明珠江景"></p>
<h3 id="午餐-3">午餐</h3>
<p>东方明珠旋转餐厅</p>
<h3 id="下午-4">下午</h3>
<h4 id="复旦大学">复旦大学</h4>
<p>路线:东北1门➡️旦苑食堂➡️光华楼15层星空餐厅&amp;30层观景或蹲日落➡️光草➡️光华大道➡️主席像➡️复旦大学大门➡️纪念品商店➡️燕园➡️校史馆➡️相草➡️相辉堂校训墙➡️北食</p>
<p><img src="%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A61.jpg" alt="复旦大学校门"></p>
<p><img src="%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A62.jpg" alt="光华楼"></p>
<h3 id="上海-深圳">上海-深圳</h3>
<table>
<thead>
<tr>
<th>航班信息</th>
<th>起降时间</th>
<th>起降机场</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>南航CZ3558</strong></td>
<td>18:40 - 21:10</td>
<td>虹桥国际机场T2 - 宝安国际机场T3</td>
</tr>
<tr>
<td><strong>深航ZH9538</strong></td>
<td>19:50 - 22:15</td>
<td>虹桥国际机场T2 - 宝安国际机场T3</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>攻略</category>
      </categories>
      <tags>
        <tag>杭州</tag>
        <tag>上海</tag>
        <tag>乌镇</tag>
        <tag>攻略</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT系列模型</title>
    <url>/2024/08/02/GPT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>GPT系列模型原理对比及技术演进。</p>
<span id="more"></span>
<p>总体演进过程：</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>维度</strong></th>
<th style="text-align:center"><strong>GPT-1</strong></th>
<th style="text-align:center"><strong>GPT-2</strong></th>
<th style="text-align:center"><strong>GPT-3</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>参数量</strong></td>
<td style="text-align:center">1.17亿</td>
<td style="text-align:center">15亿</td>
<td style="text-align:center">1750亿</td>
</tr>
<tr>
<td style="text-align:center"><strong>训练数据</strong></td>
<td style="text-align:center">5GB（书籍）</td>
<td style="text-align:center">40GB（网页）</td>
<td style="text-align:center">45TB（混合互联网数据）</td>
</tr>
<tr>
<td style="text-align:center"><strong>架构改进</strong></td>
<td style="text-align:center">基础单向Transformer解码器</td>
<td style="text-align:center">层归一化前置、上下文窗口扩大</td>
<td style="text-align:center">稀疏注意力、深度扩展</td>
</tr>
<tr>
<td style="text-align:center"><strong>任务泛化方式</strong></td>
<td style="text-align:center">预训练+微调</td>
<td style="text-align:center">零样本提示</td>
<td style="text-align:center">少样本上下文学习</td>
</tr>
<tr>
<td style="text-align:center"><strong>关键创新</strong></td>
<td style="text-align:center">预训练-微调范式</td>
<td style="text-align:center">语言模型即多任务系统</td>
<td style="text-align:center">涌现能力、Prompt工程</td>
</tr>
</tbody>
</table>
<h2 id="GPT1-Improving-Language-Understanding-by-Generative-Pre-Training">GPT1: Improving Language Understanding by Generative Pre-Training</h2>
<p><strong>预训练-微调范式的奠基</strong></p>
<ul>
<li><strong>技术架构</strong>：
<ul>
<li>基于<strong>Transformer解码器</strong>的纯单向结构，仅使用<strong>掩码自注意力机制</strong>（屏蔽未来词），共12层，参数量1.17亿。</li>
<li>输入处理采用<strong>位置编码+词嵌入</strong>，通过自回归语言模型（从左到右预测下一个词）学习上下文表示。</li>
</ul>
</li>
<li><strong>训练范式</strong>：
<ul>
<li><strong>两阶段流程</strong>：
<ul>
<li><strong>无监督预训练</strong>：使用约5GB的书籍语料（BooksCorpus），学习通用语言规律。</li>
<li><strong>有监督微调</strong>：针对下游任务（如分类、问答）添加任务适配层，调整模型参数。</li>
</ul>
</li>
<li>目标函数融合语言模型损失和任务损失，提升泛化性。</li>
</ul>
</li>
<li><strong>能力与局限</strong>：
<ul>
<li>在9项NLP任务上超越当时SOTA模型，证明预训练的有效性。</li>
<li>局限：单向上下文理解不足；微调需大量标注数据；生成文本长度和质量有限。</li>
</ul>
</li>
</ul>
<h2 id="GPT2-Language-Models-are-Unsupervised-Multitask-Learners">GPT2: Language Models are Unsupervised Multitask Learners</h2>
<p><strong>零样本学习与规模跃升</strong></p>
<ul>
<li><strong>技术架构</strong>：
<ul>
<li>沿用单向Transformer解码器，但<strong>深度增至48层</strong>，参数量达<strong>15亿</strong>（10倍于GPT-1）。</li>
<li>优化：<strong>层归一化位置前置</strong>（提升训练稳定性）；上下文窗口从512扩至1024词。</li>
</ul>
</li>
<li><strong>训练范式</strong>：
<ul>
<li><strong>纯无监督预训练</strong>：数据量增至40GB（WebText网页文本），覆盖更广泛的语言模式。</li>
<li><strong>零样本（Zero-Shot）学习</strong>：
<ul>
<li>取消微调阶段，直接通过<strong>任务描述+自然语言提示</strong>（如“翻译为中文：{文本}”）执行任务。</li>
<li>核心思想：大规模数据蕴含多任务模式，模型仅需理解提示即可泛化。</li>
</ul>
</li>
</ul>
</li>
<li><strong>能力与突破</strong>：
<ul>
<li>生成长文本的<strong>连贯性显著提升</strong>（人类评估可信度达83%）。</li>
<li>在7/8个语言任务中超越微调模型，验证“语言模型即多任务系统”的假设。</li>
<li>争议：因生成虚假信息风险，OpenAI分阶段开源模型。</li>
</ul>
</li>
</ul>
<h2 id="GPT3-Language-Models-are-Few-Shot-Learners">GPT3: Language Models are Few-Shot Learners</h2>
<p><strong>少样本学习与暴力美学的巅峰</strong></p>
<ul>
<li><strong>技术架构</strong>：
<ul>
<li>参数量<strong>1750亿</strong>（GPT-2的100倍），使用<strong>稀疏注意力机制</strong>（Sparse Transformer）降低计算复杂度。</li>
<li>模型深度增至96层，特征维度扩展至12,288。</li>
</ul>
</li>
<li><strong>训练范式</strong>：
<ul>
<li><strong>海量数据预训练</strong>：45TB混合数据（经严格过滤、去重），包括Common Crawl、维基百科等高质文本。</li>
<li><strong>上下文学习（In-Context Learning）</strong>：
<ul>
<li>通过<strong>Few-shot提示</strong>（任务描述+少量示例）激活模型内部知识，无需梯度更新。</li>
<li>三种模式：Zero-shot（无示例）、One-shot（1个示例）、Few-shot（多个示例）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>能力与突破</strong>：
<ul>
<li><strong>涌现能力（Emergent Abilities）</strong>：规模超临界值后，突现算术推理、代码生成等小模型不具备的能力。</li>
<li>生成文本与人类写作难以区分，支持跨领域任务（如翻译、编程、创作）。</li>
<li>局限性：逻辑推理仍依赖模式匹配；训练成本极高（约1200万美元）；数据时效性受限。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>gpt</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA系列模型</title>
    <url>/2024/08/03/LLaMA%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>LLaMA系列论文</p>
<span id="more"></span>
<h2 id="LLaMA1">LLaMA1</h2>
<p><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></p>
<h3 id="Data">Data</h3>
<h4 id="Mixture-of-data">Mixture of data</h4>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Sampling prop.</th>
<th>Epochs</th>
<th>Disk size</th>
</tr>
</thead>
<tbody>
<tr>
<td>CommonCrawl</td>
<td>67.0%</td>
<td>1.10</td>
<td>3.3 TB</td>
</tr>
<tr>
<td>C4</td>
<td>15.0%</td>
<td>1.06</td>
<td>783 GB</td>
</tr>
<tr>
<td>Github</td>
<td>4.5%</td>
<td>0.64</td>
<td>328 GB</td>
</tr>
<tr>
<td>Wikipedia</td>
<td>4.5%</td>
<td>2.45</td>
<td>83 GB</td>
</tr>
<tr>
<td>Books</td>
<td>4.5%</td>
<td>2.23</td>
<td>85 GB</td>
</tr>
<tr>
<td>ArXiv</td>
<td>2.5%</td>
<td>1.06</td>
<td>92 GB</td>
</tr>
<tr>
<td>StackExchange</td>
<td>2.0%</td>
<td>1.03</td>
<td>78 GB</td>
</tr>
</tbody>
</table>
<h3 id="Tokenizer">Tokenizer</h3>
<ul>
<li>
<p>bytepair encoding (BPE) algorithm, using the implementation from SentencePiece.</p>
</li>
<li>
<p>entire training dataset contains roughly 1.4T tokens after tokenization.</p>
</li>
<li>
<p>perform approximately two epochs.</p>
</li>
</ul>
<h3 id="Architecture">Architecture</h3>
<ol>
<li>
<p><strong>Pre-normalization [GPT3]:</strong> To improve the <strong>training stability</strong>, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the <strong>RMSNorm normalizing (Root Mean Square Layer Normalization)</strong> function, introduced by Zhang and Sennrich (2019).</p>
<blockquote>
<p>RMSNorm是对LayerNorm的一个改进，没有做re-center操作（移除了其中的均值项），可以看作LayerNorm在均值为0时的一个特例。论文通过实验证明，re-center操作不重要。</p>
<p>作者认为这种模式在简化了Layer Norm的同时，可以在各个模型上<strong>减少约 7%∼64% 的计算时间</strong></p>
</blockquote>
</li>
<li>
<p><strong>SwiGLU activation function [PaLM].</strong><br>
We use a dimension of  $\frac{2}{3} \times 4d$  instead of $4d$ as in PaLM.</p>
</li>
<li>
<p><strong>Rotary Embeddings [GPTNeo].</strong></p>
<p>寻找$f$，满足<br>
$$<br>
&lt;f(q,m), f(k,n)&gt; = g(q,k,m-n)<br>
$$</p>
<p>利用复数的旋转性质，可以得到Q、K与（m-n）相对位置的关系。</p>
<p>**具体实现过程：**对于 token 序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量，然后对每个 token 位置都计算对应的旋转位置编码，接着对每个 token 位置的 query 和 key 向量的元素按照 两两一组 应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。<br>
<strong>RoPE的优势：</strong></p>
<ul>
<li><strong>序列长度的灵活性</strong>：传统的位置嵌入通常需要定义最大序列长度，限制了它们的适应性。另一方面，RoPE 非常灵活。它可以为任意长度的序列即时生成位置嵌入。</li>
<li><strong>减少 token 间的依赖关系</strong>：RoPE 在对 token 之间的关系进行建模方面非常聪明。随着 token 在序列中彼此距离越来越远，RoPE 自然会减少它们之间的 token 依赖性。这种逐渐减弱的方式与人类理解语言的方式更加一致。</li>
<li><strong>增强的自注意力</strong>：RoPE 为线性自注意力机制配备了相对位置编码，这是传统绝对位置编码中不存在的功能。此增强功能允许更精确地利用 token 嵌入。</li>
</ul>
</li>
</ol>
<h2 id="LLaMA-2">LLaMA 2</h2>
<h3 id="Pretraining">Pretraining</h3>
<p><strong>vs LLaMA :</strong></p>
<ul>
<li>trained on 40% more total tokens</li>
<li>doubled the context length</li>
<li>used grouped-query attention (GQA) to improve inference scalability for our larger models.<br>
<img src="gqa.png" alt="GQA">
<ul>
<li>Multi-Head Attention：效果好、推理慢</li>
<li>Multi-Query Attention: 效果变差，推理快</li>
<li>Grouped Multi-Query Attention: 效果几乎无影响，推理速度较快</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>llm</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>llm</tag>
        <tag>llama</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习基础</title>
    <url>/2024/09/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>强化学习的<strong>终极目标</strong>是让一个智能体（Agent）通过与环境（Environment）的持续交互，学会一个<strong>最优策略（Policy）</strong>，从而获得<strong>最大化的累积奖励（Cumulative Reward）</strong>。</p>
<span id="more"></span>
<h2 id="基本概念">基本概念</h2>
<p><strong>1. 智能体 (Agent)</strong></p>
<ul>
<li>学习者或决策者。例如：下棋的AI程序、自动驾驶汽车、玩游戏的角色。</li>
</ul>
<p><strong>2. 环境 (Environment)</strong></p>
<ul>
<li>智能体所处的外部世界，智能体与之交互的一切。例如：棋盘、道路、游戏界面。</li>
</ul>
<p><strong>3. 状态 (State, S)</strong></p>
<ul>
<li>环境在某一时刻的特定情况或配置。例如：棋盘上所有棋子的位置、汽车当前的坐标和速度。</li>
<li><strong>注意</strong>：智能体有时无法看到完整状态，只能获得一个<strong>观测 (Observation)</strong>，这通常被称为部分可观测环境。</li>
</ul>
<p><strong>4. 动作 (Action, A)</strong></p>
<ul>
<li>智能体在某个状态下可以做出的行为。例如：移动一个棋子、踩油门或刹车、按“向左”键。</li>
<li>动作空间 (Action Space)：所有可能动作的集合。</li>
</ul>
<p><strong>5. 奖励 (Reward, R)</strong></p>
<ul>
<li>环境在智能体执行一个动作后，反馈给智能体的一个<strong>标量信号</strong>。这是智能体判断好坏的核心依据。</li>
<li>例如：赢得比赛（+100），吃掉豆子（+10），碰到敌人（-100）。</li>
<li><strong>设计奖励函数是强化学习成功的关键</strong>，它定义了智能体需要完成的任务。</li>
</ul>
<p><strong>6. 策略 (Policy, π)</strong></p>
<ul>
<li>智能体的<strong>行为函数</strong>，是状态到动作的映射。它决定了在某个状态下应该采取什么动作。</li>
<li>可以看作是智能体的大脑。策略可以是确定性的（<code>a = π(s)</code>），也可以是随机性的（<code>π(a|s)</code>表示在状态s下选择动作a的概率）。</li>
</ul>
<p><strong>7. 轨迹（trajectory）</strong>：有时也称为 <strong>episode</strong> 或 <strong>rollout</strong>，是一系列State -&gt; Action  -&gt; Reward的动作链。<br>
$$<br>
S_{1} \xrightarrow[r=0]{a_{2}} S_{2} \xrightarrow[r=0]{a_{3}} S_{5} \xrightarrow[r=0]{a_{3}} S_{8} \xrightarrow[r=1]{a_{2}} S_{9}<br>
$$<br>
<strong>8. 价值函数 (Value Function, V(s))</strong></p>
<ul>
<li>这是强化学习中最核心的概念之一。奖励代表的是<strong>即时好处</strong>，而价值函数代表的是<strong>长期好处</strong>。</li>
<li><strong>状态价值函数 V(s)</strong>：表示从状态 <code>s</code>开始，遵循某个策略 <code>π</code>所能获得的<strong>预期累积奖励</strong>。</li>
<li><strong>动作价值函数 Q(s, a)</strong>：表示在状态 <code>s</code>下执行动作 <code>a</code>后，再遵循策略 <code>π</code>所能获得的<strong>预期累积奖励</strong>。</li>
<li><strong>核心思想</strong>：一个动作可能带来很小的即时奖励，但能导向未来有巨大奖励的状态（高价值），那么这个动作总体上也是好的。例如，象棋中牺牲一个卒（负奖励）以获取局面优势（高价值状态）。</li>
</ul>
<p><strong>9. 模型 (Model)</strong></p>
<ul>
<li>智能体对环境动态变化规律的内部表示。模型预测环境下一步会变成什么样子。</li>
<li>例如：预测“在状态s下执行动作a，下一个状态s’是什么，以及会得到多少奖励”。</li>
<li>并非所有强化学习方法都需要模型。因此产生了两种主要方法：
<ul>
<li><strong>基于模型 (Model-based)</strong>：有环境模型，可以进行“想象”和规划。</li>
<li><strong>无模型 (Model-free)</strong>：没有环境模型，直接通过试错来学习策略或价值函数（如著名的Q-Learning, DQN）。</li>
</ul>
</li>
</ul>
<h2 id="贝尔曼公式">贝尔曼公式</h2>
<p>令$G_t$代表一个轨迹的Return，即为：<br>
$$<br>
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + …<br>
$$</p>
<h3 id="State-Value">State Value</h3>
<p>我们如何用一个值来衡量策略（policy）的好坏，答案就是State Value.</p>
<p>State value是从一个状态出发，得到的所有可能轨迹的平均Return，可以用State Value来指导优化所需要选用的策略。</p>
<p>那么State Value为：<br>
$$<br>
v_{\pi}(s) = E[G_t | S_t =s].<br>
$$</p>
<h3 id="Action-Value">Action Value</h3>
<p>Action value是指从一个state出发，选用某个Action所得到的平均Return，可以用来评估某个Action的好坏。</p>
<p>那么Action Value为：<br>
$$<br>
q_{\pi}(s,a) = E\left[G_{t}|S_{t} = s, A_{t}=a\right]<br>
$$</p>
<h3 id="State-value和Action-Value的联系：">State value和Action Value的联系：</h3>
<p>$$<br>
\underbrace{\mathbb{E}[G_{t}\mid S_{t}=s]}_{v_{\pi}(s)}=\sum_{a\in\mathcal{A}}\underbrace{\mathbb{E}[G_{t}\mid S_{t}=s,A_{t}=a]}_{q_{\pi}(s,a)}\pi(a\mid s).<br>
$$</p>
<p>即：<br>
$$<br>
v_{\pi}(s) = \sum_{a}\pi(a|s)q_{\pi}(s,a)<br>
$$</p>
<p><strong>如果知道action value，即可求解state value.</strong></p>
<h3 id="Bellman-Equation">Bellman Equation</h3>
<p>贝尔曼公式描述的是两个状态之间的state value之间的关系：</p>
<p>$G_t$和$G_{t+1}$的关系为：<br>
$$<br>
G_t = R_{t+1} + \gamma G_{t+1}<br>
$$<br>
那么有：<br>
$$<br>
\begin{aligned}<br>
v_{\pi}(s) &amp;= \mathbb{E}[G_{t} \vert S_{t} = s] \\<br>
&amp;= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \vert S_{t} = s] \\<br>
&amp;= \mathbb{E}[R_{t+1} \vert S_{t} = s] + \gamma \mathbb{E}[G_{t+1} \vert S_{t} = s].<br>
\end{aligned}<br>
$$</p>
<ul>
<li>第一项为状态$s$下的即时奖励期望。</li>
<li>第二项为状态$s$下的将来的奖励期望。</li>
</ul>
<h4 id="Elementwise-form">Elementwise form:</h4>
<p>$$<br>
\begin{aligned}<br>
v_{\pi}(s) &amp;= \sum_{a} {\pi}(a|s) \left[ \underbrace{\sum_{r}p(r|s,a)r + \gamma \sum_{s’}p(s’|s,a)v_{\pi}(s’)}_{q_{\pi}(s,a)}\right] \\<br>
&amp;= \sum_{a}\pi(a|s)q_{\pi}(s,a)<br>
\end{aligned}<br>
$$</p>
<p><strong>Action-value function:</strong><br>
$$<br>
q_{\pi}(s,a) = \sum_{r}p(r|s,a)r + \gamma \sum_{s’}p(s’|s,a)v_{\pi}(s’)<br>
$$</p>
<p><strong>如果知道state value，即可求解Action Value</strong></p>
<h4 id="Matrix-form">Matrix form:</h4>
<p>$$<br>
\begin{align*}<br>
v &amp;= r + \gamma Pv \\<br>
&amp;= r + \gamma v’<br>
\end{align*}<br>
$$<br>
其中$r$为即时奖励，$\gamma$为时间折扣，$P$为状态转移矩阵。</p>
<h2 id="贝尔曼最优公式">贝尔曼最优公式</h2>
<p>对于每个状态$S$​，策略$\pi^*$的State Value都大于其他任意策略$\pi$的State Value，即$\pi^*(s) \geq \pi(s)$,那么$\pi^*$即为最优策略。</p>
<p><strong>Elementwise Form of BOE(Bellman Optimality Equation)：</strong><br>
$$<br>
\begin{align*}<br>
v(s) &amp;= \max_{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a|s) \left( \sum_{r \in \mathcal{R}} p(r|s, a) r + \gamma \sum_{s’ \in \mathcal{S}} p(s’|s, a) v(s’) \right) \\<br>
&amp;= \max_{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a|s) q(s, a)<br>
\end{align*}<br>
$$<br>
其中$\pi(a|s)$为策略在某个状态下的动作概率，且$\pi(a|s)\geq0$，因此当$q(s,a)$取得最大值，采取动作$a$时，策略为最优策略。</p>
<h2 id="值迭代和策略迭代">值迭代和策略迭代</h2>
<h3 id="值迭代">值迭代</h3>
<p>假设值已知，求解所有可能Action的$q(s,a)$，通过最大的action value去更新策略，求解新的Value State。</p>
<p>两个步骤：</p>
<ol>
<li>
<p>策略更新：<br>
$$<br>
\pi_{k+1} = arg \underset{\pi}{max}(r_{\pi} + \gamma P_{\pi}v_{k})<br>
$$</p>
<p>$v_{k}$是已知的（迭代开始时可以随意赋值）</p>
<p><img src="vi1.png" alt="计算$q(s,a)$"></p>
</li>
<li>
<p>价值更新：选定策略后，更新state value<br>
$$<br>
v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_{k}<br>
$$</p>
<p><img src="vi2.png" alt="更新state value"></p>
<p><img src="vi3.png" alt="多次迭代"></p>
</li>
</ol>
<h3 id="策略迭代">策略迭代</h3>
<ol>
<li>
<p>策略评估：<br>
$$<br>
v_{\pi_{k}} = r_{\pi_{k}} + \gamma P_{\pi_{k}}v_{\pi_{k}}<br>
$$<br>
策略$\pi_{k}$是已知的（随机选择策略），求解该策略的state value时，采用迭代的方法求解。</p>
<p><img src="pi1.png" alt="随机选择策略"></p>
<p><img src="pi2.png" alt="评估策略价值"></p>
</li>
<li>
<p>策略更新：<br>
$$<br>
\pi_{k+1} = arg \underset{\pi}{max}(r_{\pi} + \gamma P_{\pi}v_{\pi_{k}})<br>
$$<br>
<img src="pi3.png" alt="更新策略"></p>
</li>
</ol>
<h3 id="截断的策略迭代">截断的策略迭代</h3>
<p>对比值迭代和策略迭代的步骤如下：在两个迭代算法中，假设第一次state value的值一致，那么可以同时更新相同的Action，再根据Action估计State Value时，两者差异有所体现（第4步的Value 求解），值迭代仅仅迭代一次，而策略迭代需要迭代无穷次才能求得最终的State Value($v_{\pi1}$)</p>
<p><img src="compare.png" alt="值迭代和策略迭代"></p>
<p><img src="compare2.png" alt="值迭代和策略迭代"></p>
<p>策略迭代方法中，策略评估过程中，需要无穷步的迭代才能评估出策略价值，现实过程中无法实现，因此推出基于截断的策略迭代方法。</p>
<p>当截断步骤设置为1时，等同于值迭代。步骤为无穷时，等同于策略迭代。</p>
<h3 id="三种迭代算法的收敛速度">三种迭代算法的收敛速度</h3>
<p><img src="compare3.png" alt="迭代算法的收敛速度对比"></p>
<h2 id="Monte-Carlo-Method">Monte Carlo Method</h2>
<p>值迭代和策略迭代是一个model-based的方法，这个model具体指的是马尔科夫决策过程（MDP）的核心元素：</p>
<ol>
<li><strong>状态转移概率 $P(s′∣s,a)$</strong>：在状态 <em>s</em>下执行动作 <em>a</em>后，环境转移到状态 <em>s</em>’的概率是多少。</li>
<li><strong>奖励函数 $R(s,a,s′)$</strong>：在状态 <em>s</em>下执行动作 <em>a</em>并转移到状态 s’ 后，能获得的即时奖励是多少。</li>
</ol>
<h3 id="MC-Basic">MC Basic</h3>
<p>主要就是将<strong>策略迭代</strong>算法变成无模型方法。</p>
<p>策略迭代算法中，第一步是需要评估策略价值，第二步选择$q(s,a)$最大的策略作为更新策略。</p>
<p>在下面的表达式中，我们不依赖于模型计算动作价值：<br>
$$<br>
q_{\pi_{k}}(s|a) = E[G_{t}| S_{t}=s, A_{t} = a]<br>
$$<br>
因此，我们可以利用Monte Carlo方法来计算动作价值：</p>
<ul>
<li>
<p>从状态和动作$(s,a)$出发，在初始策略$\pi_{0}$下，生成一个轨迹。</p>
</li>
<li>
<p>计算该轨迹的价值：</p>
<p>$$<br>
q_{\pi_{k}}(s|a) = E[G_{t}| S_{t}=s, A_{t} = a]<br>
$$</p>
</li>
<li>
<p>采样多条轨迹，然后求期望</p>
<p>$$<br>
q_{\pi_{k}}(s|a) = E[G_{t}| S_{t}=s, A_{t} = a] \approx \frac{1}{N}\sum_{i=1}^{N}g^{i}(s,a)<br>
$$</p>
</li>
</ul>
<p><img src="mc_basic.png" alt="MC Basic algorithm"></p>
<p><strong>缺点：</strong></p>
<ul>
<li>效率比较低，需要多次采样大量轨迹</li>
<li>轨迹长度需要足够长，否则距离较远的状态没有价值</li>
<li>没有充分利用采样轨迹信息</li>
</ul>
<h3 id="MC-Exploring-Starts">MC Exploring Starts</h3>
<p>每条轨迹中包含了以其他状态和动作为起点的轨迹，该方法对这些数据进行了利用，提高了效率。</p>
<ul>
<li>方法一：收集完所有轨迹后，便利所有state-action pair，计算平均价值，该方法需要等所有的轨迹收集完，效率低！</li>
<li>方法二：每收集完一个轨迹，立即计算action value，更新策略。（可以收敛）</li>
</ul>
<h3 id="MC-varepsilon-Greedy">MC  $\varepsilon$-Greedy</h3>
<p><strong>$\varepsilon$-greedy policy:</strong><br>
$$<br>
\pi(a|s)=<br>
\begin{cases}<br>
1-\frac{\varepsilon}{\vert\mathcal{A}(s)\vert}(\vert\mathcal{A}(s)\vert - 1) &amp;\text{ for the greedy actions.}<br>
\\<br>
\frac{\varepsilon}{\vert\mathcal{A}(s)\vert} &amp;\text{ for the other }\vert\mathcal{A}(s)\vert - 1\text{ actions.}<br>
\end{cases}<br>
$$<br>
Where $\varepsilon \in [0,1]$ and $\vert\mathcal{A}\vert$ is the number of actions.</p>
<ul>
<li>该方法保证了greedy action总是比其他的动作概率大</li>
<li>$\varepsilon$-greedy平衡了利用和探索（exploitaion and exploration）</li>
</ul>
<p><strong>MC  $\varepsilon$-Greedy</strong> 在MC RL algorithm中使用时，在估计完action value之后，需要根据最大的Action Value来更新策略，而MC Exploring Starts是贪婪策略，直接选择action value最大的策略进行更新，但该方法需要添加一些随机更新策略进来：<br>
$$<br>
\pi_{k+1}(a|s)=<br>
\begin{cases}<br>
1-\frac{|\mathcal{A}(s)|-1}{|\mathcal{A}(s)|}\varepsilon, &amp;a=a_{k}^{*},<br>
\\<br>
\frac{1}{|\mathcal{A}(s)|}\varepsilon, &amp;a\neq a_{k}^{*}.<br>
\end{cases}<br>
$$</p>
<p>使用$\varepsilon$-greedy policy进行采样的话，$\varepsilon$的值不能设置太大，否则最终得到的策略与greedy策略得到的最有策略不一致。</p>
<h2 id="随机近似与梯度下降">随机近似与梯度下降</h2>
<p>Stochastic Approximation and Stochastic Gradient Descent.</p>
<h3 id="RM-Robbins-Monro-算法">RM(Robbins-Monro)算法</h3>
<p>解决$g(w)=0$的RM算法如下：<br>
$$<br>
\begin{equation}<br>
w_{k+1}=w_{k}-a_{k}\tilde{g}(w_{k},\eta_{k}),\qquad k=1,2,3,\ldots<br>
\end{equation}<br>
$$<br>
其中$w_k$是根的第$k$次估计，$\tilde{g}(w_{k},\eta_{k})$是第$k$次带噪观测值，$a_k$是正的系数。</p>
<p>此算法不需要知道公式，只需要知道输入和输出。</p>
]]></content>
      <categories>
        <category>RL</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>rl</tag>
        <tag>Monte Carlo</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型学习笔记</title>
    <url>/2024/08/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>记录大模型学习过程中的问题。</p>
<span id="more"></span>
<h2 id="Pre-LayerNorm和Post-LayerNorm的区别？">Pre-LayerNorm和Post-LayerNorm的区别？</h2>
<h3 id="结构定义与公式对比："><strong>结构定义与公式对比：</strong></h3>
<ol>
<li>
<p><strong>后置归一化（Post-LN）</strong></p>
<ul>
<li>
<p><strong>结构</strong>：子层（自注意力/前馈网络）输出后，先与残差连接相加，再进行LayerNorm。</p>
</li>
<li>
<p><strong>公式</strong>：<br>
$$<br>
x_{out}=LayerNorm(x+Sublayer(x))<br>
$$</p>
</li>
<li>
<p><strong>特点</strong>：原始Transformer论文采用的设计（如BERT）。</p>
</li>
</ul>
</li>
<li>
<p><strong>前置归一化（Pre-LN）</strong></p>
<ul>
<li>
<p><strong>结构</strong>：输入先通过LayerNorm，再进入子层计算，结果与原始输入残差连接。</p>
</li>
<li>
<p><strong>公式</strong>：<br>
$$<br>
x_{out}=x+Sublayer(LayerNorm(x))<br>
$$</p>
</li>
</ul>
</li>
<li></li>
</ol>
<pre><code> - **特点**：GPT系列、LLaMA、T5等现代大模型的主流选择。
</code></pre>
<h3 id="核心差异与影响"><strong>核心差异与影响</strong></h3>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>维度</strong></th>
<th style="text-align:center"><strong>Pre-LN（前置）</strong></th>
<th style="text-align:center"><strong>Post-LN（后置）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>梯度稳定性</strong></td>
<td style="text-align:center">深层梯度衰减平缓（$O(1/\sqrt{L})$)，不易消失/爆炸</td>
<td style="text-align:center">低层梯度指数衰减（$O((1/2)^{(N-k)/2}$），易消失</td>
</tr>
<tr>
<td style="text-align:center"><strong>训练收敛</strong></td>
<td style="text-align:center">无需学习率预热（Warm-up），收敛更快</td>
<td style="text-align:center">需精细调整学习率和Warm-up，否则易震荡或不收敛</td>
</tr>
<tr>
<td style="text-align:center"><strong>模型深度适应性</strong></td>
<td style="text-align:center">支持深层网络（&gt;12层），可扩展至百层</td>
<td style="text-align:center">仅适用于浅层网络（≤6层），深层训练易崩溃</td>
</tr>
<tr>
<td style="text-align:center"><strong>表达能力</strong></td>
<td style="text-align:center">高层可能表征趋同（Representation Collapse）</td>
<td style="text-align:center">浅层模型泛化能力更强，输出多样性更高</td>
</tr>
<tr>
<td style="text-align:center"><strong>实际应用案例</strong></td>
<td style="text-align:center">GPT-3/4、LLaMA、PaLM、Qwen 等主流大模型</td>
<td style="text-align:center">原始Transformer、BERT（浅层微调场景）</td>
</tr>
</tbody>
</table>
<h2 id="为什么相对位置编码比绝对位置编码好？">为什么相对位置编码比绝对位置编码好？</h2>
<p><strong>相对位置编码更好地契合了语言的本质，并解决了绝对位置编码在长文本上的根本性缺陷。</strong></p>
<h3 id="语言的本质：关系驱动，而非位置驱动">语言的本质：关系驱动，而非位置驱动</h3>
<p>“美味的苹果” 和 “苹果公司很成功”</p>
<p><strong>绝对位置视角</strong>：第一个“苹果”是第2个词，第二个“苹果”是第1个词。这个“第几个”的信息本身，对于理解“苹果”的含义几乎毫无帮助。</p>
<h3 id="绝对位置编码的核心缺陷">绝对位置编码的核心缺陷</h3>
<ul>
<li><strong>长度外推（Length Extrapolation）能力差</strong>：模型在训练时见过的序列长度是有限的（例如512个token）。当你试图处理一个更长的句子（例如1000个token）时，问题就来了。</li>
<li><strong>可能引入无关的偏差</strong>：绝对位置编码可能会让模型过度关注绝对位置本身</li>
</ul>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>llm</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2024/12/30/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>激活函数介绍。</p>
<span id="more"></span>
<p><img src="activation.png" alt="Activation"></p>
<h2 id="ReLU">ReLU</h2>
<p>$$<br>
ReLU(x) = max(0, x)<br>
$$</p>
<h2 id="GELU（Gaussian-Error-Linear-Unit）">GELU（Gaussian Error Linear Unit）</h2>
<p>$$<br>
GELU(X) = x P(X &lt;= x)  = x \times \Phi(x)<br>
$$<br>
其中$\Phi(x)$表示高斯分布的累积概率分布，即在$(-∞,x]$区间对高斯分布的定积分。<br>
$$<br>
\Phi(x)=\int_{-\infty}^{x} \frac{e^{-t^{2} / 2}}{\sqrt{2 \pi}}  d t=\frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x}{\sqrt{2}}\right)\right] \tag{2}<br>
$$<br>
其中$\Phi(x)$是标准正态分布的累积分布函数，<br>
$$<br>
\operatorname{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^{2}}  dt \tag{3}<br>
$$</p>
<p><img src="gaussian.png" alt="Gaussian"></p>
<p>当方差为无穷大，均值为0的时候，GeLU就等价于ReLU了。GELU可以当作为RELU的一种平滑策略。</p>
<p>GELU函数的导数是连续的，这使得在训练深度神经网络时可以更容易地传播梯度，避免了ReLU函数在$x=0$处的导数不连续的问题，从而减少了训练过程中出现的梯度消失问题。</p>
<p>GELU函数在激活函数的非线性变换中引入了类似于sigmoid函数的变换，这使得GELU函数的输出可以落在一个更广的范围内，有助于加速模型的收敛速度。</p>
<h2 id="Swish">Swish</h2>
<p>$$<br>
Swish(x) =x * sigmoid(\beta x)<br>
$$<br>
ß 为可学习参数。Swish可以比ReLU激活函数更好，因为它在0附近提供了更平滑的转换，这可以带来更好的优化。</p>
<h2 id="GLU">GLU</h2>
<p>$$<br>
GLU(x) = sigmoid(W_1 x + b)\otimes(Vx+c)<br>
$$<br>
GLU可以有效地捕获序列中的远程依赖关系，同时避免与lstm和gru等其他门控机制相关的一些梯度消失问题。</p>
<h2 id="SwiGLU">SwiGLU</h2>
<p>$$<br>
SwiGLU(x) = Swish(W_1 x + b)\otimes(Vx+c)<br>
$$<br>
SwiGLU是一个GLU，但不是将sigmoid作为激活函数，而是使用ß=1的swish</p>
<ul>
<li>Swish对于负值的响应相对较小克服了 ReLU 某些神经元上输出始终为零的缺点</li>
<li>GLU 的门控特性，这意味着它可以根据输入的情况决定哪些信息应该通过、哪些信息应该被过滤。这种机制可以使网络更有效地学习到有用的表示，有助于提高模型的泛化能力。</li>
<li>计算效率相比某些较复杂的激活函数（如 GELU）更高，同时仍能保持较好的性能。</li>
</ul>
<p><img src="swiglu.png" alt="SwiGLU"></p>
<p>当β趋近于0时，Swish函数趋近于线性函数$y = x^2$  ；当β 趋近于无穷大时，Swish函数趋近于ReLU函数；当β 取值为1时，Swish函数是光滑且非单调的。</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>activation</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-LM</title>
    <url>/2025/04/29/Megatron-LM/</url>
    <content><![CDATA[<p>Megatron-LM 是一个基于 <strong>Megatron-Core</strong> 实现高效大模型训练的框架。</p>
<p>Megatron-core是一个<strong>GPU 优化的训练技术库</strong>，专注于提升大模型训练的性能和效率。</p>
<span id="more"></span>
<h1>Todo list:</h1>
<h2 id="数据处理：">数据处理：</h2>
<ul>
<li>
<p>Indexed_dataset是干什么的？</p>
</li>
<li>
<p>apex的作用是什么？</p>
</li>
</ul>
<h1>背景</h1>
<ul>
<li><strong>内存限制的挑战：</strong> 随着模型规模的增大，现代处理器的内存限制成为瓶颈。例如，单个 GPU 的内存通常无法容纳数十亿参数的模型，因此需要额外的内存管理技术，如激活检查点。</li>
<li><strong>现有方法的局限性：</strong> 现有的模型并行方法（如 GPipe 和 Mesh-TensorFlow）需要重写模型，并依赖于仍在开发中的自定义编译器和框架。</li>
</ul>
<p>通过在 512 个 GPU 上训练高达 83 亿参数的 Transformer 模型，Megatron-LM 实现了 15.1 PetaFLOPs 的计算效率，相比单 GPU 基线有 76% 的扩展效率。</p>
<h1>Model Parallel Transformers</h1>
<h1>其他信息</h1>
<blockquote>
<p>We show that the existing BERT architecture results in model degradation as the size increases. We overcome this challenge by rearranging the layer normalization and residual connection in the transformer layers and show that with this change, results for the downstream tasks on development sets improve monotonically as the model size increases.</p>
</blockquote>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="http://arxiv.org/abs/1909.08053">http://arxiv.org/abs/1909.08053</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>llm</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>model parallel</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSeek技术报告</title>
    <url>/2025/02/20/DeepSeek%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<p>DeepSeek相关技术报告</p>
<span id="more"></span>
<h1>DeepSeek V2</h1>
<h2 id="Architecture">Architecture</h2>
<h3 id="Multi-head-Latent-Attention">Multi-head Latent Attention</h3>
<h3 id="Decoupled-Rope（-font-color-red-todo-font-）">Decoupled Rope（<font color='red'> todo </font>）</h3>
<p>为了解决原始 RoPE 与低秩压缩不兼容的问题，DeepSeek-V2 提出了 Decoupled RoPE（解耦旋转位置编码）。其核心思想是：</p>
<ul>
<li>**解耦 Query 和 Key 的位置编码：**将 Query 和 Key 分为两部分：
<ol>
<li>压缩部分：用于低秩压缩，不包含位置信息。</li>
<li>位置编码部分：专门用于存储位置信息，不参与压缩。</li>
</ol>
</li>
<li>具体实现：
<ol>
<li>引入额外的 Query 和 Key 向量（ $q_t^R$ 和 $k_t^R$ ），专门存储位置信息。</li>
<li>使用独立的权重矩阵 $W^{QR} $和$ W^{KR}$ 将输入投影到位置编码部分。</li>
<li>对位置编码部分应用 RoPE，而压缩部分保持不变。</li>
<li>将压缩部分和位置编码部分拼接起来，形成完整的 Query 和 Key。</li>
</ol>
</li>
</ul>
<h3 id="DeepSeekMoE">DeepSeekMoE</h3>
]]></content>
      <categories>
        <category>llm</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>deepseek</tag>
        <tag>MLA</tag>
        <tag>Decoupled Rope</tag>
        <tag>position encoding</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepSpeed</title>
    <url>/2025/03/05/DeepSpeed/</url>
    <content><![CDATA[<p>Deepspeed训练大模型细节</p>
<span id="more"></span>
<h2 id="MMAP">MMAP</h2>
<p><strong>设计目的：</strong></p>
<ul>
<li>
<p><strong>高效I/O性能</strong>：通过内存映射技术，数据在训练时可按需读取到显存，减少内存占用并避免重复加载开销。</p>
</li>
<li>
<p><strong>分布式训练支持</strong>：在多GPU或多节点场景下，MMAP文件支持并行访问，不同GPU可独立读取数据的不同部分，提升数据吞吐量。</p>
</li>
<li>
<p><strong>兼容性与扩展性</strong>：适用于不同分词器（如BPE、SentencePiece）生成的Token化结果，并能处理动态增长的训练数据。</p>
</li>
</ul>
<p><strong>数据格式：</strong></p>
<ul>
<li><strong>二进制编码</strong>：文本数据经过分词（Tokenization）后，将每个Token转换为固定长度的整数（如int32或int64），并按顺序存储为二进制流。</li>
<li><strong>索引文件</strong>：通常伴随一个索引文件（如<code>idx</code>），记录每个数据块的起始位置和长度，便于快速随机访问或分片加载。</li>
<li><strong>分块存储</strong>：大型数据集会被切分为多个MMAP文件块（例如每块1GB），便于分布式训练时按需加载。</li>
</ul>
<h2 id="ZeRO-Zero-Redundancy-Optimizer">ZeRO(Zero Redundancy Optimizer)</h2>
<ul>
<li>a memory efficient form of data parallelism</li>
<li>leverage CPU and NVMe memory</li>
<li></li>
</ul>
<h2 id="3D-Parallelism">3D Parallelism</h2>
<ul>
<li>tensor-slicing, pipeline-parallelism, and data parallelism</li>
<li></li>
</ul>
<h2 id="ToDo">ToDo</h2>
<ul>
<li>什么时候用Zero，什么时候用3D parallism</li>
<li>3D parallism的使用：<a href="https://github.com/deepspeedai/megatron-deepspeed">https://github.com/deepspeedai/megatron-deepspeed</a></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>todo</tag>
      </tags>
  </entry>
  <entry>
    <title>熵、交叉熵、KL散度</title>
    <url>/2024/09/23/%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81KL%E6%95%A3%E5%BA%A6/</url>
    <content><![CDATA[<p>熵通常用来评价随机变量的不确定性；交叉熵是评价两个概率分布之间的差异；KL散度是评价两个概率分布之间的相对熵差。</p>
<span id="more"></span>
<h1>熵</h1>
<p>上世纪，人们用电报来传输信息，但电报里的字越多，花费的钱越多。为了节约成本，我们需要尽可能的缩短电报内容长度。</p>
<p>在天气预报场景中，如果我们有2种天气（好、坏），我们可以用过一位的二进制编码来表示，如果有4中天气（晴天、刮风、下雨、下雪），我们可以用2位编码来表示，即总天气数取对数$log_2N$即可。</p>
<p><strong>熵与平均编码长度</strong></p>
<p>如果我们知道每种天气的出现概率，那么就可以计算出编码的平均长度。<br>
$$<br>
L_N = p(x) * log_2{N}<br>
$$<br>
一般情况下，如果某种天气出现的概率是$P$，对应平均分布概率种类数为$log_2{\frac{1}{P}} = -log_2P$，因此，当这种天气的编码长度是$−log_2P$时，整个编码方案是最优的，此时最有平均编码长度为：<br>
$$<br>
H(P) = -\sum_iP(i)log_2P(i)<br>
$$</p>
<blockquote>
<ul>
<li>在这个公式里，由于被累加的情况是离散的，我们使用了求和符号。对于连续的情况，要把求和变成求积分。</li>
<li>上述解释为理想情况下的情况，$-log_2{P}$可能不为整数，而离散种类数必须为整数，因此“编码长度”为较为抽象的概念。</li>
</ul>
</blockquote>
<p>最后，再对熵的公式做一个补充。一般情况下，算熵时，对数的底不是2，而是自然常数e。由换底公式<br>
$$<br>
log_2x = \frac{log_ex}{log_e2}<br>
$$<br>
可知，对数用不同的底只是差了一个常数系数而已，使用什么数为底并不影响其相对大小。在使用熵时，我们也只关心多个熵的相对大小，而不怎么会关注绝对数值。</p>
<blockquote>
<p>以2为底时，熵的单位是比特。以e为底时，熵的单位是奈特（nat）。</p>
</blockquote>
<h1>交叉熵</h1>
<p>假设电报员正在和气象中心商量天气的编码方式。可是，这个电报员刚来这个城市不久，不知道这里是晴天比较多，还是下雨比较多。于是，对于晴天、刮风、下雨、下雪这四种天气，他采用了最朴素的平均法，让每种天气的编码都占2位。</p>
<p>大概100天后，电报员统计出了每种天气的出现频率。他猛然发现，这个城市大部分时间都在下雨。如果让下雨的编码只占1位，会节省得多。于是，他连忙修改了编码的方式。</p>
<p>这时，他开始后悔了：“要是早点用新的编码就好了。两种方案的平均编码长度差了多少呢？”</p>
<p>假设真实天气的概率分布为$P$，该电报员估计的概率分布为$Q$，这种较差的编码的平均编码长度为：<br>
$$<br>
H(P,Q) = -\sum_iP(i)log_2Q(i)<br>
$$<br>
上式即为<strong>交叉熵</strong>的公式，用来评估编码方案的优劣。</p>
<p>交叉熵有一个很重要的性质：交叉熵一定不小于熵。熵是最优的编码长度，你估计出来的编码方案，一定不会比最优的更好。所有交叉熵的应用基本上都是利用了这一性质。</p>
<p>在机器学习中，我们会为分类任务使用交叉熵作为损失函数。这正是利用了交叉熵不比熵更大这一性质。模型输出的概率分布就是交叉熵公式里的$Q$，实际的概率分布（one-hot编码）就是交叉熵公式里的$P$。由于熵是0（样本的概率一定），交叉熵的值一定大于等于0。因此，交叉熵的值可以表示它和熵——最优的概率分布的信息量——之间的差距。</p>
<h1>KL散度</h1>
<p>用交叉熵算出旧方案的平均编码长度后，电报员打算统计一下旧编码浪费了多少编码量。既然熵是最优编码的编码长度，那么交叉熵减去熵就能反映出旧编码平均浪费了多少编码长度。这个计算过程如下：<br>
$$<br>
\begin{align}<br>
D_{KL}(P||Q) &amp;= H(P,Q) - H(P) \\<br>
&amp;=-\sum_iP(i)log_2Q(i) - (-\sum_iP(i)log_2P(i))\\<br>
&amp;= \sum_{i}P(i)log_2\frac{P(i)}{Q(i)}<br>
\end{align}<br>
$$<br>
这个公式描述的量叫做<strong>相对熵</strong>，又叫做<strong>KL散度</strong>。KL散度的定义非常易懂，它只不过是交叉熵和熵的差而已，反映了一个分布与另一个分布的差异程度。最理想情况下，$P=Q$，则KL散度为0。</p>
<p>KL散度不是一个距离指标。从公式中能够看出，$D_{KL}(P||Q)≠ D_{KL}(Q||P)$，这个指标并不满足交换律。</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>熵</tag>
        <tag>交叉熵</tag>
        <tag>KL散度</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式并行训练</title>
    <url>/2025/03/05/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>为了解决算力增速不足的问题，人们考虑用多节点集群进行分布式训练，以提升算力，分布式训练势在必行。</p>
<span id="more"></span>
<h1>并行训练的评价指标</h1>
<h2 id="FLOPs-GPU-利用率"><strong>FLOPs/GPU 利用率</strong></h2>
<p>关注计算性能是否充分发挥：</p>
<ul>
<li><strong>FLOPs（浮点运算次数）</strong>：计算模型训练过程中执行的浮点运算次数，通常以 TFLOPs（TeraFLOPs）或 PFLOPs（PetaFLOPs）衡量。</li>
<li><strong>FLOPs 利用率（FLOPs Utilization）</strong>：实际执行的 FLOPs 与硬件理论峰值 FLOPs 的比值，衡量计算资源利用率。</li>
<li><strong>GPU 利用率（GPU Utilization）</strong>：统计 GPU 计算核心的使用情况，反映 GPU 是否被充分利用。</li>
</ul>
<h2 id="吞吐量与并行效率"><strong>吞吐量与并行效率</strong></h2>
<p>关注扩展性是否合理：</p>
<ul>
<li><strong>吞吐量（Throughput）</strong>：每秒处理的样本数（samples/sec）或 tokens/sec，通常用于衡量训练速度。</li>
<li><strong>加速比（Speedup）</strong>：$ S(N) = \frac{T(1)}{T(N)} $其中$ T(1)$ 是单 GPU 训练时间，$T(N)$ 是 N 个 GPU 训练时间。</li>
<li><strong>并行效率（Scaling Efficiency）</strong>:$ E(N) = \frac{S(N)}{N} = \frac{T(1)}{N \cdot T(N)} $.衡量并行扩展性，理想情况下应接近 100%。</li>
</ul>
<blockquote>
<p>Megatron-LM的实验中：</p>
<p>We sustain 15.1 PetaFLOPs with 512 V100 GPUS across the entire application with <strong>76% scaling efficiency</strong> when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs.</p>
<p>计算过程：</p>
<p>1 PetaFLOPs = 1000 TeraFLOPs</p>
<p>39 * 512 =19,968 TeraFLOPS</p>
<p>15.1 / 20.0 = 75.5 %</p>
</blockquote>
<h1>并行方法</h1>
<h2 id="数据并行">数据并行</h2>
<p><img src="matmul_data_paralelism.png" alt="数据并行"></p>
<p><strong>运行原理：</strong></p>
<ul>
<li>所谓的数据并行，就是将数据 x 进行切分，而每个设备上的模型 w 是完整的、一致的。</li>
<li>数据并行策略下，在反向传播过程中，需要对各个设备上的梯度进行 <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce">AllReduce</a>，以确保各个设备上的模型始终保持一致。</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li>当数据集较大，模型较小时</li>
</ul>
<blockquote>
<p>注意：If your model contains any <code>BatchNorm</code> layers, it needs to be converted to <code>SyncBatchNorm</code> to sync the running stats of <code>BatchNorm</code> layers across replicas.</p>
</blockquote>
<p><strong>训练步骤：</strong></p>
<ol>
<li>
<p>创建进程组：</p>
<ul>
<li>
<p>给每个进程设置cuda id</p>
</li>
<li>
<p>Init_process_progress</p>
<ul>
<li>
<p><strong>初始化分布式通信环境</strong></p>
<ul>
<li>
<p><strong>建立进程组（Process Group）</strong>：为多个进程（如不同 GPU/机器上的训练进程）创建通信组，使它们能够相互通信。</p>
</li>
<li>
<p><strong>设置通信后端</strong>：支持多种后端：</p>
<ul>
<li>
<p><code>nccl</code>：NVIDIA GPU 最佳选择，高性能。</p>
</li>
<li>
<p><code>gloo</code>：CPU 或 GPU（跨平台支持）。</p>
</li>
<li>
<p><code>mpi</code>：需 MPI 环境支持。</p>
</li>
<li>
<p>后端决定了通信效率和硬件兼容性。</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>定义进程拓扑</strong></p>
<ul>
<li><strong>指定全局进程数（<code>world_size</code>）</strong>：设置参与训练的总进程数（如 GPU 总数）。</li>
<li><strong>分配唯一标识（<code>rank</code>）</strong>：为当前进程分配全局唯一 ID（<code>0</code>到 <code>world_size-1</code>），<code>rank=0</code>通常为主节点。</li>
</ul>
</li>
<li>
<p><strong>配置通信方式（<code>init_method</code>）</strong></p>
</li>
<li>
<p><strong>环境变量初始化（<code>env://</code>）</strong>：通过环境变量 <code>MASTER_ADDR</code>和 <code>MASTER_PORT</code>指定主节点地址和端口（最常用）。</p>
</li>
<li>
<p><strong>TCP 初始化</strong>：直接指定主节点 IP 和端口（<code>tcp://10.1.1.20:23456</code>）。</p>
</li>
<li>
<p><strong>共享文件初始化</strong>：通过共享文件系统同步（<code>file:///path/to/shared_file</code>）。</p>
</li>
<li>
<p><strong>协调进程启动</strong></p>
<ul>
<li><strong>同步所有进程</strong>：确保所有进程完成初始化后再继续执行（避免死锁）。</li>
<li><strong>建立通信链路</strong>：例如，使用 <code>nccl</code>时为 GPU 建立高速通信通道。</li>
</ul>
</li>
<li>
<p><strong>准备核心通信操作</strong></p>
<ul>
<li><code>dist.all_reduce()</code>：全局求和等规约操作。</li>
<li><code>dist.broadcast()</code>：主节点向其他节点广播数据。</li>
<li><code>dist.barrier()</code>：进程同步等待。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>创建ddp模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="variable language_">self</span>.model = DDP(model, device_ids=[gpu_id])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建数据分发器</p>
<ul>
<li>
<p><a href="https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler">DistributedSampler</a> chunks the input data across all distributed processes.</p>
</li>
<li>
<p>The <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a> combines a dataset and a sampler, and provides an iterable over the given dataset.</p>
</li>
<li>
<p>Each process will receive an input batch of 32 samples; the effective batch size is <code>32 * nprocs</code>, or 128 when using 4 GPUs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = torch.utils.data.DataLoader(</span><br><span class="line">    dataset=train_dataset,</span><br><span class="line">    batch_size=<span class="number">32</span>,</span><br><span class="line">    shuffle=<span class="literal">False</span>,  <span class="comment"># We don&#x27;t shuffle</span></span><br><span class="line">    sampler=DistributedSampler(train_dataset), <span class="comment"># Use the Distributed Sampler here.</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Calling the <code>set_epoch()</code> method on the <code>DistributedSampler</code> at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_run_epoch</span>(<span class="params">self, epoch</span>):</span><br><span class="line">    b_sz = <span class="built_in">len</span>(<span class="built_in">next</span>(<span class="built_in">iter</span>(<span class="variable language_">self</span>.train_data))[<span class="number">0</span>])</span><br><span class="line">    <span class="variable language_">self</span>.train_data.sampler.set_epoch(epoch)   <span class="comment"># call this additional line at every epoch</span></span><br><span class="line">    <span class="keyword">for</span> source, targets <span class="keyword">in</span> <span class="variable language_">self</span>.train_data:</span><br><span class="line">      ...</span><br><span class="line">      <span class="variable language_">self</span>._run_batch(source, targets)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>保存模型</p>
<p>We only need to save model checkpoints from one process. Without this condition, each process would save its copy of the identical mode.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- ckp = <span class="variable language_">self</span>.model.state_dict()</span><br><span class="line">+ ckp = <span class="variable language_">self</span>.model.module.state_dict()</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">- <span class="keyword">if</span> epoch % <span class="variable language_">self</span>.save_every == <span class="number">0</span>:</span><br><span class="line">+ <span class="keyword">if</span> <span class="variable language_">self</span>.gpu_id == <span class="number">0</span> <span class="keyword">and</span> epoch % <span class="variable language_">self</span>.save_every == <span class="number">0</span>:</span><br><span class="line">  <span class="variable language_">self</span>._save_checkpoint(epoch)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行分布式训练</p>
<ul>
<li>
<p>Include new arguments <code>rank</code> (replacing <code>device</code>) and <code>world_size</code>.</p>
<ul>
<li><code>rank</code> is auto-allocated by DDP when calling <a href="https://pytorch.org/docs/stable/multiprocessing.html#spawning-subprocesses">mp.spawn</a>.</li>
<li><code>world_size</code> is the number of processes across the training job. For GPU training, this corresponds to the number of GPUs in use, and each process works on a dedicated GPU.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">device, total_epochs, save_every</span>):</span><br><span class="line">+ <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">rank, world_size, total_epochs, save_every</span>):</span><br><span class="line">+  ddp_setup(rank, world_size)</span><br><span class="line">   dataset, model, optimizer = load_train_objs()</span><br><span class="line">   train_data = prepare_dataloader(dataset, batch_size=<span class="number">32</span>)</span><br><span class="line">-  trainer = Trainer(model, train_data, optimizer, device, save_every)</span><br><span class="line">+  trainer = Trainer(model, train_data, optimizer, rank, save_every)</span><br><span class="line">   trainer.train(total_epochs)</span><br><span class="line">+  destroy_process_group()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">   <span class="keyword">import</span> sys</span><br><span class="line">   total_epochs = <span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">   save_every = <span class="built_in">int</span>(sys.argv[<span class="number">2</span>])</span><br><span class="line">-  device = <span class="number">0</span>      <span class="comment"># shorthand for cuda:0</span></span><br><span class="line">-  main(device, total_epochs, save_every)</span><br><span class="line">+  world_size = torch.cuda.device_count()</span><br><span class="line">+  mp.spawn(main, args=(world_size, total_epochs, save_every,), nprocs=world_size)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<blockquote>
<p>也可以改成torchrun运行：torchrun相比于该方法，具有更好的容错性，支持在节点计算失败时，重新启动训练，并且能够简化分布式训练环境的配置与进程管理。</p>
<p><strong>与 <code>torch.multiprocessing.spawn</code>的对比优势</strong></p>
<ul>
<li><strong>跨节点支持</strong>：<code>spawn</code>仅适用于<strong>单机多卡</strong>，而 <code>torchrun</code>原生支持<strong>多机多卡</strong>分布式训练</li>
<li><strong>配置简化</strong>：<code>spawn</code>需手动设置 <code>MASTER_ADDR</code>/<code>MASTER_PORT</code>并处理端口冲突，<code>torchrun</code>通过参数自动管理。</li>
<li><strong>高级特性</strong>：<code>spawn</code>缺乏弹性伸缩、进程监控和自动恢复等能力，需用户自行实现容错逻辑。</li>
</ul>
</blockquote>
<h2 id="模型并行">模型并行</h2>
<p><img src="matmul_model_paralelism.png" alt="模型并行"></p>
<p><strong>运行步骤：</strong></p>
<ul>
<li>所谓的模型并行，就是每个设备上的数据是完整的、一致的，而模型 w 被切分到了各个设备上，每个设备只拥有模型的一部分，所有计算设备上的模型拼在一起，才是完整的模型。</li>
<li>模型并行的好处是，省去了多个设备之间的梯度 AllReduce；但是，由于每个设备都需要完整的数据输入，因此，数据会在多个设备之间进行广播，产生通信代价。</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li>神经网络巨大，梯度同步代价大，甚至网络参数无法放倒单一设备上。</li>
<li>通信占比高，适合在机器内做模型并行。</li>
</ul>
<h2 id="流水并行">流水并行</h2>
<p><img src="realy.png" alt="流水并行"></p>
<p><strong>运行步骤：</strong></p>
<ul>
<li>
<p>流水并行指将网络切为多个阶段，并分发到不同的计算设备上，各个计算设备之间以“接力”的方式完成训练。</p>
</li>
<li>
<p>4层网络被切分到2个计算设备上，其中 <code>GPU0</code> 上进行 <code>T1</code> 与 <code>T2</code> 的运算，<code>GPU1</code> 上进行 <code>T3</code> 与 <code>T4</code> 的计算。</p>
</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li>流水线并行，训练设备容易出现空闲状态，加速效率没有数据并行高；但能减少通信边界支持更多的层数，适合在机器间使用。</li>
</ul>
<h2 id="混合并行">混合并行</h2>
<p><img src="gpt3-overview.png" alt="GPT3 训练架构"></p>
<ol>
<li>它首先被分为 64 个阶段，进行流水并行。每个阶段都运行在 6 台 DGX-A100 主机上；</li>
<li>在6台主机之间，进行的是数据并行训练；</li>
<li>每台主机有 8 张 GPU 显卡，同一台机器上的8张 GPU 显卡之间是进行模型并行训练。</li>
</ol>
<h1>参考文献</h1>
<blockquote>
<ol>
<li><a href="https://docs.oneflow.org/master/parallelism/01_introduction.html">常见的分布式并行策略</a></li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>framework</category>
      </categories>
      <tags>
        <tag>并行训练</tag>
        <tag>数据并行</tag>
        <tag>模型并行</tag>
        <tag>torchrun</tag>
        <tag>ddp</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt Engineering</title>
    <url>/2025/05/23/Prompt-Engineering/</url>
    <content><![CDATA[<p>提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。</p>
<span id="more"></span>
<h2 id="简介">简介</h2>
<h3 id="参数设置">参数设置</h3>
<ul>
<li>
<p><strong>Temperature</strong>：调整模型的softmax输出层中预测词的概率。<br>
<code>softmax</code>的计算公式为<br>
$$<br>
p(x_i)=\frac{e^{x_i}}{\sum_{j=1}^{V}e^{x_j}}<br>
$$<br>
加入温度系数<code>tempmerature</code>后：<br>
$$<br>
p(x_i)=\frac{\frac{e^{x_i}}{T}}{\sum_{j=1}^{V}\frac{e^{x_j}}{T}}<br>
$$</p>
<ul>
<li><code>temperature</code>为0到1的数值，数值越大，$p(x_i)$的分布越平滑，采样结果更随机。数值越小，$p(x_i)$的分布差异越大，采样结果越确定。</li>
<li>对于质量保障（QA）等任务，我们可以设置更低的 <code>temperature</code> 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 <code>temperature</code> 参数值可能会更好。</li>
</ul>
</li>
<li>
<p><strong>Top_p</strong>：使用Top P意味着只有词元集合（tokens）中包含<code>top_p</code>概率质量的才会被考虑用于响应。</p>
<ul>
<li><font color='red'>公式</font></li>
<li>如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。</li>
</ul>
</li>
<li>
<p><strong>Max Length</strong>：您可以通过调整 <code>max length</code> 来控制大模型生成的 token 数。指定 Max Length 有助于防止大模型生成冗长或不相关的响应并控制成本。</p>
</li>
<li>
<p><font color='red'><strong>Stop Sequences</strong>：<code>stop sequence</code> 是一个字符串，可以阻止模型生成 token，指定 <code>stop sequences</code> 是控制大模型响应长度和结构的另一种方法。例如，您可以通过添加 “11” 作为 <code>stop sequence</code> 来告诉模型生成不超过 10 个项的列表。</font></p>
</li>
<li>
<p><font color='red'><strong>Frequency Penalty</strong>：<code>frequency penalty</code> 是对下一个生成的 token 进行惩罚，这个惩罚和 token 在响应和提示中已出现的次数成比例， <code>frequency penalty</code> 越高，某个词再次出现的可能性就越小，这个设置通过给 重复数量多的 Token 设置更高的惩罚来减少响应中单词的重复。</font></p>
</li>
<li>
<p><font color='red'><strong>Presence Penalty</strong>：<code>presence penalty</code> 也是对重复的 token 施加惩罚，但与 <code>frequency penalty</code> 不同的是，惩罚对于所有重复 token 都是相同的。出现两次的 token 和出现 10 次的 token 会受到相同的惩罚。 此设置可防止模型在响应中过于频繁地生成重复的词。 如果您希望模型生成多样化或创造性的文本，您可以设置更高的 <code>presence penalty</code>，如果您希望模型生成更专注的内容，您可以设置更低的 <code>presence penalty</code>。</font></p>
</li>
</ul>
<h3 id="提示词要素">提示词要素</h3>
<ul>
<li><strong>指令</strong>：想要模型执行的特定任务或指令。</li>
<li><strong>上下文</strong>：包含外部信息或额外的上下文信息，引导语言模型更好地响应。</li>
<li>输入数据**：用户输入的内容或问题。**</li>
<li><strong>输出指示</strong>：指定输出的类型或格式。</li>
</ul>
<h2 id="提示词技术">提示词技术</h2>
<h3 id="零样本提示">零样本提示</h3>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">将文本分类为中性、负面或正面。</span><br><span class="line"></span><br><span class="line">文本：我认为这次假期还可以。</span><br><span class="line">情感：</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">中性</span><br></pre></td></tr></table></figure>
<p>当零样本不起作用时，建议在提示中提供演示或示例，这就引出了少样本提示。</p>
<h3 id="少样本提示">少样本提示</h3>
<p>使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。</p>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：</span><br><span class="line">我们在非洲旅行时看到了这些非常可爱的whatpus。</span><br><span class="line">“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">当我们赢得比赛时，我们都开始farduddle。</span><br></pre></td></tr></table></figure>
<p>模型通过提供一个示例（即1-shot）已经学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示（例如3-shot、5-shot、10-shot等）。</p>
<h3 id="思维链（CoT）提示">思维链（CoT）提示</h3>
<p>链式思考（CoT）提示通过中间推理步骤实现了复杂的推理能力。包括少样本思维链提示和零样本思维链提示。</p>
<p><img src="zero-cot.jpg" alt="CoT"></p>
<p><font color='red'>自动思维链（Auto-CoT）<a href="https://www.promptingguide.ai/zh/techniques/cot">https://www.promptingguide.ai/zh/techniques/cot</a></font></p>
]]></content>
      <categories>
        <category>llm</category>
      </categories>
      <tags>
        <tag>todo</tag>
        <tag>prompt</tag>
      </tags>
  </entry>
</search>
